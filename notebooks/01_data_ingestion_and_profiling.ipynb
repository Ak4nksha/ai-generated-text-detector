{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/01_data_ingestion_and_profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Ingestion & Profiling\n",
        "\n",
        "**Objective:**\n",
        "Collect and preprocess diverse text data from \"Human\" and \"AI\" sources to augment the training set. This notebook handles web scraping, API fetching, and parsing personal data dumps.\n",
        "\n",
        "**Data Sources Processed:**\n",
        "* **Wikipedia (Human):** Scraped articles across STEM, History, and Culture topics.\n",
        "* **arXiv (Human):** Abstract summaries from ML, Stats, and Physics papers.\n",
        "* **News Feeds (Human):** RSS articles from BBC, Reuters, and AP.\n",
        "* **ChatGPT History (AI):** Personal conversations exported (with consent) from OpenAI (Label 1).\n",
        "\n",
        "**Output:**\n",
        "* Generates `scraped_data_combined.csv` (saved to Drive), which will be merged with the larger Kaggle dataset in Notebook 2."
      ],
      "metadata": {
        "id": "fAEN_Ts94SiU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rygIZjsuFtvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287eee82-6c8c-4fc1-cdc5-dc92f066869c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pandas numpy scikit-learn tqdm wikipedia feedparser beautifulsoup4 readability-lxml requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document\n",
        "import feedparser\n",
        "import wikipedia\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"label\"     # 0=human, 1=AI\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    if t is None:\n",
        "        return \"\"\n",
        "    t = re.sub(r\"http\\S+\", \" \", str(t))\n",
        "    t = re.sub(r\"\\S+@\\S+\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def stable_hash(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "def chunk_text_words(text: str, chunk_words=200, overlap=40, min_words=40):\n",
        "    words = re.findall(r\"\\S+\", text)\n",
        "    if len(words) < min_words:\n",
        "        return []\n",
        "    step = max(1, chunk_words - overlap)\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), step):\n",
        "        ch = \" \".join(words[i:i+chunk_words]).strip()\n",
        "        if len(ch.split()) >= min_words:\n",
        "            chunks.append(ch)\n",
        "    return chunks\n",
        "\n",
        "def add_len_bins(df: pd.DataFrame):\n",
        "    df[\"len_words\"] = df[TEXT_COL].str.split().str.len()\n",
        "    df[\"len_bin\"] = pd.cut(df[\"len_words\"], bins=[0,10,25,50,100,200,400,1000,10_000],\n",
        "                           labels=False, include_lowest=True)\n",
        "    return df\n",
        "\n",
        "def dedup_by_text(df: pd.DataFrame):\n",
        "    df[\"text_hash\"] = df[TEXT_COL].map(stable_hash)\n",
        "    df = df.drop_duplicates(subset=[\"text_hash\"]).drop(columns=[\"text_hash\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "yI7ZCTwQHOym"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbewffA1HHdk",
        "outputId": "30ef4122-47c1-4924-f5ce-280018c37809"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n"
      ],
      "metadata": {
        "id": "_4AT70PUHeke"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHATGPT_JSON = \"/content/drive/MyDrive/conversations.json\"  # upload this\n",
        "\n",
        "with open(CHATGPT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    convos = json.load(f)\n",
        "\n",
        "rows = []\n",
        "for convo in convos:\n",
        "    convo_id = convo.get(\"id\", \"unknown_convo\")\n",
        "    mapping = convo.get(\"mapping\", {})\n",
        "    for node in mapping.values():\n",
        "        msg = node.get(\"message\")\n",
        "        if not msg:\n",
        "            continue\n",
        "        role = msg.get(\"author\", {}).get(\"role\")\n",
        "        parts = (msg.get(\"content\", {}) or {}).get(\"parts\", []) or []\n",
        "        if role == \"assistant\" and parts:\n",
        "            text = clean_text(\" \".join([p for p in parts if isinstance(p, str)]))\n",
        "            if len(text.split()) >= 20:\n",
        "                rows.append({\n",
        "                    \"doc_id\": f\"chatgpt_{convo_id}\",\n",
        "                    \"source\": \"chatgpt\",\n",
        "                    \"text\": text,\n",
        "                    \"label\": 1\n",
        "                })\n",
        "\n",
        "chatgpt_df = pd.DataFrame(rows)\n",
        "print(\"ChatGPT raw samples:\", chatgpt_df.shape)\n",
        "chatgpt_df.head()"
      ],
      "metadata": {
        "id": "DyJvts4xITq2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "74029a96-0845-4043-9662-935456f4d574"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT raw samples: (4178, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         doc_id   source  \\\n",
              "0  chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590  chatgpt   \n",
              "1  chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590  chatgpt   \n",
              "2  chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590  chatgpt   \n",
              "3  chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590  chatgpt   \n",
              "4  chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590  chatgpt   \n",
              "\n",
              "                                                text  label  \n",
              "0  Short answer: treat this like *two* binary cla...      1  \n",
              "1  Text. 100%. Given your background and what you...      1  \n",
              "2  Perfect — here is a **clean, actionable list**...      1  \n",
              "3  Nice, that Kaggle dataset is a good choice. Le...      1  \n",
              "4  Yes — you **already have a proper validation s...      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09c36a06-7098-4fa8-802f-5d13a0ec315c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>source</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590</td>\n",
              "      <td>chatgpt</td>\n",
              "      <td>Short answer: treat this like *two* binary cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590</td>\n",
              "      <td>chatgpt</td>\n",
              "      <td>Text. 100%. Given your background and what you...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590</td>\n",
              "      <td>chatgpt</td>\n",
              "      <td>Perfect — here is a **clean, actionable list**...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590</td>\n",
              "      <td>chatgpt</td>\n",
              "      <td>Nice, that Kaggle dataset is a good choice. Le...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>chatgpt_69351ea1-82b0-8325-8e60-8b957ba06590</td>\n",
              "      <td>chatgpt</td>\n",
              "      <td>Yes — you **already have a proper validation s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09c36a06-7098-4fa8-802f-5d13a0ec315c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09c36a06-7098-4fa8-802f-5d13a0ec315c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09c36a06-7098-4fa8-802f-5d13a0ec315c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7ec262fc-d9fe-40bd-8d3e-43e839c170a8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ec262fc-d9fe-40bd-8d3e-43e839c170a8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7ec262fc-d9fe-40bd-8d3e-43e839c170a8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "chatgpt_df",
              "summary": "{\n  \"name\": \"chatgpt_df\",\n  \"rows\": 4178,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 479,\n        \"samples\": [\n          \"chatgpt_6906a2e1-b7b0-8327-9263-8ce0f6d6be1b\",\n          \"chatgpt_6862eafe-2e50-8001-bf14-908575a3bd96\",\n          \"chatgpt_691b3cf7-f5c4-8321-9bec-8eed07306b7a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"chatgpt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4174,\n        \"samples\": [\n          \"Here\\u2019s a **story-led, human, natural, non-generic** version \\u2014 no \\u201ctruly excited,\\u201d no corporate clich\\u00e9s, just a genuine narrative that builds into the interest organically. --- # \\u2b50 **Message to the Hiring Team (Story-Driven, Human, No Generic Excitement)** When I first started working in AI consulting, one of the earliest projects I supported involved helping a client understand why their customers\\u2019 transactions were getting flagged incorrectly. I remember sitting in a room with their risk team, walking through dashboards we\\u2019d built, and realizing how much trust hinges on a single moment \\u2014 a purchase going through, a fraud attempt being stopped, a legitimate user not being blocked. That experience made something click for me: AI isn\\u2019t just technical horsepower; it\\u2019s a responsibility to help people feel safe when they interact with the financial world. That\\u2019s what drew me to this role at Visa. The scale, the mission, and the focus on risk and identity intelligence align perfectly with what I care about \\u2014 using AI in a way that protects people and supports businesses, without adding friction. My background in AI commercialization, GTM storytelling, and cross-functional collaboration has taught me how to translate complex capabilities into value that clients can actually see and trust. I\\u2019d love the chance to bring that skillset to Visa, continue learning from the experts on this team, and contribute to products that genuinely matter in people\\u2019s daily lives. --- If you want an even more emotional, more quantified, or more polished storytelling version, I can generate that too.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_chunks = []\n",
        "for _, r in chatgpt_df.iterrows():\n",
        "    for ch in chunk_text_words(r[\"text\"], chunk_words=180, overlap=40, min_words=40):\n",
        "        chat_chunks.append({\"doc_id\": r[\"doc_id\"], \"source\": r[\"source\"], \"text\": ch, \"label\": r[\"label\"]})\n",
        "\n",
        "chatgpt_df = pd.DataFrame(chat_chunks)\n",
        "print(\"ChatGPT chunks:\", chatgpt_df.shape)\n",
        "\n",
        "#So that long outputs don't dominate"
      ],
      "metadata": {
        "id": "PZ6mcqkyO8S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e3a07b-4f50-49f5-8521-d8f900933172"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT chunks: (12179, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WIKI_TOPICS = [\n",
        "    # --- Core AI / CS / Tech ---\n",
        "    \"Artificial intelligence\",\n",
        "    \"Machine learning\",\n",
        "    \"Deep learning\",\n",
        "    \"Natural language processing\",\n",
        "    \"Computer vision\",\n",
        "    \"Algorithms\",\n",
        "    \"Data science\",\n",
        "    \"Cloud computing\",\n",
        "    \"Cybersecurity\",\n",
        "    \"Blockchain\",\n",
        "\n",
        "    # --- Physical & Life Sciences ---\n",
        "    \"Quantum mechanics\",\n",
        "    \"Relativity\",\n",
        "    \"Thermodynamics\",\n",
        "    \"Biochemistry\",\n",
        "    \"Genetics\",\n",
        "    \"Molecular biology\",\n",
        "    \"Neuroscience\",\n",
        "    \"Climate change\",\n",
        "    \"Evolution\",\n",
        "    \"Epidemiology\",\n",
        "\n",
        "    # --- Mathematics & Logic ---\n",
        "    \"Calculus\",\n",
        "    \"Linear algebra\",\n",
        "    \"Probability theory\",\n",
        "    \"Statistics\",\n",
        "    \"Graph theory\",\n",
        "    \"Number theory\",\n",
        "    \"Mathematical logic\",\n",
        "\n",
        "    # --- Law, Policy & Society ---\n",
        "    \"Indian Contract Act\",\n",
        "    \"Constitution of India\",\n",
        "    \"International law\",\n",
        "    \"Human rights\",\n",
        "    \"Intellectual property\",\n",
        "    \"Privacy law\",\n",
        "    \"Cyber law\",\n",
        "    \"Regulation of artificial intelligence\",\n",
        "\n",
        "    # --- Economics, Business & Finance ---\n",
        "    \"Macroeconomics\",\n",
        "    \"Microeconomics\",\n",
        "    \"Behavioral economics\",\n",
        "    \"Game theory\",\n",
        "    \"Financial markets\",\n",
        "    \"Inflation\",\n",
        "    \"Globalization\",\n",
        "\n",
        "    # --- History & Politics ---\n",
        "    \"World War I\",\n",
        "    \"World War II\",\n",
        "    \"Cold War\",\n",
        "    \"French Revolution\",\n",
        "    \"Indian independence movement\",\n",
        "    \"United States presidential election\",\n",
        "    \"Geopolitics\",\n",
        "\n",
        "    # --- Literature, Language & Arts ---\n",
        "    \"William Shakespeare\",\n",
        "    \"English literature\",\n",
        "    \"Poetry\",\n",
        "    \"Drama\",\n",
        "    \"Literary criticism\",\n",
        "    \"Rhetoric\",\n",
        "    \"Comparative literature\",\n",
        "\n",
        "    # --- Philosophy & Social Thought ---\n",
        "    \"Philosophy\",\n",
        "    \"Ethics\",\n",
        "    \"Epistemology\",\n",
        "    \"Metaphysics\",\n",
        "    \"Existentialism\",\n",
        "    \"Philosophy of science\",\n",
        "\n",
        "    # --- Media, Sports & Culture ---\n",
        "    \"Formula One\",\n",
        "    \"Olympic Games\",\n",
        "    \"Association football\",\n",
        "    \"Cricket\",\n",
        "    \"Film theory\",\n",
        "    \"Music theory\",\n",
        "    \"Popular culture\",\n",
        "\n",
        "    # --- Current / General ---\n",
        "    \"Current events\",\n",
        "    \"Global health\",\n",
        "    \"Sustainability\",\n",
        "    \"Energy transition\",\n",
        "    \"Artificial general intelligence\"\n",
        "]\n",
        "\n",
        "\n",
        "wiki_docs = []\n",
        "for topic in WIKI_TOPICS:\n",
        "    try:\n",
        "        page = wikipedia.page(topic, auto_suggest=False)\n",
        "        text = clean_text(page.content)\n",
        "        wiki_docs.append({\n",
        "            \"doc_id\": f\"wiki_{page.pageid}\",\n",
        "            \"source\": \"wikipedia\",\n",
        "            \"text\": text,\n",
        "            \"label\": 0\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(\"Skip wiki:\", topic, \"|\", str(e)[:120])\n",
        "\n",
        "wiki_docs = pd.DataFrame(wiki_docs)\n",
        "\n",
        "wiki_chunks = []\n",
        "for _, r in wiki_docs.iterrows():\n",
        "    for ch in chunk_text_words(r[\"text\"], chunk_words=220, overlap=50, min_words=60):\n",
        "        wiki_chunks.append({\"doc_id\": r[\"doc_id\"], \"source\": r[\"source\"], \"text\": ch, \"label\": r[\"label\"]})\n",
        "\n",
        "wiki_df = pd.DataFrame(wiki_chunks)\n",
        "print(\"Wikipedia chunks:\", wiki_df.shape)\n"
      ],
      "metadata": {
        "id": "zctA2tlgL4dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfdf10d-d41a-4bcd-8ae9-b96273c21b2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip wiki: Relativity | \"Relativity\" may refer to: \n",
            "Galilean relativity\n",
            "Numerical relativity\n",
            "Principle of relativity\n",
            "Theory of relativity\n",
            "Genera\n",
            "Wikipedia chunks: (3355, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViDEPurZPD51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import urllib.parse\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "\n",
        "# import re\n",
        "# def clean_text(t: str) -> str:\n",
        "#     t = re.sub(r\"http\\S+\", \" \", str(t))\n",
        "#     t = re.sub(r\"\\S+@\\S+\", \" \", t)\n",
        "#     t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "#     return t\n",
        "\n",
        "ARXIV_QUERIES = [\n",
        "    \"cat:stat.ML\",        # Stats ML\n",
        "    \"cat:math.ST\",        # Statistics\n",
        "    \"cat:physics.optics\", # Physics\n",
        "    \"cat:q-bio.BM\"        # Bioinformatics\n",
        "]\n",
        "\n",
        "MAX_RESULTS_PER_QUERY = 200     # per category\n",
        "ARXIV_START = 0\n",
        "MIN_WORDS = 60                  # keep decent-length abstracts\n",
        "SLEEP_SECONDS = 3               # be polite to arXiv\n",
        "\n",
        "paper_rows = []\n",
        "seen_ids = set()\n",
        "\n",
        "for q in ARXIV_QUERIES:\n",
        "    encoded_q = urllib.parse.quote(q)\n",
        "    url = (\n",
        "        \"http://export.arxiv.org/api/query\"\n",
        "        f\"?search_query={encoded_q}\"\n",
        "        f\"&start={ARXIV_START}\"\n",
        "        f\"&max_results={MAX_RESULTS_PER_QUERY}\"\n",
        "    )\n",
        "\n",
        "    feed = feedparser.parse(url)\n",
        "    print(f\"Query={q} | fetched entries={len(feed.entries)}\")\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        arxiv_id = entry.id.split(\"/\")[-1]\n",
        "        if arxiv_id in seen_ids:\n",
        "            continue\n",
        "        seen_ids.add(arxiv_id)\n",
        "\n",
        "        title = clean_text(getattr(entry, \"title\", \"\"))\n",
        "        abstract = clean_text(getattr(entry, \"summary\", \"\"))\n",
        "\n",
        "        text = f\"{title}. {abstract}\".strip()\n",
        "        if len(text.split()) < MIN_WORDS:\n",
        "            continue\n",
        "\n",
        "        paper_rows.append({\n",
        "            \"doc_id\": f\"arxiv_{arxiv_id}\",\n",
        "            \"source\": \"arxiv\",\n",
        "            \"text\": text,\n",
        "            \"label\": 0\n",
        "        })\n",
        "\n",
        "    time.sleep(SLEEP_SECONDS)\n",
        "\n",
        "papers_df = pd.DataFrame(paper_rows).drop_duplicates(subset=[\"doc_id\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nDone\")\n",
        "print(\"Total arXiv samples:\", len(papers_df))\n",
        "print(papers_df.head(3))"
      ],
      "metadata": {
        "id": "tZNn6XSGMksW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba388de-7976-41b4-f29d-ce012ebb9f41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query=cat:stat.ML | fetched entries=200\n",
            "Query=cat:math.ST | fetched entries=200\n",
            "Query=cat:physics.optics | fetched entries=200\n",
            "Query=cat:q-bio.BM | fetched entries=200\n",
            "\n",
            "Done\n",
            "Total arXiv samples: 780\n",
            "               doc_id source  \\\n",
            "0  arxiv_2012.12056v1  arxiv   \n",
            "1  arxiv_2012.13115v1  arxiv   \n",
            "2  arxiv_2012.13190v2  arxiv   \n",
            "\n",
            "                                                text  label  \n",
            "0  Data Assimilation in the Latent Space of a Neu...      0  \n",
            "1  Upper Confidence Bounds for Combining Stochast...      0  \n",
            "2  QUACKIE: A NLP Classification Task With Ground...      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_chunks = []\n",
        "for _, r in papers_df.iterrows():\n",
        "    for ch in chunk_text_words(r[\"text\"], chunk_words=200, overlap=40, min_words=60):\n",
        "        paper_chunks.append({\"doc_id\": r[\"doc_id\"], \"source\": r[\"source\"], \"text\": ch, \"label\": r[\"label\"]})\n",
        "\n",
        "papers_df = pd.DataFrame(paper_chunks)\n",
        "print(\"arXiv chunks:\", papers_df.shape)\n",
        "\n",
        "\n",
        "#So long outputs don't dominate"
      ],
      "metadata": {
        "id": "jwA5lfcnO1Q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b795796-2f8b-4a69-9370-77e6d062e125"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arXiv chunks: (893, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NEWS_RSS = [\n",
        "    # --- BBC ---\n",
        "    \"https://feeds.bbci.co.uk/news/world/rss.xml\",\n",
        "    \"https://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://feeds.bbci.co.uk/news/business/rss.xml\",\n",
        "    \"https://feeds.bbci.co.uk/news/science_and_environment/rss.xml\",\n",
        "\n",
        "    # --- NPR (US, high editorial quality) ---\n",
        "    \"https://www.npr.org/rss/rss.php?id=1001\",   # News\n",
        "    \"https://www.npr.org/rss/rss.php?id=1019\",   # Technology\n",
        "    \"https://www.npr.org/rss/rss.php?id=1007\",   # Science\n",
        "    \"https://www.npr.org/rss/rss.php?id=1014\",   # World\n",
        "\n",
        "    # --- Reuters (excellent for neutral tone) ---\n",
        "    \"https://feeds.reuters.com/reuters/worldNews\",\n",
        "    \"https://feeds.reuters.com/reuters/businessNews\",\n",
        "    \"https://feeds.reuters.com/reuters/technologyNews\",\n",
        "    \"https://feeds.reuters.com/reuters/scienceNews\",\n",
        "\n",
        "    # --- Associated Press ---\n",
        "    \"https://apnews.com/rss\",\n",
        "    \"https://apnews.com/hub/technology/rss\",\n",
        "    \"https://apnews.com/hub/science/rss\",\n",
        "\n",
        "    # --- The Guardian ---\n",
        "    \"https://www.theguardian.com/world/rss\",\n",
        "    \"https://www.theguardian.com/technology/rss\",\n",
        "    \"https://www.theguardian.com/science/rss\",\n",
        "    \"https://www.theguardian.com/business/rss\",\n",
        "\n",
        "    # --- Financial / Economics ---\n",
        "    \"https://www.ft.com/rss/home\",            # Financial Times (some paywall, still usable)\n",
        "    \"https://www.economist.com/rss\"            # Economist (often truncated)\n",
        "]\n",
        "\n",
        "\n",
        "def fetch_article_text(url: str, timeout=10) -> str:\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
        "        if r.status_code != 200:\n",
        "            return \"\"\n",
        "        doc = Document(r.text)\n",
        "        html = doc.summary()\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = clean_text(soup.get_text(\" \"))\n",
        "        return text\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "news_rows = []\n",
        "MAX_ARTICLES_PER_FEED = 50\n",
        "\n",
        "for feed_url in NEWS_RSS:\n",
        "    f = feedparser.parse(feed_url)\n",
        "    for entry in f.entries[:MAX_ARTICLES_PER_FEED]:\n",
        "        url = entry.get(\"link\", \"\")\n",
        "        title = clean_text(entry.get(\"title\", \"\"))\n",
        "        body = fetch_article_text(url)\n",
        "        text = f\"{title}. {body}\".strip()\n",
        "        if len(text.split()) >= 80:\n",
        "            news_rows.append({\n",
        "                \"doc_id\": f\"news_{stable_hash(url)[:12]}\",\n",
        "                \"source\": \"news\",\n",
        "                \"text\": text,\n",
        "                \"label\": 0\n",
        "            })\n",
        "        time.sleep(0.2)  # be polite\n",
        "\n",
        "news_df = pd.DataFrame(news_rows)\n",
        "print(\"News samples:\", news_df.shape)"
      ],
      "metadata": {
        "id": "kXs2pCbFPM4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7facd4-10f4-4d17-aa3d-d0ed9be843b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News samples: (329, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_chunks = []\n",
        "for _, r in news_df.iterrows():\n",
        "    for ch in chunk_text_words(r[\"text\"], chunk_words=220, overlap=50, min_words=80):\n",
        "        news_chunks.append({\"doc_id\": r[\"doc_id\"], \"source\": r[\"source\"], \"text\": ch, \"label\": r[\"label\"]})\n",
        "\n",
        "news_df = pd.DataFrame(news_chunks)\n",
        "print(\"News chunks:\", news_df.shape)"
      ],
      "metadata": {
        "id": "zEH3oczRSaV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf2438c-6f7a-4f97-c72e-7f9ef6099998"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News chunks: (1822, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dfs_to_merge = []\n",
        "\n",
        "if 'chatgpt_df' in locals(): dfs_to_merge.append(chatgpt_df)\n",
        "if 'wiki_df' in locals(): dfs_to_merge.append(wiki_df)\n",
        "if 'papers_df' in locals(): dfs_to_merge.append(papers_df)\n",
        "if 'news_df' in locals(): dfs_to_merge.append(news_df)\n",
        "\n",
        "if dfs_to_merge:\n",
        "    scraped_combined = pd.concat(dfs_to_merge, ignore_index=True)\n",
        "    scraped_combined[\"text\"] = scraped_combined[\"text\"].astype(str).str.strip()\n",
        "    output_path = \"/content/drive/MyDrive/scraped_data_combined.csv\"\n",
        "    print(f\"Merging {len(dfs_to_merge)} data sources...\")\n",
        "    scraped_combined.to_csv(output_path, index=False)\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"SUCCESS: Scraped data merged and saved to: {output_path}\")\n",
        "    print(f\"Total Combined Shape: {scraped_combined.shape}\")\n",
        "    print(\"Source Distribution:\")\n",
        "    print(scraped_combined[\"source\"].value_counts())\n",
        "else:\n",
        "    print(\"ERROR: No dataframes found to merge. Please run the scraping cells first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfhZyj_7qXvV",
        "outputId": "f248ab6d-e04a-4a48-fa0a-fbfcbd685ef4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging 4 data sources...\n",
            "------------------------------\n",
            "SUCCESS: Scraped data merged and saved to: /content/drive/MyDrive/scraped_data_combined.csv\n",
            "Total Combined Shape: (18249, 4)\n",
            "Source Distribution:\n",
            "source\n",
            "chatgpt      12179\n",
            "wikipedia     3355\n",
            "news          1822\n",
            "arxiv          893\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Completed"
      ],
      "metadata": {
        "id": "rukmXqodqMN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}