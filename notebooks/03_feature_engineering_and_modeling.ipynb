{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/03_feature_engineering_and_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM_6yTasgkhO"
      },
      "outputs": [],
      "source": [
        "!pip install -q textstat spacy tqdm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import textstat\n",
        "import spacy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tqdm.pandas()  # enable progress bar on apply\n",
        "\n",
        "# load lightweight spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "Q5RghvVihO0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "WVnPF3vS6IIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"/content/drive/MyDrive/final_merged_dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, engine=\"python\", escapechar='\\\\',on_bad_lines='skip')\n",
        "print(\"Columns:\", df.columns)\n",
        "print(len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "82zEiH1DiGmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"/content/drive/MyDrive/final_merged_dataset.csv\"\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"label\"\n",
        "\n",
        "# Robust CSV load (handles quotes, bad lines)\n",
        "df = pd.read_csv(\n",
        "    CSV_PATH,\n",
        "    engine=\"python\",\n",
        "    escapechar=\"\\\\\",\n",
        "    on_bad_lines=\"skip\"\n",
        ")\n",
        "\n",
        "# Drop rows with missing text/label\n",
        "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "\n",
        "# Clean label column: ensure numeric 0/1\n",
        "df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors=\"coerce\")\n",
        "df[LABEL_COL] = df[LABEL_COL].fillna(0).astype(int)\n",
        "\n",
        "#additional fixes for NAN values\n",
        "df[\"source\"] = df[\"source\"].fillna(\"kaggle\")\n",
        "nan_rows = df['doc_id'].isna()\n",
        "df.loc[nan_rows, 'doc_id'] = \"kaggle_\" + df.loc[nan_rows].index.astype(str)\n",
        "\n",
        "\n",
        "print(\"Columns:\", df.columns)\n",
        "print(\"First few rows:\")\n",
        "display(df.head())\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "\n",
        "print(\"\\nLabel value counts:\")\n",
        "# Count class distribution\n",
        "counts = df[LABEL_COL].value_counts().sort_index()\n",
        "print(counts)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "counts.plot(kind='bar', color=['steelblue', 'darkorange'])\n",
        "plt.title(\"Distribution of Human (0) vs AI (1) Texts\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "\n",
        "# Annotate bars with counts\n",
        "for i, v in enumerate(counts):\n",
        "    plt.text(i, v + 500, str(v), ha='center', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h6YojEsOi857"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_counts(text: str):\n",
        "    text = text or \"\"\n",
        "    num_chars = len(text)\n",
        "\n",
        "    # sentence split\n",
        "    sentences = re.split(r\"[.!?]+\", text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    num_sent = len(sentences) if sentences else 1\n",
        "\n",
        "    # word tokens\n",
        "    words = re.findall(r\"\\w+\", text)\n",
        "    num_words = len(words) if words else 1\n",
        "\n",
        "    avg_sent_len = num_words / num_sent\n",
        "    return {\n",
        "        \"num_chars\": num_chars,\n",
        "        \"num_words\": num_words,\n",
        "        \"num_sentences\": num_sent,\n",
        "        \"avg_sentence_length\": avg_sent_len,\n",
        "    }\n",
        "\n",
        "\n",
        "def lexical_diversity(text: str):\n",
        "    words = re.findall(r\"\\w+\", str(text).lower())\n",
        "    if not words:\n",
        "        return {\n",
        "            \"type_token_ratio\": 0.0,\n",
        "            \"unique_words\": 0,\n",
        "        }\n",
        "    unique = set(words)\n",
        "    ttr = len(unique) / len(words)\n",
        "    return {\n",
        "        \"type_token_ratio\": ttr,\n",
        "        \"unique_words\": len(unique),\n",
        "    }\n",
        "\n",
        "\n",
        "def punctuation_stats(text: str):\n",
        "    text = text or \"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            \"pct_punct\": 0.0,\n",
        "            \"pct_upper\": 0.0,\n",
        "            \"pct_digit\": 0.0,\n",
        "        }\n",
        "    total = len(text)\n",
        "    punct = sum(ch in string.punctuation for ch in text)\n",
        "    upper = sum(ch.isupper() for ch in text)\n",
        "    digit = sum(ch.isdigit() for ch in text)\n",
        "    return {\n",
        "        \"pct_punct\": punct / total,\n",
        "        \"pct_upper\": upper / total,\n",
        "        \"pct_digit\": digit / total,\n",
        "    }\n",
        "\n",
        "\n",
        "def readability_features(text: str):\n",
        "    clean = text if isinstance(text, str) else \"\"\n",
        "    if len(clean.split()) < 3:\n",
        "        return {\n",
        "            \"flesch_reading_ease\": 0.0,\n",
        "            \"flesch_kincaid_grade\": 0.0,\n",
        "            \"gunning_fog\": 0.0,\n",
        "        }\n",
        "    try:\n",
        "        fre = textstat.flesch_reading_ease(clean)\n",
        "        fkg = textstat.flesch_kincaid_grade(clean)\n",
        "        gf  = textstat.gunning_fog(clean)\n",
        "    except Exception:\n",
        "        fre, fkg, gf = 0.0, 0.0, 0.0\n",
        "    return {\n",
        "        \"flesch_reading_ease\": fre,\n",
        "        \"flesch_kincaid_grade\": fkg,\n",
        "        \"gunning_fog\": gf,\n",
        "    }\n",
        "\n",
        "\n",
        "def repetition_features(text: str):\n",
        "    tokens = re.findall(r\"\\w+\", str(text).lower())\n",
        "    if len(tokens) < 4:\n",
        "        return {\"bigram_repetition_ratio\": 0.0}\n",
        "    bigrams = list(zip(tokens, tokens[1:]))\n",
        "    total_bigrams = len(bigrams)\n",
        "    counts = Counter(bigrams)\n",
        "    repeated = sum(c for c in counts.values() if c > 1)\n",
        "    return {\n",
        "        \"bigram_repetition_ratio\": repeated / total_bigrams\n",
        "    }\n",
        "\n",
        "\n",
        "def pos_features_spacy(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [t for t in doc if not t.is_space]\n",
        "    total_tokens = len(tokens)\n",
        "    if total_tokens == 0:\n",
        "        return {\n",
        "            \"pos_ratio_NOUN\": 0.0,\n",
        "            \"pos_ratio_VERB\": 0.0,\n",
        "            \"pos_ratio_ADJ\": 0.0,\n",
        "            \"pos_ratio_ADV\": 0.0,\n",
        "            \"pos_ratio_PRON\": 0.0,\n",
        "            \"pos_ratio_ADP\": 0.0,\n",
        "            \"pos_ratio_DET\": 0.0,\n",
        "        }\n",
        "    counts = Counter(tok.pos_ for tok in tokens)\n",
        "\n",
        "    def ratio(tag):\n",
        "        return counts.get(tag, 0) / total_tokens\n",
        "\n",
        "    return {\n",
        "        \"pos_ratio_NOUN\": ratio(\"NOUN\"),\n",
        "        \"pos_ratio_VERB\": ratio(\"VERB\"),\n",
        "        \"pos_ratio_ADJ\":  ratio(\"ADJ\"),\n",
        "        \"pos_ratio_ADV\":  ratio(\"ADV\"),\n",
        "        \"pos_ratio_PRON\": ratio(\"PRON\"),\n",
        "        \"pos_ratio_ADP\":  ratio(\"ADP\"),\n",
        "        \"pos_ratio_DET\":  ratio(\"DET\"),\n",
        "    }\n",
        "\n",
        "\n",
        "def sentence_length_stats(text: str):\n",
        "    sentences = re.split(r\"[.!?]+\", str(text))\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    if len(sentences) < 2:\n",
        "        return {\n",
        "            \"sentence_length_std\": 0.0,\n",
        "            \"sentence_length_mean\": len(str(text).split()),\n",
        "        }\n",
        "    lens = [len(s.split()) for s in sentences]\n",
        "    return {\n",
        "        \"sentence_length_std\": float(np.std(lens)),\n",
        "        \"sentence_length_mean\": float(np.mean(lens)),\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_all_features(text: str):\n",
        "    feats = {}\n",
        "    feats.update(basic_counts(text))\n",
        "    feats.update(lexical_diversity(text))\n",
        "    feats.update(punctuation_stats(text))\n",
        "    feats.update(readability_features(text))\n",
        "    feats.update(repetition_features(text))\n",
        "    feats.update(pos_features_spacy(text))\n",
        "    feats.update(sentence_length_stats(text))\n",
        "    return feats"
      ],
      "metadata": {
        "id": "Lr0oR0k-llAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing features for each text\")\n",
        "\n",
        "features_series = df[TEXT_COL].progress_apply(compute_all_features)\n",
        "features_df = pd.DataFrame(list(features_series))\n",
        "\n",
        "print(\"New feature columns:\", features_df.columns.tolist())\n",
        "\n",
        "df_aug = pd.concat(\n",
        "    [df.reset_index(drop=True), features_df.reset_index(drop=True)],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\nAugmented dataframe shape:\", df_aug.shape)\n",
        "df_aug.head()"
      ],
      "metadata": {
        "id": "w5YaCaMElsC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(\n",
        "    df_aug,\n",
        "    test_size=0.3,\n",
        "    stratify=df_aug[LABEL_COL],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,\n",
        "    stratify=temp_df[LABEL_COL],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(len(train_df), len(val_df), len(test_df))"
      ],
      "metadata": {
        "id": "uqt9YEOGrHFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Check that there is no overlap between train/val/test ----\n",
        "train_idx = set(train_df.index)\n",
        "val_idx   = set(val_df.index)\n",
        "test_idx  = set(test_df.index)\n",
        "\n",
        "print(\"Overlap train ∩ val:\", len(train_idx & val_idx))\n",
        "print(\"Overlap train ∩ test:\", len(train_idx & test_idx))\n",
        "print(\"Overlap val ∩ test:\", len(val_idx & test_idx))\n",
        "\n",
        "assert len(train_idx & val_idx) == 0, \"Train and val sets overlap!\"\n",
        "assert len(train_idx & test_idx) == 0, \"Train and test sets overlap!\"\n",
        "assert len(val_idx & test_idx) == 0, \"Val and test sets overlap!\"\n",
        "\n",
        "print(\"\\n✅ No index overlap between train, val, and test.\")\n"
      ],
      "metadata": {
        "id": "wmGT1rLjwT2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = [\n",
        "    c for c in df_aug.columns\n",
        "    if c not in [TEXT_COL, LABEL_COL]\n",
        "]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"tfidf\", TfidfVectorizer(\n",
        "            max_features=20000,\n",
        "            ngram_range=(1,3),\n",
        "            sublinear_tf=True\n",
        "        ), TEXT_COL),\n",
        "        (\"num\", StandardScaler(), numeric_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"logreg\", LogisticRegression(\n",
        "        max_iter=3000,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"Training model...\")\n",
        "clf.fit(train_df, train_df[LABEL_COL])\n",
        "\n",
        "val_pred = clf.predict(val_df)\n",
        "test_pred = clf.predict(test_df)\n",
        "\n",
        "print(\"\\n=== VALIDATION REPORT ===\")\n",
        "print(classification_report(val_df[LABEL_COL], val_pred, digits=4))\n",
        "\n",
        "print(\"\\n=== TEST REPORT ===\")\n",
        "print(classification_report(test_df[LABEL_COL], test_pred, digits=4))"
      ],
      "metadata": {
        "id": "SinkmAinrRCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Extract feature names from the pipeline ----\n",
        "logreg = clf.named_steps[\"logreg\"]\n",
        "preproc = clf.named_steps[\"preprocess\"]\n",
        "\n",
        "# TF-IDF feature names\n",
        "tfidf = preproc.named_transformers_[\"tfidf\"]\n",
        "tfidf_features = tfidf.get_feature_names_out()\n",
        "\n",
        "# Numeric feature names (we already have numeric_cols list)\n",
        "num_features = np.array(numeric_cols)\n",
        "\n",
        "# Concatenate all feature names in the same order as the transformed matrix\n",
        "all_feature_names = np.concatenate([tfidf_features, num_features])\n",
        "\n",
        "# Logistic regression coefficients for class 1 (binary: shape = (1, n_features))\n",
        "coefs = logreg.coef_[0]\n",
        "\n",
        "coef_df = pd.DataFrame({\n",
        "    \"feature\": all_feature_names,\n",
        "    \"coef\": coefs\n",
        "})\n",
        "\n",
        "# ---- Top 20 positive (AI indicators) & Top 20 negative (Human indicators) ----\n",
        "top_pos = coef_df.nlargest(20, \"coef\")   # pushes towards class 1 (AI)\n",
        "top_neg = coef_df.nsmallest(20, \"coef\") # pushes towards class 0 (Human)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Positive coefficients (AI)\n",
        "axes[0].barh(top_pos[\"feature\"], top_pos[\"coef\"])\n",
        "axes[0].set_title(\"Top Features Pushing Towards AI (class 1)\")\n",
        "axes[0].invert_yaxis()  # largest at top\n",
        "axes[0].set_xlabel(\"Coefficient value\")\n",
        "\n",
        "# Negative coefficients (Human)\n",
        "axes[1].barh(top_neg[\"feature\"], top_neg[\"coef\"])\n",
        "axes[1].set_title(\"Top Features Pushing Towards Human (class 0)\")\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].set_xlabel(\"Coefficient value\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "top_pos, top_neg.head()"
      ],
      "metadata": {
        "id": "7Q6IX44xsKTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def predict_text_is_ai(text: str):\n",
        "    \"\"\"\n",
        "    Take a raw text string, compute stylometric features,\n",
        "    and run it through the trained clf pipeline.\n",
        "    Returns predicted label + probability.\n",
        "    \"\"\"\n",
        "    # 1) Compute features using the same function as training\n",
        "    feats = compute_all_features(text)\n",
        "\n",
        "    # 2) Build one-row DataFrame with text + all numeric feature cols\n",
        "    row = {TEXT_COL: text}\n",
        "    row.update(feats)\n",
        "\n",
        "    # Ensure all expected numeric columns are present (in correct order)\n",
        "    for col in numeric_cols:\n",
        "        if col not in row:\n",
        "            row[col] = 0.0  # fallback, should not happen if funcs are consistent\n",
        "\n",
        "    df_input = pd.DataFrame([row])\n",
        "\n",
        "    # 3) Predict with the pipeline\n",
        "    pred = clf.predict(df_input)[0]                # 0 or 1\n",
        "    proba = clf.predict_proba(df_input)[0]         # [P(human), P(AI)]\n",
        "\n",
        "    label = \"AI-generated\" if pred == 1 else \"Human-written\"\n",
        "    confidence_ai = proba[1]\n",
        "    confidence_human = proba[0]\n",
        "\n",
        "    return {\n",
        "        \"predicted_class\": int(pred),\n",
        "        \"label\": label,\n",
        "        \"confidence_ai\": float(confidence_ai),\n",
        "        \"confidence_human\": float(confidence_human),\n",
        "    }\n",
        "\n",
        "# Simple interactive loop\n",
        "while True:\n",
        "    user_text = input(\"\\nPaste text to test (or type 'quit' to stop):\\n> \")\n",
        "    if user_text.lower().strip() in [\"quit\", \"exit\", \"q\"]:\n",
        "        print(\"Exiting.\")\n",
        "        break\n",
        "\n",
        "    result = predict_text_is_ai(user_text)\n",
        "    print(f\"\\nPrediction: {result['label']}\")\n",
        "    print(f\"  → P(Human) = {result['confidence_human']:.3f}\")\n",
        "    print(f\"  → P(AI)    = {result['confidence_ai']:.3f}\")"
      ],
      "metadata": {
        "id": "KEKUcL5-ww1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsNjESfyz0-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9T4D2CdMzwko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}