{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fc3048-09c4-48de-ba26-778a6f5f62e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Install all required libraries for this project\n",
    "# =========================\n",
    "!pip -q install -U \\\n",
    "  numpy pandas scipy \\\n",
    "  scikit-learn \\\n",
    "  matplotlib \\\n",
    "  tqdm \\\n",
    "  textstat \\\n",
    "  torch \\\n",
    "  transformers \\\n",
    "  datasets \\\n",
    "  accelerate\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61239cbd-96be-4410-b3a5-286dfa7389fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re, math, hashlib, zlib\n",
    "from collections import Counter\n",
    "\n",
    "import textstat\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tqdm.pandas()  # enable progress bar on apply\n",
    "\n",
    "# load lightweight spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9507689a-3aec-4110-9118-6bd67024b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['text', 'label', 'doc_id', 'source'], dtype='object')\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Face in the picture was not created by ali...</td>\n",
       "      <td>0</td>\n",
       "      <td>kaggle_0</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: My Dream Career: Becoming a Software En...</td>\n",
       "      <td>1</td>\n",
       "      <td>kaggle_1</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attending classes at home is not a great benef...</td>\n",
       "      <td>0</td>\n",
       "      <td>kaggle_2</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's Your Opinion? By: Generic_Name Have you...</td>\n",
       "      <td>0</td>\n",
       "      <td>kaggle_3</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The exploration of nearby planets has always b...</td>\n",
       "      <td>1</td>\n",
       "      <td>kaggle_4</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label    doc_id  source\n",
       "0  The Face in the picture was not created by ali...      0  kaggle_0  kaggle\n",
       "1  Title: My Dream Career: Becoming a Software En...      1  kaggle_1  kaggle\n",
       "2  Attending classes at home is not a great benef...      0  kaggle_2  kaggle\n",
       "3  What's Your Opinion? By: Generic_Name Have you...      0  kaggle_3  kaggle\n",
       "4  The exploration of nearby planets has always b...      1  kaggle_4  kaggle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: (505254, 4)\n",
      "\n",
      "Label value counts:\n",
      "label\n",
      "0    311712\n",
      "1    193542\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CSV_PATH = \"final_merged_dataset.csv\"\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# Robust CSV load (handles quotes, bad lines)\n",
    "df = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    engine=\"python\",\n",
    "    escapechar=\"\\\\\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Drop rows with missing text/label\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
    "\n",
    "# Clean label column: ensure numeric 0/1\n",
    "df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors=\"coerce\")\n",
    "df[LABEL_COL] = df[LABEL_COL].fillna(0).astype(int)\n",
    "\n",
    "#additional fixes for NAN values\n",
    "df[\"source\"] = df[\"source\"].fillna(\"kaggle\")\n",
    "nan_rows = df['doc_id'].isna()\n",
    "df.loc[nan_rows, 'doc_id'] = \"kaggle_\" + df.loc[nan_rows].index.astype(str)\n",
    "\n",
    "\n",
    "print(\"Columns:\", df.columns)\n",
    "print(\"First few rows:\")\n",
    "display(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nLabel value counts:\")\n",
    "# Count class distribution\n",
    "counts = df[LABEL_COL].value_counts().sort_index()\n",
    "print(counts)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot\n",
    "plt.figure(figsize=(6,4))\n",
    "counts.plot(kind='bar', color=['steelblue', 'darkorange'])\n",
    "plt.title(\"Distribution of Human (0) vs AI (1) Texts\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "\n",
    "# Annotate bars with counts\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 500, str(v), ha='center', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a22624-6fef-4582-b12a-c429d7035be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FINAL FEATURE WRAPPER CELL\n",
    "# (Does NOT modify your existing features)\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def get_lines(text: str, max_lines: int):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    return text.splitlines()[:max_lines]\n",
    "\n",
    "def basic_counts(text: str):\n",
    "    text = text or \"\"\n",
    "    num_chars = len(text)\n",
    "\n",
    "    # sentence split\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    num_sent = len(sentences) if sentences else 1\n",
    "\n",
    "    # word tokens\n",
    "    words = re.findall(r\"\\w+\", text)\n",
    "    num_words = len(words) if words else 1\n",
    "\n",
    "    avg_sent_len = num_words / num_sent\n",
    "    return {\n",
    "        \"num_chars\": num_chars,\n",
    "        \"num_words\": num_words,\n",
    "        \"num_sentences\": num_sent,\n",
    "        \"avg_sentence_length\": avg_sent_len,\n",
    "    }\n",
    "\n",
    "\n",
    "def lexical_diversity(text: str):\n",
    "    words = re.findall(r\"\\w+\", str(text).lower())\n",
    "    if not words:\n",
    "        return {\n",
    "            \"type_token_ratio\": 0.0,\n",
    "            \"unique_words\": 0,\n",
    "        }\n",
    "    unique = set(words)\n",
    "    ttr = len(unique) / len(words)\n",
    "    return {\n",
    "        \"type_token_ratio\": ttr,\n",
    "        \"unique_words\": len(unique),\n",
    "    }\n",
    "\n",
    "\n",
    "def punctuation_stats(text: str):\n",
    "    text = text or \"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            \"pct_punct\": 0.0,\n",
    "            \"pct_upper\": 0.0,\n",
    "            \"pct_digit\": 0.0,\n",
    "        }\n",
    "    total = len(text)\n",
    "    punct = sum(ch in string.punctuation for ch in text)\n",
    "    upper = sum(ch.isupper() for ch in text)\n",
    "    digit = sum(ch.isdigit() for ch in text)\n",
    "    return {\n",
    "        \"pct_punct\": punct / total,\n",
    "        \"pct_upper\": upper / total,\n",
    "        \"pct_digit\": digit / total,\n",
    "    }\n",
    "\n",
    "\n",
    "def readability_features(text: str):\n",
    "    clean = text if isinstance(text, str) else \"\"\n",
    "    if len(clean.split()) < 3:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": 0.0,\n",
    "            \"flesch_kincaid_grade\": 0.0,\n",
    "            \"gunning_fog\": 0.0,\n",
    "        }\n",
    "    try:\n",
    "        fre = textstat.flesch_reading_ease(clean)\n",
    "        fkg = textstat.flesch_kincaid_grade(clean)\n",
    "        gf  = textstat.gunning_fog(clean)\n",
    "    except Exception:\n",
    "        fre, fkg, gf = 0.0, 0.0, 0.0\n",
    "    return {\n",
    "        \"flesch_reading_ease\": fre,\n",
    "        \"flesch_kincaid_grade\": fkg,\n",
    "        \"gunning_fog\": gf,\n",
    "    }\n",
    "\n",
    "\n",
    "def repetition_features(text: str):\n",
    "    tokens = re.findall(r\"\\w+\", str(text).lower())\n",
    "    if len(tokens) < 4:\n",
    "        return {\"bigram_repetition_ratio\": 0.0}\n",
    "    bigrams = list(zip(tokens, tokens[1:]))\n",
    "    total_bigrams = len(bigrams)\n",
    "    counts = Counter(bigrams)\n",
    "    repeated = sum(c for c in counts.values() if c > 1)\n",
    "    return {\n",
    "        \"bigram_repetition_ratio\": repeated / total_bigrams\n",
    "    }\n",
    "\n",
    "\n",
    "def pos_features_spacy(text: str):\n",
    "    doc = nlp(str(text))\n",
    "    tokens = [t for t in doc if not t.is_space]\n",
    "    total_tokens = len(tokens)\n",
    "    if total_tokens == 0:\n",
    "        return {\n",
    "            \"pos_ratio_NOUN\": 0.0,\n",
    "            \"pos_ratio_VERB\": 0.0,\n",
    "            \"pos_ratio_ADJ\": 0.0,\n",
    "            \"pos_ratio_ADV\": 0.0,\n",
    "            \"pos_ratio_PRON\": 0.0,\n",
    "            \"pos_ratio_ADP\": 0.0,\n",
    "            \"pos_ratio_DET\": 0.0,\n",
    "        }\n",
    "    counts = Counter(tok.pos_ for tok in tokens)\n",
    "\n",
    "    def ratio(tag):\n",
    "        return counts.get(tag, 0) / total_tokens\n",
    "\n",
    "    return {\n",
    "        \"pos_ratio_NOUN\": ratio(\"NOUN\"),\n",
    "        \"pos_ratio_VERB\": ratio(\"VERB\"),\n",
    "        \"pos_ratio_ADJ\":  ratio(\"ADJ\"),\n",
    "        \"pos_ratio_ADV\":  ratio(\"ADV\"),\n",
    "        \"pos_ratio_PRON\": ratio(\"PRON\"),\n",
    "        \"pos_ratio_ADP\":  ratio(\"ADP\"),\n",
    "        \"pos_ratio_DET\":  ratio(\"DET\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def sentence_length_stats(text: str):\n",
    "    sentences = re.split(r\"[.!?]+\", str(text))\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if len(sentences) < 2:\n",
    "        return {\n",
    "            \"sentence_length_std\": 0.0,\n",
    "            \"sentence_length_mean\": len(str(text).split()),\n",
    "        }\n",
    "    lens = [len(s.split()) for s in sentences]\n",
    "    return {\n",
    "        \"sentence_length_std\": float(np.std(lens)),\n",
    "        \"sentence_length_mean\": float(np.mean(lens)),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_all_features(text: str):\n",
    "    feats = {}\n",
    "    feats.update(basic_counts(text))\n",
    "    feats.update(lexical_diversity(text))\n",
    "    feats.update(punctuation_stats(text))\n",
    "    feats.update(readability_features(text))\n",
    "    feats.update(repetition_features(text))\n",
    "    feats.update(pos_features_spacy(text))\n",
    "    feats.update(sentence_length_stats(text))\n",
    "    return feats\n",
    "\n",
    "# ============================================================\n",
    "# NEW FEATURES (ADDITIVE ONLY — your features remain unchanged)\n",
    "# ============================================================\n",
    "\n",
    "# --- A) Em-dash / emphasis punctuation ---\n",
    "def emphasis_punctuation_features(text: str, max_lines: int = 5) -> Dict:\n",
    "    lines = get_lines(text, max_lines)\n",
    "    joined = \"\\n\".join(lines)\n",
    "    n = len(joined) if joined else 1\n",
    "\n",
    "    return {\n",
    "        \"emdash_ratio\": joined.count(\"—\") / n,\n",
    "        \"double_dash_ratio\": joined.count(\"--\") / n,\n",
    "        \"colon_ratio_emphasis\": joined.count(\":\") / n,\n",
    "        \"semicolon_ratio_emphasis\": joined.count(\";\") / n,\n",
    "        \"emphasis_punctuation_ratio\": (\n",
    "            joined.count(\"—\")\n",
    "            + joined.count(\"--\")\n",
    "            + joined.count(\":\")\n",
    "            + joined.count(\";\")\n",
    "        ) / n,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- B) Hedging / cautious language ---\n",
    "HEDGE_PHRASES = [\n",
    "    \"some might argue\",\n",
    "    \"it could be said\",\n",
    "    \"it may be\",\n",
    "    \"it might be\",\n",
    "    \"it is possible that\",\n",
    "    \"it appears that\",\n",
    "    \"it seems that\",\n",
    "    \"may suggest\",\n",
    "    \"could suggest\",\n",
    "    \"is often considered\",\n",
    "    \"is generally regarded\",\n",
    "]\n",
    "\n",
    "def hedging_language_features(text: str, max_lines: int = 5) -> Dict:\n",
    "    lines = get_lines(text.lower(), max_lines)\n",
    "    if not lines:\n",
    "        return {\n",
    "            \"hedge_phrase_count\": 0,\n",
    "            \"hedge_phrase_ratio\": 0.0,\n",
    "            \"hedge_line_ratio\": 0.0,\n",
    "        }\n",
    "\n",
    "    hedge_count = 0\n",
    "    hedge_lines = 0\n",
    "\n",
    "    for ln in lines:\n",
    "        c = sum(ln.count(p) for p in HEDGE_PHRASES)\n",
    "        if c > 0:\n",
    "            hedge_count += c\n",
    "            hedge_lines += 1\n",
    "\n",
    "    total = len(lines)\n",
    "\n",
    "    return {\n",
    "        \"hedge_phrase_count\": hedge_count,\n",
    "        \"hedge_phrase_ratio\": hedge_count / total,\n",
    "        \"hedge_line_ratio\": hedge_lines / total,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- C) Vague / hallucination-proxy language ---\n",
    "GENERIC_CLAIMS = [\n",
    "    \"studies suggest\",\n",
    "    \"research shows\",\n",
    "    \"experts believe\",\n",
    "    \"it is believed\",\n",
    "    \"it is widely accepted\",\n",
    "]\n",
    "\n",
    "VAGUE_QUANTIFIERS = [\n",
    "    \"various\",\n",
    "    \"numerous\",\n",
    "    \"several\",\n",
    "    \"many\",\n",
    "    \"a number of\",\n",
    "    \"some\",\n",
    "]\n",
    "\n",
    "FAKE_CITATION_PATTERNS = [\n",
    "    r\"\\[\\d+\\]\",        # [1], [23]\n",
    "    r\"\\(\\d{4}\\)\",      # (2021)\n",
    "    r\"et al\\.\",        # et al.\n",
    "]\n",
    "\n",
    "def vague_and_hallucination_features(text: str, max_lines: int = 5) -> Dict:\n",
    "    lines = get_lines(text.lower(), max_lines)\n",
    "    if not lines:\n",
    "        return {\n",
    "            \"generic_claim_ratio\": 0.0,\n",
    "            \"vague_quantifier_ratio\": 0.0,\n",
    "            \"fake_citation_ratio\": 0.0,\n",
    "        }\n",
    "\n",
    "    generic = 0\n",
    "    vague = 0\n",
    "    fake = 0\n",
    "\n",
    "    for ln in lines:\n",
    "        if any(p in ln for p in GENERIC_CLAIMS):\n",
    "            generic += 1\n",
    "        if any(q in ln for q in VAGUE_QUANTIFIERS):\n",
    "            vague += 1\n",
    "        if any(re.search(rx, ln) for rx in FAKE_CITATION_PATTERNS):\n",
    "            fake += 1\n",
    "\n",
    "    total = len(lines)\n",
    "\n",
    "    return {\n",
    "        \"generic_claim_ratio\": generic / total,\n",
    "        \"vague_quantifier_ratio\": vague / total,\n",
    "        \"fake_citation_ratio\": fake / total,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FINAL WRAPPER (calls YOUR features + new ones)\n",
    "# ============================================================\n",
    "\n",
    "def compute_all_features_final(text: str, max_lines: int = 5) -> Dict:\n",
    "    feats = {}\n",
    "    feats.update(basic_counts(text))\n",
    "    feats.update(lexical_diversity(text))\n",
    "    feats.update(punctuation_stats(text))\n",
    "    feats.update(readability_features(text))\n",
    "    feats.update(repetition_features(text))\n",
    "    feats.update(pos_features_spacy(text))\n",
    "    feats.update(sentence_length_stats(text))\n",
    "    feats.update(emphasis_punctuation_features(text, max_lines))\n",
    "    feats.update(hedging_language_features(text, max_lines))\n",
    "    feats.update(vague_and_hallucination_features(text, max_lines))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "-------------------------\n",
    "quick sanity check\n",
    "-------------------------\n",
    "print(compute_all_features_final(\n",
    "    \"Some might argue — it could be said that research shows various factors.\\n[1] et al. (2021)\",\n",
    "    max_lines=5\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b6a8ba-0856-46dc-93b3-4d99eca4c6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2319/893950601.py:43: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(remaining // 2, len(x)), random_state=RANDOM_SEED))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (80000, 4)\n",
      "Label distribution:\n",
      " label\n",
      "1    40000\n",
      "0    40000\n",
      "Name: count, dtype: int64\n",
      "Source distribution (top):\n",
      " source\n",
      "kaggle       76770\n",
      "chatgpt       2511\n",
      "wikipedia      411\n",
      "news           200\n",
      "arxiv          108\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [39:46<00:00, 33.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stylometric feature matrix: (80000, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>pct_punct</th>\n",
       "      <th>pct_upper</th>\n",
       "      <th>pct_digit</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>...</th>\n",
       "      <th>double_dash_ratio</th>\n",
       "      <th>colon_ratio_emphasis</th>\n",
       "      <th>semicolon_ratio_emphasis</th>\n",
       "      <th>emphasis_punctuation_ratio</th>\n",
       "      <th>hedge_phrase_count</th>\n",
       "      <th>hedge_phrase_ratio</th>\n",
       "      <th>hedge_line_ratio</th>\n",
       "      <th>generic_claim_ratio</th>\n",
       "      <th>vague_quantifier_ratio</th>\n",
       "      <th>fake_citation_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2086</td>\n",
       "      <td>380</td>\n",
       "      <td>27</td>\n",
       "      <td>14.074074</td>\n",
       "      <td>0.544737</td>\n",
       "      <td>207</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.021093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.176109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>390</td>\n",
       "      <td>29</td>\n",
       "      <td>13.448276</td>\n",
       "      <td>0.376923</td>\n",
       "      <td>147</td>\n",
       "      <td>0.038579</td>\n",
       "      <td>0.018782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.356148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>364</td>\n",
       "      <td>20</td>\n",
       "      <td>18.200001</td>\n",
       "      <td>0.478022</td>\n",
       "      <td>174</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.118083</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>62.404022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>702</td>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>85</td>\n",
       "      <td>0.027066</td>\n",
       "      <td>0.021368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.102501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1182</td>\n",
       "      <td>202</td>\n",
       "      <td>10</td>\n",
       "      <td>20.200001</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>120</td>\n",
       "      <td>0.021151</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.081505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_chars  num_words  num_sentences  avg_sentence_length  type_token_ratio  \\\n",
       "0       2086        380             27            14.074074          0.544737   \n",
       "1       1970        390             29            13.448276          0.376923   \n",
       "2       2024        364             20            18.200001          0.478022   \n",
       "3        702        113              5            22.600000          0.752212   \n",
       "4       1182        202             10            20.200001          0.594059   \n",
       "\n",
       "   unique_words  pct_punct  pct_upper  pct_digit  flesch_reading_ease  ...  \\\n",
       "0           207   0.033557   0.021093   0.000000            66.176109  ...   \n",
       "1           147   0.038579   0.018782   0.000000            73.356148  ...   \n",
       "2           174   0.027174   0.118083   0.002964            62.404022  ...   \n",
       "3            85   0.027066   0.021368   0.000000            35.102501  ...   \n",
       "4           120   0.021151   0.016920   0.000000            56.081505  ...   \n",
       "\n",
       "   double_dash_ratio  colon_ratio_emphasis  semicolon_ratio_emphasis  \\\n",
       "0           0.000000              0.000000                  0.000000   \n",
       "1           0.000000              0.000000                  0.000000   \n",
       "2           0.000000              0.000494                  0.000494   \n",
       "3           0.001425              0.000000                  0.000000   \n",
       "4           0.000000              0.000000                  0.000000   \n",
       "\n",
       "   emphasis_punctuation_ratio  hedge_phrase_count  hedge_phrase_ratio  \\\n",
       "0                    0.000000                   0                 0.0   \n",
       "1                    0.000000                   0                 0.0   \n",
       "2                    0.000988                   0                 0.0   \n",
       "3                    0.002849                   0                 0.0   \n",
       "4                    0.000000                   0                 0.0   \n",
       "\n",
       "   hedge_line_ratio  generic_claim_ratio  vague_quantifier_ratio  \\\n",
       "0               0.0                  0.0                     0.0   \n",
       "1               0.0                  0.0                     1.0   \n",
       "2               0.0                  0.0                     1.0   \n",
       "3               0.0                  0.0                     0.0   \n",
       "4               0.0                  0.0                     1.0   \n",
       "\n",
       "   fake_citation_ratio  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "MAX_LINES_FOR_NEW_FEATURES = 5\n",
    "SAMPLE_N = 80000               #start here; increase later if needed\n",
    "\n",
    "# ---- 1) Create a stratified sample (label-balanced + length-bin coverage) ----\n",
    "# Length binning based on character length tertiles (computed on the full df, but only uses text length)\n",
    "char_len = df[\"text\"].astype(str).str.len()\n",
    "\n",
    "q1, q2 = char_len.quantile([1/3, 2/3]).values\n",
    "length_bin = pd.cut(char_len, bins=[-np.inf, q1, q2, np.inf], labels=[\"short\", \"medium\", \"long\"])\n",
    "\n",
    "# Add a temp bin column (small metadata, OK)\n",
    "df_tmp = df.copy()\n",
    "df_tmp[\"length_bin_tmp\"] = length_bin.astype(str)\n",
    "\n",
    "# Stratify by (label, length_bin)\n",
    "group_cols = [\"label\", \"length_bin_tmp\"]\n",
    "groups = df_tmp.groupby(group_cols, group_keys=False)\n",
    "\n",
    "n_groups = groups.ngroups\n",
    "per_group = SAMPLE_N // n_groups\n",
    "\n",
    "# Sample per group (balanced across label + bins)\n",
    "parts = []\n",
    "for key, g in groups:\n",
    "    take = min(per_group, len(g))\n",
    "    parts.append(g.sample(n=take, random_state=RANDOM_SEED))\n",
    "\n",
    "df_sample = pd.concat(parts, axis=0).sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# If rounding left us short, top up with label-stratified sampling\n",
    "remaining = SAMPLE_N - len(df_sample)\n",
    "if remaining > 0:\n",
    "    df_rest = df_tmp.drop(df_sample.index, errors=\"ignore\")\n",
    "    topup = (\n",
    "        df_rest.groupby(\"label\", group_keys=False)\n",
    "        .apply(lambda x: x.sample(n=min(remaining // 2, len(x)), random_state=RANDOM_SEED))\n",
    "    )\n",
    "    df_sample = pd.concat([df_sample, topup], axis=0).sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# Keep only needed columns (avoid carrying temp column forward)\n",
    "df_sample = df_sample[[\"text\", \"label\", \"doc_id\", \"source\"]].copy()\n",
    "\n",
    "print(\"Sample shape:\", df_sample.shape)\n",
    "print(\"Label distribution:\\n\", df_sample[\"label\"].value_counts())\n",
    "print(\"Source distribution (top):\\n\", df_sample[\"source\"].value_counts().head())\n",
    "\n",
    "#2) Compute features on the sample only\n",
    "#Returns one dict per row → DataFrame\n",
    "features_series = df_sample[\"text\"].progress_apply(\n",
    "    lambda t: compute_all_features_final(t, max_lines=MAX_LINES_FOR_NEW_FEATURES)\n",
    ")\n",
    "\n",
    "X_style = pd.DataFrame(list(features_series))\n",
    "\n",
    "# Optional: cast to float32 to reduce memory\n",
    "for c in X_style.columns:\n",
    "    if X_style[c].dtype == \"float64\":\n",
    "        X_style[c] = X_style[c].astype(\"float32\")\n",
    "\n",
    "print(\"Stylometric feature matrix:\", X_style.shape)\n",
    "display(X_style.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "835becab-519f-432f-bef5-31b11e4ded1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 55999 8000 16001\n",
      "Label dist train: [27999 28000]\n",
      "Label dist val:   [4000 4000]\n",
      "Label dist test:  [8001 8000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 300 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=300).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Logistic Regression (TF-IDF + Style) ====================\n",
      "VAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9872    0.9850    0.9861      4000\n",
      "           1     0.9850    0.9872    0.9861      4000\n",
      "\n",
      "    accuracy                         0.9861      8000\n",
      "   macro avg     0.9861    0.9861    0.9861      8000\n",
      "weighted avg     0.9861    0.9861    0.9861      8000\n",
      "\n",
      "TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9858    0.9861    0.9859      8001\n",
      "           1     0.9861    0.9858    0.9859      8000\n",
      "\n",
      "    accuracy                         0.9859     16001\n",
      "   macro avg     0.9859    0.9859    0.9859     16001\n",
      "weighted avg     0.9859    0.9859    0.9859     16001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Linear SVM (TF-IDF + Style) ====================\n",
      "VAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9970    0.9965    0.9967      4000\n",
      "           1     0.9965    0.9970    0.9968      4000\n",
      "\n",
      "    accuracy                         0.9968      8000\n",
      "   macro avg     0.9968    0.9968    0.9967      8000\n",
      "weighted avg     0.9968    0.9968    0.9967      8000\n",
      "\n",
      "TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9962    0.9954    0.9958      8001\n",
      "           1     0.9954    0.9962    0.9958      8000\n",
      "\n",
      "    accuracy                         0.9958     16001\n",
      "   macro avg     0.9958    0.9958    0.9958     16001\n",
      "weighted avg     0.9958    0.9958    0.9958     16001\n",
      "\n",
      "\n",
      "==================== Random Forest (TF-IDF + Style via SVD) ====================\n",
      "VAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9817    0.9902    0.9859      4000\n",
      "           1     0.9902    0.9815    0.9858      4000\n",
      "\n",
      "    accuracy                         0.9859      8000\n",
      "   macro avg     0.9859    0.9859    0.9859      8000\n",
      "weighted avg     0.9859    0.9859    0.9859      8000\n",
      "\n",
      "TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9851    0.9885    0.9868      8001\n",
      "           1     0.9885    0.9850    0.9867      8000\n",
      "\n",
      "    accuracy                         0.9868     16001\n",
      "   macro avg     0.9868    0.9868    0.9868     16001\n",
      "weighted avg     0.9868    0.9868    0.9868     16001\n",
      "\n",
      "\n",
      "TF-IDF feature count: 575408\n",
      "Example TF-IDF features: ['00' '00 00' '00 000' '00 along' '00 am' '00 and' '00 fine' '00 fins'\n",
      " '00 gallon' '00 in' '00 on' '00 or' '00 out' '00 people' '00 per' '00 pm'\n",
      " '00 the' '00 then' '00 to' '000' '000 000' '000 accidents'\n",
      " '000 additionally' '000 along' '000 although']\n",
      "Total hybrid feature count: 575441\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FINAL: \"Together\" pipeline for TF-IDF(text) + Stylometry(features)\n",
    "# Trains LR, LinearSVC (full sparse), and KNN/RF (SVD-compressed but still TF-IDF+style).\n",
    "#\n",
    "# Requires:\n",
    "#   df_sample: has columns [\"text\",\"label\", ...]\n",
    "#   X_style:   DataFrame of your computed stylometric features, row-aligned with df_sample\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.20\n",
    "VAL_SIZE  = 0.10  # of total df_sample\n",
    "\n",
    "# -------------------------\n",
    "# 0) Merge df_sample + X_style into one DataFrame (so ColumnTransformer can use both)\n",
    "# -------------------------\n",
    "assert len(df_sample) == len(X_style), \"df_sample and X_style must be row-aligned and same length.\"\n",
    "df_all = pd.concat([df_sample.reset_index(drop=True), X_style.reset_index(drop=True)], axis=1)\n",
    "\n",
    "text_col = \"text\"\n",
    "y = df_all[\"label\"].astype(int).values\n",
    "style_cols = list(X_style.columns)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Random, stratified train/val/test split\n",
    "# -------------------------\n",
    "idx = np.arange(len(df_all))\n",
    "\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    idx,\n",
    "    test_size=(TEST_SIZE + VAL_SIZE),\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "val_frac_of_temp = VAL_SIZE / (VAL_SIZE + TEST_SIZE)\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=(1 - val_frac_of_temp),\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y[temp_idx]\n",
    ")\n",
    "\n",
    "train_df = df_all.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = df_all.iloc[val_idx].reset_index(drop=True)\n",
    "test_df  = df_all.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "y_train = train_df[\"label\"].astype(int).values\n",
    "y_val   = val_df[\"label\"].astype(int).values\n",
    "y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n",
    "print(\"Label dist train:\", np.bincount(y_train))\n",
    "print(\"Label dist val:  \", np.bincount(y_val))\n",
    "print(\"Label dist test: \", np.bincount(y_test))\n",
    "\n",
    "# -------------------------\n",
    "# 2) Shared feature preprocessor: TF-IDF(text) + scaled style features\n",
    "#    - TF-IDF output is sparse\n",
    "#    - StandardScaler(with_mean=False) is required for sparse compatibility\n",
    "# -------------------------\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tfidf\", tfidf, text_col),\n",
    "        (\"style\", StandardScaler(with_mean=False), style_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3  # keep output sparse when possible\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Models\n",
    "#    LR + LinearSVC can handle the full sparse matrix\n",
    "#    KNN + RF will be trained on a compressed representation via TruncatedSVD\n",
    "# -------------------------\n",
    "def fit_eval(name, pipe):\n",
    "    pipe.fit(train_df, y_train)\n",
    "    print(f\"\\n==================== {name} ====================\")\n",
    "    print(\"VAL:\")\n",
    "    print(classification_report(y_val, pipe.predict(val_df), digits=4))\n",
    "    print(\"TEST:\")\n",
    "    print(classification_report(y_test, pipe.predict(test_df), digits=4))\n",
    "    return pipe\n",
    "\n",
    "# 3A) Logistic Regression (FULL sparse TF-IDF + style)\n",
    "lr_pipe = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(max_iter=300, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# 3B) Linear SVM (FULL sparse TF-IDF + style)\n",
    "svm_pipe = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# For KNN/RF: SVD makes it dense and manageable while still using TF-IDF info\n",
    "SVD_DIM = 300  # 200-500 is a reasonable range; increase if you want more text signal\n",
    "\n",
    "# knn_pipe = Pipeline([\n",
    "#     (\"features\", preprocessor),\n",
    "#     (\"svd\", TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_SEED)),\n",
    "#     (\"scale_dense\", StandardScaler()),  # now dense, safe to mean-center\n",
    "#     (\"clf\", KNeighborsClassifier(n_neighbors=15, weights=\"distance\"))\n",
    "# ])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"svd\", TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_SEED)),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=RANDOM_SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_pipe  = fit_eval(\"Logistic Regression (TF-IDF + Style)\", lr_pipe)\n",
    "svm_pipe = fit_eval(\"Linear SVM (TF-IDF + Style)\", svm_pipe)\n",
    "# knn_pipe = fit_eval(\"KNN (TF-IDF + Style via SVD)\", knn_pipe)\n",
    "rf_pipe  = fit_eval(\"Random Forest (TF-IDF + Style via SVD)\", rf_pipe)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Getting TF-IDF vector feature names (ngram vocabulary)\n",
    "#    After fitting, you can inspect what TF-IDF \"features\" correspond to.\n",
    "# -------------------------\n",
    "tfidf_fitted = lr_pipe.named_steps[\"features\"].named_transformers_[\"tfidf\"]\n",
    "tfidf_feature_names = tfidf_fitted.get_feature_names_out()\n",
    "\n",
    "print(\"\\nTF-IDF feature count:\", len(tfidf_feature_names))\n",
    "print(\"Example TF-IDF features:\", tfidf_feature_names[:25])\n",
    "\n",
    "# If you want full hybrid feature names (TF-IDF + style), you can combine manually:\n",
    "hybrid_feature_names = np.concatenate([tfidf_feature_names, np.array(style_cols, dtype=object)])\n",
    "print(\"Total hybrid feature count:\", len(hybrid_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "662d2cf6-872f-445b-bd39-15d22a294bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within df_sample:\n",
      "  Kaggle rows : 76770\n",
      "  Scraped rows: 3230\n",
      "  Dropped scraped overlaps vs Kaggle: 0\n",
      "  Final TEST rows (scraped, deduped): 3230\n",
      "\n",
      "Text-hash overlaps (should be 0):\n",
      "  TRAIN ∩ VAL : 126\n",
      "  TRAIN ∩ TEST: 0\n",
      "  VAL   ∩ TEST: 0\n",
      "\n",
      "Split sizes: 65254 11516 3230\n",
      "Label dist TRAIN: [33389 31865]\n",
      "Label dist VAL:   [5892 5624]\n",
      "Label dist TEST:  [ 719 2511]\n"
     ]
    }
   ],
   "source": [
    "#Simulating real world scenario by pre training on ML data and testing on real-life data to see if model training fails\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# STRICT SPLIT: Train/Val on Kaggle ONLY, Test on Scraped ONLY (from df_sample)\n",
    "# - Uses df_sample as the universe (as you requested)\n",
    "# - Keeps X_style aligned (no recompute)\n",
    "# - Removes any scraped rows whose normalized-text hash appears in Kaggle (within df_sample)\n",
    "#\n",
    "# Outputs:\n",
    "#   train_df, val_df, test_df\n",
    "#   X_style_train, X_style_val, X_style_test\n",
    "#   y_train, y_val, y_test\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, hashlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "VAL_SIZE = 0.15   # fraction of Kaggle portion used for validation (e.g., 15%)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "#anity checks\n",
    "assert \"source\" in df_sample.columns, \"df_sample must have a 'source' column.\"\n",
    "assert \"text\" in df_sample.columns and \"label\" in df_sample.columns, \"df_sample must have 'text' and 'label'.\"\n",
    "assert len(df_sample) == len(X_style), \"df_sample and X_style must be row-aligned and same length.\"\n",
    "\n",
    "#helper: text hash for overlap removal\n",
    "_ws = re.compile(r\"\\s+\")\n",
    "def norm_for_hash(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    return _ws.sub(\" \", s.strip().lower())\n",
    "\n",
    "def text_hash(s: str) -> str:\n",
    "    return hashlib.sha1(norm_for_hash(s).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "#1) Get Kaggle indices and Scraped indices (within df_sample)\n",
    "all_idx = np.arange(len(df_sample))\n",
    "k_idx = all_idx[df_sample[\"source\"].astype(str).values == \"kaggle\"]\n",
    "s_idx = all_idx[df_sample[\"source\"].astype(str).values != \"kaggle\"]\n",
    "\n",
    "print(\"Within df_sample:\")\n",
    "print(\"  Kaggle rows :\", len(k_idx))\n",
    "print(\"  Scraped rows:\", len(s_idx))\n",
    "\n",
    "if len(k_idx) == 0:\n",
    "    raise ValueError(\"No Kaggle rows found inside df_sample. Cannot do Kaggle-only train/val.\")\n",
    "if len(s_idx) == 0:\n",
    "    raise ValueError(\"No scraped rows found inside df_sample. Cannot do scraped-only test.\")\n",
    "\n",
    "# 2) Remove Kaggle↔Scraped overlaps (within df_sample)\n",
    "# Hash Kaggle texts\n",
    "k_text = df_sample.loc[k_idx, \"text\"].astype(str)\n",
    "k_hash_set = set(k_text.map(text_hash).values)\n",
    "\n",
    "#Hash scraped texts and keep only non-overlapping\n",
    "s_text = df_sample.loc[s_idx, \"text\"].astype(str)\n",
    "s_hash = s_text.map(text_hash)\n",
    "keep_scraped = ~s_hash.isin(k_hash_set)\n",
    "\n",
    "test_idx = s_idx[keep_scraped.values]\n",
    "dropped = int((~keep_scraped).sum())\n",
    "\n",
    "print(f\"  Dropped scraped overlaps vs Kaggle: {dropped}\")\n",
    "print(f\"  Final TEST rows (scraped, deduped): {len(test_idx)}\")\n",
    "\n",
    "if len(test_idx) == 0:\n",
    "    raise ValueError(\"After overlap removal, no scraped rows remain for TEST. Increase df_sample size or change sampling.\")\n",
    "\n",
    "#3) Train/Val split ONLY on Kaggle (stratified by label)\n",
    "k_labels = df_sample.loc[k_idx, \"label\"].astype(int).values\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    k_idx,\n",
    "    test_size=VAL_SIZE,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=k_labels\n",
    ")\n",
    "\n",
    "#4) Slice df_sample + X_style together (keeps alignment)\n",
    "train_df = df_sample.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = df_sample.iloc[val_idx].reset_index(drop=True)\n",
    "test_df  = df_sample.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "X_style_train = X_style.iloc[train_idx].reset_index(drop=True)\n",
    "X_style_val   = X_style.iloc[val_idx].reset_index(drop=True)\n",
    "X_style_test  = X_style.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "y_train = train_df[\"label\"].astype(int).values\n",
    "y_val   = val_df[\"label\"].astype(int).values\n",
    "y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "# Hash overlap checks\n",
    "train_hashes = set(train_df[\"text\"].astype(str).map(text_hash).values)\n",
    "val_hashes   = set(val_df[\"text\"].astype(str).map(text_hash).values)\n",
    "test_hashes  = set(test_df[\"text\"].astype(str).map(text_hash).values)\n",
    "\n",
    "print(\"\\nText-hash overlaps (should be 0):\")\n",
    "print(\"  TRAIN ∩ VAL :\", len(train_hashes & val_hashes))\n",
    "print(\"  TRAIN ∩ TEST:\", len(train_hashes & test_hashes))\n",
    "print(\"  VAL   ∩ TEST:\", len(val_hashes & test_hashes))\n",
    "\n",
    "print(\"\\nSplit sizes:\", len(train_df), len(val_df), len(test_df))\n",
    "print(\"Label dist TRAIN:\", np.bincount(y_train))\n",
    "print(\"Label dist VAL:  \", np.bincount(y_val))\n",
    "print(\"Label dist TEST: \", np.bincount(y_test))\n",
    "\n",
    "#Kaggle-only TRAIN/VAL, Scraped-only TEST, with deduping and aligned X_style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce3b1bf0-0788-4859-8da9-4230b4fd72df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 300 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=300).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== LR (TF-IDF + Style) ====================\n",
      "VAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9867    0.9922    0.9894      5892\n",
      "           1     0.9918    0.9860    0.9889      5624\n",
      "\n",
      "    accuracy                         0.9891     11516\n",
      "   macro avg     0.9892    0.9891    0.9891     11516\n",
      "weighted avg     0.9892    0.9891    0.9891     11516\n",
      "\n",
      "TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9681    0.1266    0.2239       719\n",
      "           1     0.7997    0.9988    0.8883      2511\n",
      "\n",
      "    accuracy                         0.8046      3230\n",
      "   macro avg     0.8839    0.5627    0.5561      3230\n",
      "weighted avg     0.8372    0.8046    0.7404      3230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Linear SVM (TF-IDF + Style) ====================\n",
      "VAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9966    0.9988    0.9977      5892\n",
      "           1     0.9988    0.9964    0.9976      5624\n",
      "\n",
      "    accuracy                         0.9977     11516\n",
      "   macro avg     0.9977    0.9976    0.9977     11516\n",
      "weighted avg     0.9977    0.9977    0.9977     11516\n",
      "\n",
      "TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9600    0.0668    0.1248       719\n",
      "           1     0.7890    0.9992    0.8817      2511\n",
      "\n",
      "    accuracy                         0.7916      3230\n",
      "   macro avg     0.8745    0.5330    0.5033      3230\n",
      "weighted avg     0.8271    0.7916    0.7133      3230\n",
      "\n",
      "\n",
      "==================== RF (TF-IDF + Style via SVD=300) ====================\n",
      "VAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9817    0.9936    0.9876      5892\n",
      "           1     0.9932    0.9806    0.9868      5624\n",
      "\n",
      "    accuracy                         0.9872     11516\n",
      "   macro avg     0.9874    0.9871    0.9872     11516\n",
      "weighted avg     0.9873    0.9872    0.9872     11516\n",
      "\n",
      "TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3575    0.1780    0.2377       719\n",
      "           1     0.7942    0.9084    0.8475      2511\n",
      "\n",
      "    accuracy                         0.7458      3230\n",
      "   macro avg     0.5759    0.5432    0.5426      3230\n",
      "weighted avg     0.6970    0.7458    0.7117      3230\n",
      "\n",
      "\n",
      "=== Compact Results Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR (TF-IDF + Style)</td>\n",
       "      <td>TEST</td>\n",
       "      <td>0.8046</td>\n",
       "      <td>0.7997</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.8883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVM (TF-IDF + Style)</td>\n",
       "      <td>TEST</td>\n",
       "      <td>0.7916</td>\n",
       "      <td>0.7890</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.8817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF (TF-IDF + Style via SVD=300)</td>\n",
       "      <td>TEST</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>0.8475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM (TF-IDF + Style)</td>\n",
       "      <td>VAL</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR (TF-IDF + Style)</td>\n",
       "      <td>VAL</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.9889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF (TF-IDF + Style via SVD=300)</td>\n",
       "      <td>VAL</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model split  accuracy  precision  recall      f1\n",
       "1              LR (TF-IDF + Style)  TEST    0.8046     0.7997  0.9988  0.8883\n",
       "3      Linear SVM (TF-IDF + Style)  TEST    0.7916     0.7890  0.9992  0.8817\n",
       "5  RF (TF-IDF + Style via SVD=300)  TEST    0.7458     0.7942  0.9084  0.8475\n",
       "2      Linear SVM (TF-IDF + Style)   VAL    0.9977     0.9988  0.9964  0.9976\n",
       "0              LR (TF-IDF + Style)   VAL    0.9891     0.9918  0.9860  0.9889\n",
       "4  RF (TF-IDF + Style via SVD=300)   VAL    0.9872     0.9932  0.9806  0.9868"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔴 Top 25 AI-indicative features (positive coef)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276195</th>\n",
       "      <td>important</td>\n",
       "      <td>4.806169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189070</th>\n",
       "      <td>essay</td>\n",
       "      <td>3.963152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276743</th>\n",
       "      <td>important to</td>\n",
       "      <td>3.434541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430404</th>\n",
       "      <td>potential</td>\n",
       "      <td>3.134831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268206</th>\n",
       "      <td>however</td>\n",
       "      <td>2.893413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301196</th>\n",
       "      <td>it like</td>\n",
       "      <td>2.890956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525603</th>\n",
       "      <td>super</td>\n",
       "      <td>2.780419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77668</th>\n",
       "      <td>believe that</td>\n",
       "      <td>2.667229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13344</th>\n",
       "      <td>additionally</td>\n",
       "      <td>2.636651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626588</th>\n",
       "      <td>who</td>\n",
       "      <td>2.596545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525975</th>\n",
       "      <td>support</td>\n",
       "      <td>2.595658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316057</th>\n",
       "      <td>lead to</td>\n",
       "      <td>2.574994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443195</th>\n",
       "      <td>provide</td>\n",
       "      <td>2.557127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565419</th>\n",
       "      <td>this essay</td>\n",
       "      <td>2.493005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374607</th>\n",
       "      <td>number of</td>\n",
       "      <td>2.470063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450019</th>\n",
       "      <td>re</td>\n",
       "      <td>2.460343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301427</th>\n",
       "      <td>it not</td>\n",
       "      <td>2.455330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315937</th>\n",
       "      <td>lead</td>\n",
       "      <td>2.417049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301016</th>\n",
       "      <td>it important</td>\n",
       "      <td>2.402594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361475</th>\n",
       "      <td>name</td>\n",
       "      <td>2.369398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316575</th>\n",
       "      <td>learn</td>\n",
       "      <td>2.314098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374523</th>\n",
       "      <td>number</td>\n",
       "      <td>2.305670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134450</th>\n",
       "      <td>cool</td>\n",
       "      <td>2.286976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296497</th>\n",
       "      <td>is important</td>\n",
       "      <td>2.286847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260637</th>\n",
       "      <td>hey</td>\n",
       "      <td>2.230826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  coefficient\n",
       "276195     important     4.806169\n",
       "189070         essay     3.963152\n",
       "276743  important to     3.434541\n",
       "430404     potential     3.134831\n",
       "268206       however     2.893413\n",
       "301196       it like     2.890956\n",
       "525603         super     2.780419\n",
       "77668   believe that     2.667229\n",
       "13344   additionally     2.636651\n",
       "626588           who     2.596545\n",
       "525975       support     2.595658\n",
       "316057       lead to     2.574994\n",
       "443195       provide     2.557127\n",
       "565419    this essay     2.493005\n",
       "374607     number of     2.470063\n",
       "450019            re     2.460343\n",
       "301427        it not     2.455330\n",
       "315937          lead     2.417049\n",
       "301016  it important     2.402594\n",
       "361475          name     2.369398\n",
       "316575         learn     2.314098\n",
       "374523        number     2.305670\n",
       "134450          cool     2.286976\n",
       "296497  is important     2.286847\n",
       "260637           hey     2.230826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 Top 25 Human-indicative features (negative coef)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71869</th>\n",
       "      <td>because</td>\n",
       "      <td>-7.624013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639162</th>\n",
       "      <td>would</td>\n",
       "      <td>-4.456290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605753</th>\n",
       "      <td>very</td>\n",
       "      <td>-4.038457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338218</th>\n",
       "      <td>many</td>\n",
       "      <td>-3.513801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179054</th>\n",
       "      <td>electors</td>\n",
       "      <td>-3.496044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408677</th>\n",
       "      <td>paragraph</td>\n",
       "      <td>-3.462419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100313</th>\n",
       "      <td>car</td>\n",
       "      <td>-3.347272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102739</th>\n",
       "      <td>cars</td>\n",
       "      <td>-3.302405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609493</th>\n",
       "      <td>vote for</td>\n",
       "      <td>-3.250699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519447</th>\n",
       "      <td>students</td>\n",
       "      <td>-3.162122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604868</th>\n",
       "      <td>venus</td>\n",
       "      <td>-3.069457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72355</th>\n",
       "      <td>because of</td>\n",
       "      <td>-2.993116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643835</th>\n",
       "      <td>you</td>\n",
       "      <td>-2.982177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416025</th>\n",
       "      <td>percent</td>\n",
       "      <td>-2.880035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610803</th>\n",
       "      <td>voting</td>\n",
       "      <td>-2.877566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201121</th>\n",
       "      <td>extracurricular</td>\n",
       "      <td>-2.844885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551512</th>\n",
       "      <td>the students</td>\n",
       "      <td>-2.841146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489469</th>\n",
       "      <td>should</td>\n",
       "      <td>-2.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535467</th>\n",
       "      <td>technology</td>\n",
       "      <td>-2.807183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269811</th>\n",
       "      <td>humans</td>\n",
       "      <td>-2.764026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178863</th>\n",
       "      <td>electoral</td>\n",
       "      <td>-2.760131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433851</th>\n",
       "      <td>president</td>\n",
       "      <td>-2.737591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26495</th>\n",
       "      <td>although</td>\n",
       "      <td>-2.668173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123279</th>\n",
       "      <td>community service</td>\n",
       "      <td>-2.625142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609247</th>\n",
       "      <td>vote</td>\n",
       "      <td>-2.564777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  coefficient\n",
       "71869             because    -7.624013\n",
       "639162              would    -4.456290\n",
       "605753               very    -4.038457\n",
       "338218               many    -3.513801\n",
       "179054           electors    -3.496044\n",
       "408677          paragraph    -3.462419\n",
       "100313                car    -3.347272\n",
       "102739               cars    -3.302405\n",
       "609493           vote for    -3.250699\n",
       "519447           students    -3.162122\n",
       "604868              venus    -3.069457\n",
       "72355          because of    -2.993116\n",
       "643835                you    -2.982177\n",
       "416025            percent    -2.880035\n",
       "610803             voting    -2.877566\n",
       "201121    extracurricular    -2.844885\n",
       "551512       the students    -2.841146\n",
       "489469             should    -2.828001\n",
       "535467         technology    -2.807183\n",
       "269811             humans    -2.764026\n",
       "178863          electoral    -2.760131\n",
       "433851          president    -2.737591\n",
       "26495            although    -2.668173\n",
       "123279  community service    -2.625142\n",
       "609247               vote    -2.564777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Stylometric features pushing AI prediction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>648784</th>\n",
       "      <td>gunning_fog</td>\n",
       "      <td>1.945949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648795</th>\n",
       "      <td>emdash_ratio</td>\n",
       "      <td>1.862866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648785</th>\n",
       "      <td>bigram_repetition_ratio</td>\n",
       "      <td>1.429472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648777</th>\n",
       "      <td>type_token_ratio</td>\n",
       "      <td>1.020520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648779</th>\n",
       "      <td>pct_punct</td>\n",
       "      <td>0.777256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648774</th>\n",
       "      <td>num_words</td>\n",
       "      <td>0.622144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648778</th>\n",
       "      <td>unique_words</td>\n",
       "      <td>0.416999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648788</th>\n",
       "      <td>pos_ratio_ADJ</td>\n",
       "      <td>0.398344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648790</th>\n",
       "      <td>pos_ratio_PRON</td>\n",
       "      <td>0.366413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648799</th>\n",
       "      <td>emphasis_punctuation_ratio</td>\n",
       "      <td>0.206270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648791</th>\n",
       "      <td>pos_ratio_ADP</td>\n",
       "      <td>0.136815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648797</th>\n",
       "      <td>colon_ratio_emphasis</td>\n",
       "      <td>0.108274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648786</th>\n",
       "      <td>pos_ratio_NOUN</td>\n",
       "      <td>0.043955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648789</th>\n",
       "      <td>pos_ratio_ADV</td>\n",
       "      <td>0.025334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648803</th>\n",
       "      <td>generic_claim_ratio</td>\n",
       "      <td>0.025098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature  coefficient\n",
       "648784                 gunning_fog     1.945949\n",
       "648795                emdash_ratio     1.862866\n",
       "648785     bigram_repetition_ratio     1.429472\n",
       "648777            type_token_ratio     1.020520\n",
       "648779                   pct_punct     0.777256\n",
       "648774                   num_words     0.622144\n",
       "648778                unique_words     0.416999\n",
       "648788               pos_ratio_ADJ     0.398344\n",
       "648790              pos_ratio_PRON     0.366413\n",
       "648799  emphasis_punctuation_ratio     0.206270\n",
       "648791               pos_ratio_ADP     0.136815\n",
       "648797        colon_ratio_emphasis     0.108274\n",
       "648786              pos_ratio_NOUN     0.043955\n",
       "648789               pos_ratio_ADV     0.025334\n",
       "648803         generic_claim_ratio     0.025098"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Stylometric features pushing Human prediction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>648782</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>-1.757937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648783</th>\n",
       "      <td>flesch_kincaid_grade</td>\n",
       "      <td>-1.735075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648793</th>\n",
       "      <td>sentence_length_std</td>\n",
       "      <td>-1.157718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648775</th>\n",
       "      <td>num_sentences</td>\n",
       "      <td>-1.046652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648776</th>\n",
       "      <td>avg_sentence_length</td>\n",
       "      <td>-1.030368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648773</th>\n",
       "      <td>num_chars</td>\n",
       "      <td>-0.581812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648781</th>\n",
       "      <td>pct_digit</td>\n",
       "      <td>-0.231864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648792</th>\n",
       "      <td>pos_ratio_DET</td>\n",
       "      <td>-0.231411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648798</th>\n",
       "      <td>semicolon_ratio_emphasis</td>\n",
       "      <td>-0.227237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648794</th>\n",
       "      <td>sentence_length_mean</td>\n",
       "      <td>-0.202468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648780</th>\n",
       "      <td>pct_upper</td>\n",
       "      <td>-0.137292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648796</th>\n",
       "      <td>double_dash_ratio</td>\n",
       "      <td>-0.072638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648802</th>\n",
       "      <td>hedge_line_ratio</td>\n",
       "      <td>-0.026354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648804</th>\n",
       "      <td>vague_quantifier_ratio</td>\n",
       "      <td>-0.021927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648787</th>\n",
       "      <td>pos_ratio_VERB</td>\n",
       "      <td>-0.018881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         feature  coefficient\n",
       "648782       flesch_reading_ease    -1.757937\n",
       "648783      flesch_kincaid_grade    -1.735075\n",
       "648793       sentence_length_std    -1.157718\n",
       "648775             num_sentences    -1.046652\n",
       "648776       avg_sentence_length    -1.030368\n",
       "648773                 num_chars    -0.581812\n",
       "648781                 pct_digit    -0.231864\n",
       "648792             pos_ratio_DET    -0.231411\n",
       "648798  semicolon_ratio_emphasis    -0.227237\n",
       "648794      sentence_length_mean    -0.202468\n",
       "648780                 pct_upper    -0.137292\n",
       "648796         double_dash_ratio    -0.072638\n",
       "648802          hedge_line_ratio    -0.026354\n",
       "648804    vague_quantifier_ratio    -0.021927\n",
       "648787            pos_ratio_VERB    -0.018881"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# Train + Evaluate models on STRICT split:\n",
    "#   TRAIN/VAL = Kaggle only\n",
    "#   TEST      = Scraped only\n",
    "# Uses ALL features together:\n",
    "#   TF-IDF(text) + your precomputed stylometric X_style_*\n",
    "#\n",
    "# Assumes you already have from the split cell:\n",
    "#   train_df, val_df, test_df\n",
    "#   X_style_train, X_style_val, X_style_test\n",
    "#   y_train, y_val, y_test\n",
    "#\n",
    "# And you said: KNN removed ✅\n",
    "# Trains: Logistic Regression, Linear SVM, Random Forest (via SVD) all using TF-IDF+Style \"together\".\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# -------------------------\n",
    "# 0) Merge text+labels with precomputed style features (keeps alignment)\n",
    "# -------------------------\n",
    "assert len(train_df) == len(X_style_train)\n",
    "assert len(val_df)   == len(X_style_val)\n",
    "assert len(test_df)  == len(X_style_test)\n",
    "\n",
    "train_all = pd.concat([train_df.reset_index(drop=True), X_style_train.reset_index(drop=True)], axis=1)\n",
    "val_all   = pd.concat([val_df.reset_index(drop=True),   X_style_val.reset_index(drop=True)], axis=1)\n",
    "test_all  = pd.concat([test_df.reset_index(drop=True),  X_style_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "text_col = \"text\"\n",
    "style_cols = list(X_style_train.columns)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Shared preprocessor: TF-IDF + scaled style (together)\n",
    "# -------------------------\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tfidf\", tfidf, text_col),\n",
    "        (\"style\", StandardScaler(with_mean=False), style_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Pipelines (KNN removed)\n",
    "# -------------------------\n",
    "lr_pipe = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(max_iter=300, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Random Forest needs dense input -> use SVD compression (still TF-IDF + style together)\n",
    "SVD_DIM = 300  # 200-500 reasonable. Increase for more text signal.\n",
    "rf_pipe = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"svd\", TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_SEED)),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=RANDOM_SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"LR (TF-IDF + Style)\": lr_pipe,\n",
    "    \"Linear SVM (TF-IDF + Style)\": svm_pipe,\n",
    "    f\"RF (TF-IDF + Style via SVD={SVD_DIM})\": rf_pipe,\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 3) Compact results table (VAL + TEST)\n",
    "# -------------------------\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(train_all, y_train)\n",
    "\n",
    "    val_pred = model.predict(val_all)\n",
    "    test_pred = model.predict(test_all)\n",
    "\n",
    "    rows.append({\"model\": name, \"split\": \"VAL\", **eval_metrics(y_val, val_pred)})\n",
    "    rows.append({\"model\": name, \"split\": \"TEST\", **eval_metrics(y_test, test_pred)})\n",
    "\n",
    "    print(f\"\\n==================== {name} ====================\")\n",
    "    print(\"VAL:\")\n",
    "    print(classification_report(y_val, val_pred, digits=4))\n",
    "    print(\"TEST:\")\n",
    "    print(classification_report(y_test, test_pred, digits=4))\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values([\"split\", \"f1\"], ascending=[True, False])\n",
    "print(\"\\n=== Compact Results Table ===\")\n",
    "display(results_df.round(4))\n",
    "\n",
    "# -------------------------\n",
    "# 4) LR coefficient interpretation (Top AI vs Human features)\n",
    "# -------------------------\n",
    "# Only for LR (interpretable). SVM coefficients are also available but less nicely probabilistic.\n",
    "tfidf_fitted = lr_pipe.named_steps[\"features\"].named_transformers_[\"tfidf\"]\n",
    "tfidf_feature_names = tfidf_fitted.get_feature_names_out()\n",
    "\n",
    "feature_names = np.concatenate([tfidf_feature_names, np.array(style_cols, dtype=object)])\n",
    "coef = lr_pipe.named_steps[\"clf\"].coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame({\"feature\": feature_names, \"coefficient\": coef})\n",
    "\n",
    "TOPK = 25\n",
    "top_ai = coef_df.sort_values(\"coefficient\", ascending=False).head(TOPK)\n",
    "top_human = coef_df.sort_values(\"coefficient\", ascending=True).head(TOPK)\n",
    "\n",
    "print(f\"\\n🔴 Top {TOPK} AI-indicative features (positive coef)\")\n",
    "display(top_ai)\n",
    "\n",
    "print(f\"\\n🔵 Top {TOPK} Human-indicative features (negative coef)\")\n",
    "display(top_human)\n",
    "\n",
    "# Stylometry-only interpretation (often the most report-friendly)\n",
    "n_tfidf = len(tfidf_feature_names)\n",
    "style_coef_df = coef_df.iloc[n_tfidf:].copy()\n",
    "\n",
    "print(\"\\n🧠 Stylometric features pushing AI prediction\")\n",
    "display(style_coef_df.sort_values(\"coefficient\", ascending=False).head(15))\n",
    "\n",
    "print(\"\\n🧠 Stylometric features pushing Human prediction\")\n",
    "display(style_coef_df.sort_values(\"coefficient\", ascending=True).head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8734d5e-4bbc-42f7-8da9-252485b497bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocab size: 48,875 (PAD=0, UNK=1)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LSTM text classifier (CPU-friendly) for your STRICT split:\n",
    "#   TRAIN/VAL = Kaggle only\n",
    "#   TEST      = Scraped only\n",
    "#\n",
    "# Uses PyTorch. Includes:\n",
    "# - fast regex tokenization\n",
    "# - vocab built from TRAIN only\n",
    "# - truncation to max_len\n",
    "# - padding + packed sequences\n",
    "# - early stopping on VAL F1\n",
    "# - per-epoch timing (so you see how long it takes on your machine)\n",
    "#\n",
    "# Assumes these already exist:\n",
    "#   train_df, val_df, test_df\n",
    "#   y_train, y_val, y_test\n",
    "#\n",
    "# NOTE: I’m not giving a fixed time estimate (it depends heavily on CPU, max_len, vocab, batch size).\n",
    "# This code prints time/epoch + a simple projected total after epoch 1.\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# -------------------------\n",
    "# Config (tune these first)\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "MAX_VOCAB = 50_000        # 30k–100k typical\n",
    "MIN_FREQ = 2              # drop very rare tokens\n",
    "MAX_LEN = 256             # 256/384/512. Higher = slower but captures longer context\n",
    "BATCH_SIZE = 64           # 32/64/128 depending on CPU & RAM\n",
    "EMB_DIM = 192             # 128–256\n",
    "HID_DIM = 192             # 128–256\n",
    "NUM_LAYERS = 1            # 1–2 (2 slower)\n",
    "BIDIR = True\n",
    "DROPOUT = 0.2\n",
    "LR = 2e-3\n",
    "EPOCHS = 8\n",
    "PATIENCE = 2              # early stop if VAL F1 doesn't improve\n",
    "CLIP = 1.0\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer (fast, stable)\n",
    "# -------------------------\n",
    "_tok = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[^\\sA-Za-z0-9]\")\n",
    "def tokenize(text: str):\n",
    "    return _tok.findall((text or \"\").lower())\n",
    "\n",
    "# -------------------------\n",
    "# Build vocab from TRAIN only\n",
    "# -------------------------\n",
    "def build_vocab(texts, max_vocab=MAX_VOCAB, min_freq=MIN_FREQ):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(tokenize(t))\n",
    "    # Special tokens\n",
    "    itos = [\"<pad>\", \"<unk>\"]\n",
    "    # Keep most common above min_freq\n",
    "    for tok, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        itos.append(tok)\n",
    "        if len(itos) >= max_vocab:\n",
    "            break\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "train_texts = train_df[\"text\"].astype(str).tolist()\n",
    "val_texts   = val_df[\"text\"].astype(str).tolist()\n",
    "test_texts  = test_df[\"text\"].astype(str).tolist()\n",
    "\n",
    "stoi, itos = build_vocab(train_texts)\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "\n",
    "print(f\"Vocab size: {len(itos):,} (PAD={PAD_IDX}, UNK={UNK_IDX})\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + Collate\n",
    "# -------------------------\n",
    "def encode(text: str, max_len=MAX_LEN):\n",
    "    ids = [stoi.get(tok, UNK_IDX) for tok in tokenize(text)]\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        return self.texts[i], self.labels[i]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    seqs = [torch.tensor(encode(t), dtype=torch.long) for t in texts]\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "\n",
    "    # pad to max length in batch\n",
    "    maxl = int(lengths.max().item()) if len(lengths) else 1\n",
    "    padded = torch.full((len(seqs), maxl), PAD_IDX, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        padded[i, :len(s)] = s\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)  # binary\n",
    "    return padded.to(DEVICE), lengths.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "train_ds = TextDataset(train_texts, y_train)\n",
    "val_ds   = TextDataset(val_texts, y_val)\n",
    "test_ds  = TextDataset(test_texts, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_batch)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, bidir, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidir,\n",
    "            dropout=0.0 if num_layers == 1 else dropout\n",
    "        )\n",
    "        out_dim = hid_dim * (2 if bidir else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [B, T]\n",
    "        emb = self.dropout(self.embedding(x))  # [B, T, E]\n",
    "\n",
    "        # pack sequences for speed\n",
    "        lengths_cpu = lengths.to(\"cpu\")\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths_cpu, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed)\n",
    "\n",
    "        # h shape: [num_layers * num_directions, B, H]\n",
    "        if self.lstm.bidirectional:\n",
    "            # last layer forward + backward\n",
    "            h_f = h[-2, :, :]\n",
    "            h_b = h[-1, :, :]\n",
    "            h_cat = torch.cat([h_f, h_b], dim=1)  # [B, 2H]\n",
    "        else:\n",
    "            h_cat = h[-1, :, :]  # [B, H]\n",
    "\n",
    "        logits = self.fc(self.dropout(h_cat)).squeeze(1)  # [B]\n",
    "        return logits\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=len(itos),\n",
    "    emb_dim=EMB_DIM,\n",
    "    hid_dim=HID_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidir=BIDIR,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# -------------------------\n",
    "# Metrics helpers\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def predict_loader(loader):\n",
    "    model.eval()\n",
    "    all_probs, all_y = [], []\n",
    "    for x, lengths, y in loader:\n",
    "        logits = model(x, lengths)\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_y.append(y.detach().cpu().numpy())\n",
    "    probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
    "    ytrue = np.concatenate(all_y).astype(int) if all_y else np.array([], dtype=int)\n",
    "    return probs, ytrue\n",
    "\n",
    "def eval_split(loader, threshold=0.5):\n",
    "    probs, ytrue = predict_loader(loader)\n",
    "    ypred = (probs >= threshold).astype(int)\n",
    "    acc = accuracy_score(ytrue, ypred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(ytrue, ypred, average=\"binary\", zero_division=0)\n",
    "    return acc, p, r, f1\n",
    "\n",
    "# -------------------------\n",
    "# Train loop with timing + early stopping\n",
    "# -------------------------\n",
    "best_val_f1 = -1.0\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "epoch_times = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x, lengths, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, lengths)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += float(loss.item())\n",
    "        n_batches += 1\n",
    "\n",
    "    train_loss = running_loss / max(n_batches, 1)\n",
    "    val_acc, val_p, val_r, val_f1 = eval_split(val_loader)\n",
    "\n",
    "    t1 = time.time()\n",
    "    epoch_sec = t1 - t0\n",
    "    epoch_times.append(epoch_sec)\n",
    "\n",
    "    # After epoch 1, print a rough projection based on observed time/epoch\n",
    "    if epoch == 1:\n",
    "        projected = epoch_sec * EPOCHS\n",
    "        print(f\"\\n[Timing] Epoch 1 took {epoch_sec:.1f}s. Rough projected total for {EPOCHS} epochs: ~{projected/60:.1f} min (before early stopping).\")\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} | \"\n",
    "          f\"VAL acc={val_acc:.4f} p={val_p:.4f} r={val_r:.4f} f1={val_f1:.4f} | \"\n",
    "          f\"time={epoch_sec:.1f}s\")\n",
    "\n",
    "    # Early stopping on VAL F1\n",
    "    if val_f1 > best_val_f1 + 1e-4:\n",
    "        best_val_f1 = val_f1\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping: no VAL F1 improvement for {PATIENCE} epoch(s).\")\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# -------------------------\n",
    "# Final evaluation\n",
    "# -------------------------\n",
    "val_acc, val_p, val_r, val_f1 = eval_split(val_loader)\n",
    "test_acc, test_p, test_r, test_f1 = eval_split(test_loader)\n",
    "\n",
    "print(\"\\n===== FINAL (best checkpoint) =====\")\n",
    "print(f\"VAL  acc={val_acc:.4f} p={val_p:.4f} r={val_r:.4f} f1={val_f1:.4f}\")\n",
    "print(f\"TEST acc={test_acc:.4f} p={test_p:.4f} r={test_r:.4f} f1={test_f1:.4f}\")\n",
    "print(f\"Avg time/epoch: {np.mean(epoch_times):.1f}s over {len(epoch_times)} epoch(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53328ad-e8f8-466b-960c-f2b1a8f1881e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
