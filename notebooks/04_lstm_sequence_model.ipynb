{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/04_lstm_sequence_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Sequence Model\n",
        "\n",
        "**Objective:**\n",
        "Train a Deep Learning sequence model (LSTM) to detect AI-generated text. Unlike the feature-based models in Notebook 3 which relied on manual counts (like sentence length), this model learns patterns directly from the raw sequence of words.\n",
        "\n",
        "**Key Components:**\n",
        "1.  **Load Fixed Splits:** Import the pre-split data (`train.csv`, `val.csv`, `test.csv`) saved in Notebook 3 to ensure we are testing on the exact same \"Collected\" dataset.\n",
        "2.  **Preprocessing:**\n",
        "    * **Tokenization:** Custom regex tokenizer to split text into words.\n",
        "    * **Vocabulary:** Built *strictly* on the Training set to prevent data leakage.\n",
        "    * **Sequence Handling:** Implements padding and \"packed sequences\" to handle variable-length text efficiently in PyTorch.\n",
        "3.  **Model Architecture:**\n",
        "    * **Embedding Layer:** Converts words into dense vectors.\n",
        "    * **Bi-LSTM:** Bidirectional Long Short-Term Memory layer to capture context from both past and future words.\n",
        "    * **Classifier:** Fully connected layer with Dropout for regularization.\n",
        "4.  **Training Loop:**\n",
        "    * Uses **AdamW** optimizer and **BCEWithLogitsLoss**.\n",
        "    * Implements **Early Stopping** based on Validation F1-score to prevent overfitting.\n",
        "5.  **Final Evaluation:** Reports strict accuracy and F1 metrics on the held-out Test set (Scraped Data)."
      ],
      "metadata": {
        "id": "4hZ8uU8cltLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOAD FIXED SPLITS (exported from baseline notebook) ===\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ART_DIR = Path(\"/content/drive/MyDrive/artifacts/data_splits_v1\")  # same folder used in baseline\n",
        "\n",
        "# --- load metadata ---\n",
        "with open(ART_DIR / \"meta.json\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "fmt = meta[\"format\"]\n",
        "style_cols = meta[\"style_cols\"]\n",
        "\n",
        "# --- load datasets ---\n",
        "if fmt == \"parquet\":\n",
        "    train_df = pd.read_parquet(ART_DIR / \"train_all.parquet\")\n",
        "    val_df   = pd.read_parquet(ART_DIR / \"val_all.parquet\")\n",
        "    test_df  = pd.read_parquet(ART_DIR / \"test_all.parquet\")\n",
        "else:\n",
        "    train_df = pd.read_csv(ART_DIR / \"train_all.csv\")\n",
        "    val_df   = pd.read_csv(ART_DIR / \"val_all.csv\")\n",
        "    test_df  = pd.read_csv(ART_DIR / \"test_all.csv\")\n",
        "\n",
        "# --- sanity checks (text + label + style columns) ---\n",
        "required_cols = [\"text\", \"label\", \"source\"] + style_cols\n",
        "\n",
        "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    missing = [c for c in required_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name} split missing columns: {missing[:15]}{' ...' if len(missing) > 15 else ''}\")\n",
        "\n",
        "# --- labels as numpy arrays ---\n",
        "y_train = train_df[\"label\"].astype(int).values\n",
        "y_val   = val_df[\"label\"].astype(int).values\n",
        "y_test  = test_df[\"label\"].astype(int).values\n",
        "\n",
        "print(\"Loaded splits from:\", ART_DIR)\n",
        "print(\"Format:\", fmt)\n",
        "print(\"Sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Label dist train:\", np.bincount(y_train))\n",
        "print(\"Label dist val:  \", np.bincount(y_val))\n",
        "print(\"Label dist test: \", np.bincount(y_test))\n",
        "print(\"Num stylometry features:\", len(style_cols))\n"
      ],
      "metadata": {
        "id": "gziUbn_ll-fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "tB2ST1bYkyIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm8ZLt5bXZ7W"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# LSTM text classifier\n",
        "#   TRAIN/VAL = Kaggle only\n",
        "#   TEST      = Scraped only\n",
        "#\n",
        "# Uses PyTorch. Includes:\n",
        "# - fast regex tokenization\n",
        "# - vocab built from TRAIN only\n",
        "# - truncation to max_len\n",
        "# - padding + packed sequences\n",
        "# - early stopping on VAL F1\n",
        "# - per-epoch timing\n",
        "#\n",
        "# datasets:\n",
        "#   train_df, val_df, test_df\n",
        "#   y_train, y_val, y_test\n",
        "#\n",
        "# This code prints time/epoch + a simple projected total after epoch 1.\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# -------------------------\n",
        "# Config (tune these first)\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "MAX_VOCAB = 50_000        # 30k–100k typical\n",
        "MIN_FREQ = 2              # drop very rare tokens, since we want to focus on statistical construct of the text\n",
        "MAX_LEN = 384             # 256/384/512. Higher = slower but captures longer context\n",
        "BATCH_SIZE = 32           # 32/64/128 depending on CPU & RAM\n",
        "EMB_DIM = 192             # 128–256\n",
        "HID_DIM = 192             # 128–256\n",
        "NUM_LAYERS = 1            # 1–2 (2 slower)\n",
        "BIDIR = True\n",
        "DROPOUT = 0.2\n",
        "LR = 2e-3\n",
        "EPOCHS = 8\n",
        "PATIENCE = 4              # early stop if VAL F1 doesn't improve\n",
        "CLIP = 1.0\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Tokenizer (fast, stable)\n",
        "# -------------------------\n",
        "_tok = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[^\\sA-Za-z0-9]\")\n",
        "def tokenize(text: str):\n",
        "    return _tok.findall((text or \"\").lower())\n",
        "\n",
        "# -------------------------\n",
        "# Build vocab from TRAIN only\n",
        "# -------------------------\n",
        "def build_vocab(texts, max_vocab=MAX_VOCAB, min_freq=MIN_FREQ):\n",
        "    counter = Counter()\n",
        "    for t in texts:\n",
        "        counter.update(tokenize(t))\n",
        "    # Special tokens\n",
        "    itos = [\"<pad>\", \"<unk>\"]\n",
        "    # Keep most common above min_freq\n",
        "    for tok, freq in counter.most_common():\n",
        "        if freq < min_freq:\n",
        "            break\n",
        "        itos.append(tok)\n",
        "        if len(itos) >= max_vocab:\n",
        "            break\n",
        "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "train_texts = train_df[\"text\"].astype(str).tolist()\n",
        "val_texts   = val_df[\"text\"].astype(str).tolist()\n",
        "test_texts  = test_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "stoi, itos = build_vocab(train_texts)\n",
        "PAD_IDX = stoi[\"<pad>\"]\n",
        "UNK_IDX = stoi[\"<unk>\"]\n",
        "\n",
        "print(f\"Vocab size: {len(itos):,} (PAD={PAD_IDX}, UNK={UNK_IDX})\")\n",
        "\n",
        "# -------------------------\n",
        "# Dataset + Collate\n",
        "# -------------------------\n",
        "def encode(text: str, max_len=MAX_LEN):\n",
        "    ids = [stoi.get(tok, UNK_IDX) for tok in tokenize(text)]\n",
        "    if len(ids) > max_len:\n",
        "        ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels.astype(np.int64)\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        return self.texts[i], self.labels[i]\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    seqs = [torch.tensor(encode(t), dtype=torch.long) for t in texts]\n",
        "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "\n",
        "    # pad to max length in batch\n",
        "    maxl = int(lengths.max().item()) if len(lengths) else 1\n",
        "    padded = torch.full((len(seqs), maxl), PAD_IDX, dtype=torch.long)\n",
        "    for i, s in enumerate(seqs):\n",
        "        padded[i, :len(s)] = s\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)  # binary\n",
        "    return padded.to(DEVICE), lengths.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "train_ds = TextDataset(train_texts, y_train)\n",
        "val_ds   = TextDataset(val_texts, y_val)\n",
        "test_ds  = TextDataset(test_texts, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_batch)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, bidir, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hid_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidir,\n",
        "            dropout=0.0 if num_layers == 1 else dropout\n",
        "        )\n",
        "        out_dim = hid_dim * (2 if bidir else 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(out_dim, 1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # x: [B, T]\n",
        "        emb = self.dropout(self.embedding(x))  # [B, T, E]\n",
        "\n",
        "        # pack sequences for speed\n",
        "        lengths_cpu = lengths.to(\"cpu\")\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths_cpu, batch_first=True, enforce_sorted=False)\n",
        "        packed_out, (h, c) = self.lstm(packed)\n",
        "\n",
        "        # h shape: [num_layers * num_directions, B, H]\n",
        "        if self.lstm.bidirectional:\n",
        "            # last layer forward + backward\n",
        "            h_f = h[-2, :, :]\n",
        "            h_b = h[-1, :, :]\n",
        "            h_cat = torch.cat([h_f, h_b], dim=1)  # [B, 2H]\n",
        "        else:\n",
        "            h_cat = h[-1, :, :]  # [B, H]\n",
        "\n",
        "        logits = self.fc(self.dropout(h_cat)).squeeze(1)  # [B]\n",
        "        return logits\n",
        "\n",
        "model = LSTMClassifier(\n",
        "    vocab_size=len(itos),\n",
        "    emb_dim=EMB_DIM,\n",
        "    hid_dim=HID_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    bidir=BIDIR,\n",
        "    dropout=DROPOUT,\n",
        "    pad_idx=PAD_IDX\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "GF0aBWtip80E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Metrics helpers\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def predict_loader(loader):\n",
        "    model.eval()\n",
        "    all_probs, all_y = [], []\n",
        "    for x, lengths, y in loader:\n",
        "        logits = model(x, lengths)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "        all_y.append(y.detach().cpu().numpy())\n",
        "    probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    ytrue = np.concatenate(all_y).astype(int) if all_y else np.array([], dtype=int)\n",
        "    return probs, ytrue\n",
        "\n",
        "def eval_split(loader, threshold=0.5):\n",
        "    probs, ytrue = predict_loader(loader)\n",
        "    ypred = (probs >= threshold).astype(int)\n",
        "    acc = accuracy_score(ytrue, ypred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(ytrue, ypred, average=\"binary\", zero_division=0)\n",
        "    return acc, p, r, f1\n"
      ],
      "metadata": {
        "id": "li1Cg7drqBa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Train loop with timing + early stopping\n",
        "# -------------------------\n",
        "best_val_f1 = -1.0\n",
        "best_state = None\n",
        "no_improve = 0\n",
        "epoch_times = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss_hist = []\n",
        "    val_f1_hist = []\n",
        "    val_acc_hist = []\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, lengths, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x, lengths)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        n_batches += 1\n",
        "\n",
        "    train_loss = running_loss / max(n_batches, 1)\n",
        "    val_acc, val_p, val_r, val_f1 = eval_split(val_loader)\n",
        "\n",
        "    t1 = time.time()\n",
        "    epoch_sec = t1 - t0\n",
        "    epoch_times.append(epoch_sec)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    val_f1_hist.append(val_f1)\n",
        "    val_acc_hist.append(val_acc)\n",
        "\n",
        "\n",
        "    # After epoch 1, print a rough projection based on observed time/epoch\n",
        "    if epoch == 1:\n",
        "        projected = epoch_sec * EPOCHS\n",
        "        print(f\"\\n[Timing] Epoch 1 took {epoch_sec:.1f}s. Rough projected total for {EPOCHS} epochs: ~{projected/60:.1f} min (before early stopping).\")\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} | \"\n",
        "          f\"VAL acc={val_acc:.4f} p={val_p:.4f} r={val_r:.4f} f1={val_f1:.4f} | \"\n",
        "          f\"time={epoch_sec:.1f}s\")\n",
        "\n",
        "    # Early stopping on VAL F1\n",
        "    if val_f1 > best_val_f1 + 1e-4:\n",
        "        best_val_f1 = val_f1\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping: no VAL F1 improvement for {PATIENCE} epoch(s).\")\n",
        "            break\n",
        "\n",
        "# Restore best model\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)"
      ],
      "metadata": {
        "id": "KzYeCp-cqHOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Final evaluation\n",
        "# -------------------------\n",
        "val_acc, val_p, val_r, val_f1 = eval_split(val_loader)\n",
        "test_acc, test_p, test_r, test_f1 = eval_split(test_loader)\n",
        "\n",
        "print(\"\\n===== FINAL (best checkpoint) =====\")\n",
        "print(f\"VAL  acc={val_acc:.4f} p={val_p:.4f} r={val_r:.4f} f1={val_f1:.4f}\")\n",
        "print(f\"TEST acc={test_acc:.4f} p={test_p:.4f} r={test_r:.4f} f1={test_f1:.4f}\")\n",
        "print(f\"Avg time/epoch: {np.mean(epoch_times):.1f}s over {len(epoch_times)} epoch(s)\")"
      ],
      "metadata": {
        "id": "F_Uy7GsjqJ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    precision_recall_fscore_support, accuracy_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 1) Evaluate on a loader (return probs + labels)\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def lstm_probs_on_loader(loader):\n",
        "    model.eval()\n",
        "    all_probs, all_y = [], []\n",
        "    for x, lengths, y in loader:\n",
        "        logits = model(x, lengths)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "        all_y.append(y.detach().cpu().numpy())\n",
        "    probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    ytrue = np.concatenate(all_y).astype(int) if all_y else np.array([], dtype=int)\n",
        "    return probs, ytrue\n",
        "\n",
        "def metrics_from_probs(ytrue, probs, thr=0.5):\n",
        "    ypred = (probs >= thr).astype(int)\n",
        "    acc = accuracy_score(ytrue, ypred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(ytrue, ypred, average=\"binary\", zero_division=0)\n",
        "\n",
        "    out = {\"acc\": acc, \"prec\": p, \"rec\": r, \"f1\": f1}\n",
        "    try: out[\"roc_auc\"] = roc_auc_score(ytrue, probs)\n",
        "    except Exception: out[\"roc_auc\"] = np.nan\n",
        "    try: out[\"pr_auc\"]  = average_precision_score(ytrue, probs)\n",
        "    except Exception: out[\"pr_auc\"]  = np.nan\n",
        "    return out, ypred\n",
        "\n",
        "# -------------------------\n",
        "# 2) Plots\n",
        "# -------------------------\n",
        "def plot_training_curves(train_loss_hist, val_f1_hist):\n",
        "    epochs = np.arange(1, len(train_loss_hist) + 1)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(epochs, train_loss_hist, marker=\"o\")\n",
        "    ax.set_title(\"LSTM Training Loss\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(epochs, val_f1_hist, marker=\"o\")\n",
        "    ax.set_title(\"LSTM Validation F1\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"F1\")\n",
        "    ax.set_ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "def plot_confidence_distributions(ytrue, probs, marker_prob=None):\n",
        "    p_h = probs[ytrue == 0]\n",
        "    p_a = probs[ytrue == 1]\n",
        "    bins = np.linspace(0, 1, 51)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(p_h, bins=bins, alpha=0.6, density=True, label=\"True Human (0)\")\n",
        "    ax.hist(p_a, bins=bins, alpha=0.6, density=True, label=\"True AI (1)\")\n",
        "    if marker_prob is not None:\n",
        "        ax.axvline(marker_prob, linestyle=\"--\", linewidth=2)\n",
        "        ax.text(marker_prob, ax.get_ylim()[1]*0.95, \"Custom input\", rotation=90, va=\"top\")\n",
        "    ax.set_title(\"LSTM Prob(AI) distributions (SCRAPED TEST)\")\n",
        "    ax.set_xlabel(\"Prob(AI)\")\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_length_bin_f1(test_df, ytrue, probs, thr=0.5):\n",
        "    lens = test_df[\"text\"].astype(str).str.len()\n",
        "    q1, q2 = lens.quantile([0.33, 0.66]).values\n",
        "    bins = pd.cut(lens, [-np.inf, q1, q2, np.inf], labels=[\"short\",\"medium\",\"long\"]).astype(str)\n",
        "\n",
        "    ypred = (probs >= thr).astype(int)\n",
        "\n",
        "    labels = [\"short\",\"medium\",\"long\"]\n",
        "    f1s, counts = [], []\n",
        "    for b in labels:\n",
        "        m = (bins == b).values\n",
        "        counts.append(int(m.sum()))\n",
        "        if m.sum() == 0:\n",
        "            f1s.append(np.nan)\n",
        "        else:\n",
        "            _, _, f1, _ = precision_recall_fscore_support(ytrue[m], ypred[m], average=\"binary\", zero_division=0)\n",
        "            f1s.append(float(f1))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(labels, f1s)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_ylabel(\"F1\")\n",
        "    ax.set_title(\"LSTM F1 by Length Bin (SCRAPED TEST)\")\n",
        "    for i, c in enumerate(counts):\n",
        "        ax.text(i, 0.02, f\"n={c}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.show()\n",
        "\n",
        "def per_source_table(test_df, ytrue, probs, thr=0.5, min_n=20):\n",
        "    ypred = (probs >= thr).astype(int)\n",
        "    rows = []\n",
        "    for src, idx in test_df.groupby(\"source\").indices.items():\n",
        "        idx = np.array(list(idx))\n",
        "        if len(idx) < min_n:\n",
        "            continue\n",
        "        m, _ = metrics_from_probs(ytrue[idx], probs[idx], thr=thr)\n",
        "        rows.append({\"source\": src, \"n\": len(idx), **m})\n",
        "    out = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
        "    print(f\"\\n=== LSTM per-source performance (SCRAPED TEST, min_n={min_n}) ===\")\n",
        "    display(out.round(4))\n",
        "    return out\n",
        "\n",
        "def show_confident_errors(test_df, ytrue, probs, topk=10):\n",
        "    df_err = test_df.copy().reset_index(drop=True)\n",
        "    df_err[\"y_true\"] = ytrue\n",
        "    df_err[\"prob_ai\"] = probs\n",
        "    df_err[\"y_pred\"] = (probs >= 0.5).astype(int)\n",
        "    df_err[\"correct\"] = (df_err[\"y_true\"] == df_err[\"y_pred\"])\n",
        "\n",
        "    fp = df_err[(df_err[\"y_true\"] == 0) & (df_err[\"y_pred\"] == 1)].sort_values(\"prob_ai\", ascending=False).head(topk)\n",
        "    fn = df_err[(df_err[\"y_true\"] == 1) & (df_err[\"y_pred\"] == 0)].sort_values(\"prob_ai\", ascending=True).head(topk)\n",
        "\n",
        "    print(f\"\\n=== Top {topk} confident FALSE POSITIVES (Human predicted AI) ===\")\n",
        "    display(fp[[\"source\",\"prob_ai\",\"text\"]].assign(text=fp[\"text\"].str.slice(0, 300) + \"...\"))\n",
        "\n",
        "    print(f\"\\n=== Top {topk} confident FALSE NEGATIVES (AI predicted Human) ===\")\n",
        "    display(fn[[\"source\",\"prob_ai\",\"text\"]].assign(text=fn[\"text\"].str.slice(0, 300) + \"...\"))\n",
        "\n",
        "# -------------------------\n",
        "# 3) Run dashboard\n",
        "# -------------------------\n",
        "# training curves (only if you logged these in Cell 1)\n",
        "if \"train_loss_hist\" in globals() and \"val_f1_hist\" in globals():\n",
        "    plot_training_curves(train_loss_hist, val_f1_hist)\n",
        "else:\n",
        "    print(\"No training curves found. Add train_loss_hist / val_f1_hist logging as suggested.\")\n",
        "\n",
        "# evaluate val + test\n",
        "val_probs, val_ytrue = lstm_probs_on_loader(val_loader)\n",
        "test_probs, test_ytrue = lstm_probs_on_loader(test_loader)\n",
        "\n",
        "val_m, _ = metrics_from_probs(val_ytrue, val_probs, thr=0.5)\n",
        "test_m, test_pred = metrics_from_probs(test_ytrue, test_probs, thr=0.5)\n",
        "\n",
        "print(\"\\n=== LSTM Compact Metrics (thr=0.5) ===\")\n",
        "display(pd.DataFrame([\n",
        "    {\"split\":\"VAL\",  **val_m},\n",
        "    {\"split\":\"TEST\", **test_m},\n",
        "]).round(4))\n",
        "\n"
      ],
      "metadata": {
        "id": "YqjukAiHeOX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix on scraped test\n",
        "cm = confusion_matrix(test_ytrue, test_pred, labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Human(0)\",\"AI(1)\"])\n",
        "fig, ax = plt.subplots()\n",
        "disp.plot(ax=ax, values_format=\"d\")\n",
        "ax.set_title(\"LSTM Confusion Matrix (SCRAPED TEST)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t8cDQDCn4Qgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# confidence distributions\n",
        "plot_confidence_distributions(test_ytrue, test_probs)\n",
        "\n",
        "# length-bin performance\n",
        "plot_length_bin_f1(test_df.reset_index(drop=True), test_ytrue, test_probs, thr=0.5)\n"
      ],
      "metadata": {
        "id": "nZe8HXog4SDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# per-source\n",
        "per_source_table(test_df.reset_index(drop=True), test_ytrue, test_probs, thr=0.5, min_n=20)\n",
        "\n"
      ],
      "metadata": {
        "id": "s41SLHbv4UAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confident errors\n",
        "show_confident_errors(test_df.reset_index(drop=True), test_ytrue, test_probs, topk=10)\n",
        "\n",
        "# expose test_probs for custom input overlay later\n",
        "LSTM_TEST_PROBS = test_probs\n",
        "LSTM_TEST_YTRUE = test_ytrue"
      ],
      "metadata": {
        "id": "vZ7qLskn4WSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usThQVXH3_lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nD3N6c623_ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3qjwZHH3_Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GIE96m6e3_W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_nAs-ol3_UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G05mFNr13_Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YrsyBCWp3_PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0EdDZS593_JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pD4U9lmq3_GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0BiV9Aj3_DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
        "# install can take a minute\n",
        "\n",
        "import os\n",
        "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
        "NOTEBOOKS_DIR = \"/content/drive/MyDrive/\" # @param {type:\"string\"}\n",
        "NOTEBOOK_NAME = \"04_lstm_sequence_model.ipynb\" # @param {type:\"string\"}\n",
        "#------------------------------------------------------------------------------#\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
        "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
        "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
        "!apt install pandoc > /dev/null 2>&1\n",
        "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
        "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
        "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
        "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
      ],
      "metadata": {
        "id": "uFfGa9sD0ji0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}