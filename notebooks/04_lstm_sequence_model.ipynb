{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaKp3i52rQTULsPUoKofQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/04_lstm_sequence_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Sequence Model\n",
        "\n",
        "**Objective:**\n",
        "Train a Deep Learning sequence model (LSTM) to detect AI-generated text. Unlike the feature-based models in Notebook 3 which relied on manual counts (like sentence length), this model learns patterns directly from the raw sequence of words.\n",
        "\n",
        "**Key Components:**\n",
        "1.  **Load Fixed Splits:** Import the pre-split data (`train.csv`, `val.csv`, `test.csv`) saved in Notebook 3 to ensure we are testing on the exact same \"Collected\" dataset.\n",
        "2.  **Preprocessing:**\n",
        "    * **Tokenization:** Custom regex tokenizer to split text into words.\n",
        "    * **Vocabulary:** Built *strictly* on the Training set to prevent data leakage.\n",
        "    * **Sequence Handling:** Implements padding and \"packed sequences\" to handle variable-length text efficiently in PyTorch.\n",
        "3.  **Model Architecture:**\n",
        "    * **Embedding Layer:** Converts words into dense vectors.\n",
        "    * **Bi-LSTM:** Bidirectional Long Short-Term Memory layer to capture context from both past and future words.\n",
        "    * **Classifier:** Fully connected layer with Dropout for regularization.\n",
        "4.  **Training Loop:**\n",
        "    * Uses **AdamW** optimizer and **BCEWithLogitsLoss**.\n",
        "    * Implements **Early Stopping** based on Validation F1-score to prevent overfitting.\n",
        "5.  **Final Evaluation:** Reports strict accuracy and F1 metrics on the held-out Test set (Scraped Data)."
      ],
      "metadata": {
        "id": "4hZ8uU8cltLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOAD FIXED SPLITS (created in notebook 03) ===\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SPLITS_DIR = Path(\"/content/drive/MyDrive/artifacts/splits_v1\")\n",
        "\n",
        "train_path = SPLITS_DIR / \"train.csv\"\n",
        "val_path   = SPLITS_DIR / \"val.csv\"\n",
        "test_path  = SPLITS_DIR / \"test.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "val_df   = pd.read_csv(val_path)\n",
        "test_df  = pd.read_csv(test_path)\n",
        "\n",
        "# Ensure expected columns exist\n",
        "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    if \"text\" not in df.columns or \"label\" not in df.columns:\n",
        "        raise ValueError(f\"{name}.csv must contain columns: text, label\")\n",
        "\n",
        "# Labels as numpy arrays\n",
        "y_train = train_df[\"label\"].astype(int).values\n",
        "y_val   = val_df[\"label\"].astype(int).values\n",
        "y_test  = test_df[\"label\"].astype(int).values\n",
        "\n",
        "print(\" Loaded splits from:\", SPLITS_DIR)\n",
        "print(\"Sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Label dist train:\", np.bincount(y_train))\n",
        "print(\"Label dist val:  \", np.bincount(y_val))\n",
        "print(\"Label dist test: \", np.bincount(y_test))\n"
      ],
      "metadata": {
        "id": "gziUbn_ll-fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm8ZLt5bXZ7W"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# LSTM text classifier\n",
        "#   TRAIN/VAL = Kaggle only\n",
        "#   TEST      = Scraped only\n",
        "#\n",
        "# Uses PyTorch. Includes:\n",
        "# - fast regex tokenization\n",
        "# - vocab built from TRAIN only\n",
        "# - truncation to max_len\n",
        "# - padding + packed sequences\n",
        "# - early stopping on VAL F1\n",
        "# - per-epoch timing\n",
        "#\n",
        "# datasets:\n",
        "#   train_df, val_df, test_df\n",
        "#   y_train, y_val, y_test\n",
        "#\n",
        "# This code prints time/epoch + a simple projected total after epoch 1.\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# -------------------------\n",
        "# Config (tune these first)\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "MAX_VOCAB = 50_000        # 30k–100k typical\n",
        "MIN_FREQ = 2              # drop very rare tokens\n",
        "MAX_LEN = 256             # 256/384/512. Higher = slower but captures longer context\n",
        "BATCH_SIZE = 64           # 32/64/128 depending on CPU & RAM\n",
        "EMB_DIM = 192             # 128–256\n",
        "HID_DIM = 192             # 128–256\n",
        "NUM_LAYERS = 1            # 1–2 (2 slower)\n",
        "BIDIR = True\n",
        "DROPOUT = 0.2\n",
        "LR = 2e-3\n",
        "EPOCHS = 8\n",
        "PATIENCE = 2              # early stop if VAL F1 doesn't improve\n",
        "CLIP = 1.0\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Tokenizer (fast, stable)\n",
        "# -------------------------\n",
        "_tok = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[^\\sA-Za-z0-9]\")\n",
        "def tokenize(text: str):\n",
        "    return _tok.findall((text or \"\").lower())\n",
        "\n",
        "# -------------------------\n",
        "# Build vocab from TRAIN only\n",
        "# -------------------------\n",
        "def build_vocab(texts, max_vocab=MAX_VOCAB, min_freq=MIN_FREQ):\n",
        "    counter = Counter()\n",
        "    for t in texts:\n",
        "        counter.update(tokenize(t))\n",
        "    # Special tokens\n",
        "    itos = [\"<pad>\", \"<unk>\"]\n",
        "    # Keep most common above min_freq\n",
        "    for tok, freq in counter.most_common():\n",
        "        if freq < min_freq:\n",
        "            break\n",
        "        itos.append(tok)\n",
        "        if len(itos) >= max_vocab:\n",
        "            break\n",
        "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "train_texts = train_df[\"text\"].astype(str).tolist()\n",
        "val_texts   = val_df[\"text\"].astype(str).tolist()\n",
        "test_texts  = test_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "stoi, itos = build_vocab(train_texts)\n",
        "PAD_IDX = stoi[\"<pad>\"]\n",
        "UNK_IDX = stoi[\"<unk>\"]\n",
        "\n",
        "print(f\"Vocab size: {len(itos):,} (PAD={PAD_IDX}, UNK={UNK_IDX})\")\n",
        "\n",
        "# -------------------------\n",
        "# Dataset + Collate\n",
        "# -------------------------\n",
        "def encode(text: str, max_len=MAX_LEN):\n",
        "    ids = [stoi.get(tok, UNK_IDX) for tok in tokenize(text)]\n",
        "    if len(ids) > max_len:\n",
        "        ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels.astype(np.int64)\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        return self.texts[i], self.labels[i]\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    seqs = [torch.tensor(encode(t), dtype=torch.long) for t in texts]\n",
        "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "\n",
        "    # pad to max length in batch\n",
        "    maxl = int(lengths.max().item()) if len(lengths) else 1\n",
        "    padded = torch.full((len(seqs), maxl), PAD_IDX, dtype=torch.long)\n",
        "    for i, s in enumerate(seqs):\n",
        "        padded[i, :len(s)] = s\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)  # binary\n",
        "    return padded.to(DEVICE), lengths.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "train_ds = TextDataset(train_texts, y_train)\n",
        "val_ds   = TextDataset(val_texts, y_val)\n",
        "test_ds  = TextDataset(test_texts, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_batch)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, bidir, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hid_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidir,\n",
        "            dropout=0.0 if num_layers == 1 else dropout\n",
        "        )\n",
        "        out_dim = hid_dim * (2 if bidir else 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(out_dim, 1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # x: [B, T]\n",
        "        emb = self.dropout(self.embedding(x))  # [B, T, E]\n",
        "\n",
        "        # pack sequences for speed\n",
        "        lengths_cpu = lengths.to(\"cpu\")\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths_cpu, batch_first=True, enforce_sorted=False)\n",
        "        packed_out, (h, c) = self.lstm(packed)\n",
        "\n",
        "        # h shape: [num_layers * num_directions, B, H]\n",
        "        if self.lstm.bidirectional:\n",
        "            # last layer forward + backward\n",
        "            h_f = h[-2, :, :]\n",
        "            h_b = h[-1, :, :]\n",
        "            h_cat = torch.cat([h_f, h_b], dim=1)  # [B, 2H]\n",
        "        else:\n",
        "            h_cat = h[-1, :, :]  # [B, H]\n",
        "\n",
        "        logits = self.fc(self.dropout(h_cat)).squeeze(1)  # [B]\n",
        "        return logits\n",
        "\n",
        "model = LSTMClassifier(\n",
        "    vocab_size=len(itos),\n",
        "    emb_dim=EMB_DIM,\n",
        "    hid_dim=HID_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    bidir=BIDIR,\n",
        "    dropout=DROPOUT,\n",
        "    pad_idx=PAD_IDX\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "GF0aBWtip80E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Metrics helpers\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def predict_loader(loader):\n",
        "    model.eval()\n",
        "    all_probs, all_y = [], []\n",
        "    for x, lengths, y in loader:\n",
        "        logits = model(x, lengths)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "        all_y.append(y.detach().cpu().numpy())\n",
        "    probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    ytrue = np.concatenate(all_y).astype(int) if all_y else np.array([], dtype=int)\n",
        "    return probs, ytrue\n",
        "\n",
        "def eval_split(loader, threshold=0.5):\n",
        "    probs, ytrue = predict_loader(loader)\n",
        "    ypred = (probs >= threshold).astype(int)\n",
        "    acc = accuracy_score(ytrue, ypred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(ytrue, ypred, average=\"binary\", zero_division=0)\n",
        "    return acc, p, r, f1\n"
      ],
      "metadata": {
        "id": "li1Cg7drqBa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Train loop with timing + early stopping\n",
        "# -------------------------\n",
        "best_val_f1 = -1.0\n",
        "best_state = None\n",
        "no_improve = 0\n",
        "epoch_times = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, lengths, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x, lengths)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        n_batches += 1\n",
        "\n",
        "    train_loss = running_loss / max(n_batches, 1)\n",
        "    val_acc, val_p, val_r, val_f1 = eval_split(val_loader)\n",
        "\n",
        "    t1 = time.time()\n",
        "    epoch_sec = t1 - t0\n",
        "    epoch_times.append(epoch_sec)\n",
        "\n",
        "    # After epoch 1, print a rough projection based on observed time/epoch\n",
        "    if epoch == 1:\n",
        "        projected = epoch_sec * EPOCHS\n",
        "        print(f\"\\n[Timing] Epoch 1 took {epoch_sec:.1f}s. Rough projected total for {EPOCHS} epochs: ~{projected/60:.1f} min (before early stopping).\")\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} | \"\n",
        "          f\"VAL acc={val_acc:.4f} p={val_p:.4f} r={val_r:.4f} f1={val_f1:.4f} | \"\n",
        "          f\"time={epoch_sec:.1f}s\")\n",
        "\n",
        "    # Early stopping on VAL F1\n",
        "    if val_f1 > best_val_f1 + 1e-4:\n",
        "        best_val_f1 = val_f1\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping: no VAL F1 improvement for {PATIENCE} epoch(s).\")\n",
        "            break\n",
        "\n",
        "# Restore best model\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)"
      ],
      "metadata": {
        "id": "KzYeCp-cqHOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Final evaluation\n",
        "# -------------------------\n",
        "val_acc, val_p, val_r, val_f1 = eval_split(val_loader)\n",
        "test_acc, test_p, test_r, test_f1 = eval_split(test_loader)\n",
        "\n",
        "print(\"\\n===== FINAL (best checkpoint) =====\")\n",
        "print(f\"VAL  acc={val_acc:.4f} p={val_p:.4f} r={val_r:.4f} f1={val_f1:.4f}\")\n",
        "print(f\"TEST acc={test_acc:.4f} p={test_p:.4f} r={test_r:.4f} f1={test_f1:.4f}\")\n",
        "print(f\"Avg time/epoch: {np.mean(epoch_times):.1f}s over {len(epoch_times)} epoch(s)\")"
      ],
      "metadata": {
        "id": "F_Uy7GsjqJ-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}