{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2uZlr5iF8tjO6AMUBrQu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/02_data_integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T3DVxgqyR9o",
        "outputId": "eb6784d4-c762-41e0-d56d-f01323063350"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0efCmAEOvpQ4",
        "outputId": "8b373b17-96e0-4e03-d756-7165bdca4b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pandas numpy scikit-learn tqdm wikipedia feedparser beautifulsoup4 readability-lxml requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "eNpctbzJxn_2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"label\"     # 0=human, 1=AI\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    if t is None:\n",
        "        return \"\"\n",
        "    t = re.sub(r\"http\\S+\", \" \", str(t))\n",
        "    t = re.sub(r\"\\S+@\\S+\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def stable_hash(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "def chunk_text_words(text: str, chunk_words=200, overlap=40, min_words=40):\n",
        "    words = re.findall(r\"\\S+\", text)\n",
        "    if len(words) < min_words:\n",
        "        return []\n",
        "    step = max(1, chunk_words - overlap)\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), step):\n",
        "        ch = \" \".join(words[i:i+chunk_words]).strip()\n",
        "        if len(ch.split()) >= min_words:\n",
        "            chunks.append(ch)\n",
        "    return chunks\n",
        "\n",
        "def add_len_bins(df: pd.DataFrame):\n",
        "    df[\"len_words\"] = df[TEXT_COL].str.split().str.len()\n",
        "    df[\"len_bin\"] = pd.cut(df[\"len_words\"], bins=[0,10,25,50,100,200,400,1000,10_000],\n",
        "                           labels=False, include_lowest=True)\n",
        "    return df\n",
        "\n",
        "def dedup_by_text(df: pd.DataFrame):\n",
        "    df[\"text_hash\"] = df[TEXT_COL].map(stable_hash)\n",
        "    df = df.drop_duplicates(subset=[\"text_hash\"]).drop(columns=[\"text_hash\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "C3MF-klfx_2w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_CSV = \"/content/drive/MyDrive/AI_Human.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/kaggle_text_label.csv\"\n",
        "\n",
        "# def clean_text(x):\n",
        "#     return re.sub(r\"\\s+\", \" \", str(x)).strip()\n",
        "\n",
        "reader = pd.read_csv(\n",
        "    INPUT_CSV,\n",
        "    usecols=[\"text\", \"generated\"],\n",
        "    chunksize=25_000,          # safe for Colab\n",
        "    engine=\"python\",\n",
        "    on_bad_lines=\"skip\"\n",
        ")\n",
        "\n",
        "first_write = True\n",
        "total_rows = 0\n",
        "\n",
        "for chunk in reader:\n",
        "    # drop bad rows\n",
        "    chunk = chunk.dropna(subset=[\"text\", \"generated\"])\n",
        "\n",
        "    # label\n",
        "    chunk[\"label\"] = chunk[\"generated\"].astype(int)\n",
        "\n",
        "    # clean text\n",
        "    chunk[\"text\"] = chunk[\"text\"].astype(str).apply(clean_text)\n",
        "\n",
        "    # cheap length filter (NO split yet)\n",
        "    chunk = chunk[chunk[\"text\"].str.len() > 50]\n",
        "\n",
        "    # keep only what you want\n",
        "    chunk = chunk[[\"text\", \"label\"]]\n",
        "\n",
        "    # append to output CSV\n",
        "    chunk.to_csv(\n",
        "        OUTPUT_CSV,\n",
        "        mode=\"w\" if first_write else \"a\",\n",
        "        index=False,\n",
        "        header=first_write\n",
        "    )\n",
        "\n",
        "    total_rows += len(chunk)\n",
        "    first_write = False\n",
        "    print(f\"Written so far: {total_rows}\")\n",
        "\n",
        "print(\"✅ DONE. Final rows:\", total_rows)\n",
        "print(\"Saved to:\", OUTPUT_CSV)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utYtRSEZxoT7",
        "outputId": "5e4e94a7-182b-4cec-e875-0de2bc907faa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written so far: 24995\n",
            "Written so far: 49989\n",
            "Written so far: 74989\n",
            "Written so far: 99985\n",
            "Written so far: 124985\n",
            "Written so far: 149985\n",
            "Written so far: 174984\n",
            "Written so far: 199984\n",
            "Written so far: 224984\n",
            "Written so far: 249984\n",
            "Written so far: 274983\n",
            "Written so far: 299983\n",
            "Written so far: 324983\n",
            "Written so far: 349983\n",
            "Written so far: 374983\n",
            "Written so far: 399983\n",
            "Written so far: 424983\n",
            "Written so far: 449983\n",
            "Written so far: 474983\n",
            "Written so far: 487218\n",
            "✅ DONE. Final rows: 487218\n",
            "Saved to: /content/drive/MyDrive/kaggle_text_label.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle_path = \"/content/drive/MyDrive/kaggle_text_label.csv\"\n",
        "\n",
        "if os.path.exists(kaggle_path):\n",
        "    print(f\"Found Kaggle file at: {kaggle_path}\")\n",
        "    # Load the file into the 'kag' variable\n",
        "    kag = pd.read_csv(kaggle_path)\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Could not find {kaggle_path}. Make sure the file was actually saved there.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVtiUTHHxsN9",
        "outputId": "6749c8e1-6ae0-4dbd-c29c-0c155b97ab00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found Kaggle file at: /content/drive/MyDrive/kaggle_text_label.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output_path = \"/content/drive/MyDrive/final_merged_dataset.csv\"\n",
        "scraped_data_path = \"/content/drive/MyDrive/scraped_data_combined.csv\"\n",
        "\n",
        "# Load\n",
        "kag_df = pd.read_csv(kaggle_path, low_memory=False)\n",
        "scraped_df = pd.read_csv(scraped_data_path)\n",
        "\n",
        "print(\"Merging datasets...\")\n",
        "full_df = pd.concat([kag_df, scraped_df], ignore_index=True)\n",
        "# It is crucial to shuffle so the model doesn't see all \"Human\" samples then all \"AI\" samples\n",
        "full_df = full_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Saving final dataset to {final_output_path}...\")\n",
        "full_df.to_csv(final_output_path, index=False)\n",
        "\n",
        "print(\"\\nSUCCESS! Pipeline Complete.\")\n",
        "print(f\"Final Dataset Shape: {full_df.shape}\")\n",
        "print(\"\\nSource Distribution:\")\n",
        "print(full_df[\"source\"].value_counts())\n",
        "print(\"\\nLabel Distribution (0=Human, 1=AI):\")\n",
        "print(full_df[\"label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxUsBxM8x022",
        "outputId": "6a990934-0dac-4f23-a1b4-9f3f31ebf32f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging datasets...\n",
            "Saving final dataset to /content/drive/MyDrive/final_merged_dataset.csv...\n",
            "\n",
            "SUCCESS! Pipeline Complete.\n",
            "Final Dataset Shape: (505311, 4)\n",
            "\n",
            "Source Distribution:\n",
            "source\n",
            "chatgpt      12179\n",
            "wikipedia     3354\n",
            "news          1667\n",
            "arxiv          893\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label Distribution (0=Human, 1=AI):\n",
            "label\n",
            "0    311711\n",
            "1    193600\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}