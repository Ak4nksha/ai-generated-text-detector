{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/06_transformer_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Fine-tuning (Human vs AI)\n",
        "\n",
        "Goal: Fine-tune a pretrained transformer for binary classification:\n",
        "**human-written vs LLM-generated text**.\n",
        "\n",
        "- Uses the fixed `train/val/test` splits created earlier.\n",
        "- Trains an end-to-end transformer classifier (not frozen).\n",
        "- Reports validation and test metrics.\n"
      ],
      "metadata": {
        "id": "ZGqwEbnPeY-a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtEAg_mreSmH"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers datasets evaluate accelerate scikit-learn pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n"
      ],
      "metadata": {
        "id": "IspVZe0XeeuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "bwspZNCUOR67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOAD FIXED SPLITS (exported from baseline notebook) ===\n",
        "\n",
        "\n",
        "ART_DIR = Path(\"/content/drive/MyDrive/artifacts/data_splits_v1\")  # same folder used in baseline\n",
        "\n",
        "# --- load metadata ---\n",
        "with open(ART_DIR / \"meta.json\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "fmt = meta[\"format\"]\n",
        "style_cols = meta[\"style_cols\"]\n",
        "\n",
        "# --- load datasets ---\n",
        "if fmt == \"parquet\":\n",
        "    train_df = pd.read_parquet(ART_DIR / \"train_all.parquet\")\n",
        "    val_df   = pd.read_parquet(ART_DIR / \"val_all.parquet\")\n",
        "    test_df  = pd.read_parquet(ART_DIR / \"test_all.parquet\")\n",
        "else:\n",
        "    train_df = pd.read_csv(ART_DIR / \"train_all.csv\")\n",
        "    val_df   = pd.read_csv(ART_DIR / \"val_all.csv\")\n",
        "    test_df  = pd.read_csv(ART_DIR / \"test_all.csv\")\n",
        "\n",
        "# --- sanity checks (text + label + style columns) ---\n",
        "required_cols = [\"text\", \"label\", \"source\"] + style_cols\n",
        "\n",
        "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    missing = [c for c in required_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name} split missing columns: {missing[:15]}{' ...' if len(missing) > 15 else ''}\")\n",
        "\n",
        "# --- labels as numpy arrays ---\n",
        "y_train = train_df[\"label\"].astype(int).values\n",
        "y_val   = val_df[\"label\"].astype(int).values\n",
        "y_test  = test_df[\"label\"].astype(int).values\n",
        "\n",
        "print(\"Loaded splits from:\", ART_DIR)\n",
        "print(\"Format:\", fmt)\n",
        "print(\"Sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Label dist train:\", np.bincount(y_train))\n",
        "print(\"Label dist val:  \", np.bincount(y_val))\n",
        "print(\"Label dist test: \", np.bincount(y_test))\n",
        "print(\"Num stylometry features:\", len(style_cols))\n"
      ],
      "metadata": {
        "id": "KWua0Vomemme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
        "val_ds   = Dataset.from_pandas(val_df[[\"text\", \"label\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
        "\n",
        "print(train_ds)\n"
      ],
      "metadata": {
        "id": "daHZ5YJQeyI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN = 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=False,   # we'll pad dynamically in the collator\n",
        "    )\n",
        "\n",
        "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "val_tok   = val_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "test_tok  = test_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "print(\" Tokenized.\")\n"
      ],
      "metadata": {
        "id": "-9_SaYXge2Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "UYhsJDzze7Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n"
      ],
      "metadata": {
        "id": "68lOku3fe72J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1  = f1_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "mxHY_DLHfD6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training setup\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "OUTPUT_DIR = \"./artifacts/transformer_finetune/distilbert_run_v1\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    # save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    # load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    fp16=True,  # works on most Colab GPUs; if error, set fp16=False\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "JxN6S94FfHkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "iLE9I6UcfQHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_metrics = trainer.evaluate(val_tok)\n",
        "print(\"Val metrics:\", val_metrics)\n",
        "\n",
        "test_metrics = trainer.evaluate(test_tok)\n",
        "print(\"Test metrics:\", test_metrics)\n"
      ],
      "metadata": {
        "id": "fF4BDvI-fRIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- **DistilBERT fine-tuning results (fixed splits):**\n",
        "- Validation F1 ≈ 0.996\n",
        "- Test F1 ≈ 0.849\n",
        "\n",
        "Large generalization gap indicates strong domain shift between training and test data. -->\n"
      ],
      "metadata": {
        "id": "G_g1qMqul39O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation Results\")\n",
        "print(f\"  Accuracy : {val_metrics['eval_accuracy']*100:.2f}%\")\n",
        "print(f\"  F1-score : {val_metrics['eval_f1']*100:.2f}%\")\n",
        "print(f\"  Loss     : {val_metrics['eval_loss']*100:.2f}%\")\n",
        "print()\n",
        "\n",
        "print(\"Test Results\")\n",
        "print(f\"  Accuracy : {test_metrics['eval_accuracy']*100:.2f}%\")\n",
        "print(f\"  F1-score : {test_metrics['eval_f1']*100:.2f}%\")\n",
        "print(f\"  Loss     : {test_metrics['eval_loss']*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "JT9a4KnGZcBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "\n",
        "The fine-tuned transformer achieves near-perfect performance on the validation set, indicating strong capacity to fit the training distribution. However, test accuracy drops substantially, while F1 remains relatively high. This suggests that the model generalizes well for detecting AI-generated text but struggles with human-written examples, consistent with observations from the linear probe and LSTM models."
      ],
      "metadata": {
        "id": "-wOsOkE4Z-5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "preds = trainer.predict(test_tok)\n",
        "test_logits = preds.predictions\n",
        "test_labels = preds.label_ids\n",
        "test_preds = np.argmax(test_logits, axis=1)\n",
        "\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[\"Human (0)\", \"AI (1)\"],\n",
        "    columns=[\"Pred Human\", \"Pred AI\"]\n",
        ")\n",
        "\n",
        "cm_df"
      ],
      "metadata": {
        "id": "XcMW2dbBl9O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix shows that most errors come from human-written text being misclassified as AI, while AI-generated text is detected reliably. This explains why F1 remains high despite lower accuracy. The model strongly favors the AI class."
      ],
      "metadata": {
        "id": "SXx-wuGLcNGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy.special import softmax\n",
        "\n",
        "probs = softmax(test_logits, axis=1)\n",
        "test_df_analysis = test_df.copy()\n",
        "\n",
        "test_df_analysis[\"pred_label\"] = test_preds\n",
        "test_df_analysis[\"prob_ai\"] = probs[:, 1]\n",
        "test_df_analysis[\"correct\"] = test_df_analysis[\"label\"] == test_df_analysis[\"pred_label\"]\n"
      ],
      "metadata": {
        "id": "Y5sfTWU5NGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_analysis[\n",
        "    (test_df_analysis[\"label\"] == 1) &\n",
        "    (test_df_analysis[\"correct\"]) &\n",
        "    (test_df_analysis[\"prob_ai\"] > 0.9)\n",
        "][[\"text\", \"prob_ai\"]].sample(3)\n"
      ],
      "metadata": {
        "id": "KOcgNuycNGqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confident AI detections:\n",
        "Above texts often exhibit fluent structure, neutral tone, and consistent sentence patterns, which the transformer captures effectively after fine-tuning."
      ],
      "metadata": {
        "id": "LdVpckxQcw9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_analysis[\n",
        "    (test_df_analysis[\"label\"] == 0) &\n",
        "    (~test_df_analysis[\"correct\"]) &\n",
        "    (test_df_analysis[\"prob_ai\"] > 0.9)\n",
        "][[\"text\", \"prob_ai\"]].head(3)"
      ],
      "metadata": {
        "id": "l2mKaEAwNGoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confident misclassifications (Human → AI):\n",
        "Above human-written examples are often formal, well-structured, or informational in tone, making them stylistically similar to LLM-generated text. This suggests the model relies heavily on surface fluency cues rather than deeper semantic intent."
      ],
      "metadata": {
        "id": "xzIPG8MDdaib"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtuDXpynNGjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRsojUJNei_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiD14Gi8ei9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
        "# install can take a minute\n",
        "\n",
        "import os\n",
        "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
        "NOTEBOOKS_DIR = \"/content/drive/MyDrive/\" # @param {type:\"string\"}\n",
        "NOTEBOOK_NAME = \"06_transformer_finetune.ipynb\" # @param {type:\"string\"}\n",
        "#------------------------------------------------------------------------------#\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
        "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
        "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
        "!apt install pandoc > /dev/null 2>&1\n",
        "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
        "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
        "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
        "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
      ],
      "metadata": {
        "id": "eBSFlYluNGhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}