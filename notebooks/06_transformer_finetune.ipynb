{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEyomDzmpAFzcRaZpBJo0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/06_transformer_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Fine-tuning (Human vs AI)\n",
        "\n",
        "Goal: Fine-tune a pretrained transformer for binary classification:\n",
        "**human-written vs LLM-generated text**.\n",
        "\n",
        "- Uses the fixed `train/val/test` splits created earlier.\n",
        "- Trains an end-to-end transformer classifier (not frozen).\n",
        "- Reports validation and test metrics.\n"
      ],
      "metadata": {
        "id": "ZGqwEbnPeY-a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtEAg_mreSmH"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers datasets evaluate accelerate scikit-learn pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n"
      ],
      "metadata": {
        "id": "IspVZe0XeeuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "SPLITS_DIR = Path(\"/content/drive/MyDrive/artifacts/splits_v1\")\n",
        "\n",
        "train_df = pd.read_csv(SPLITS_DIR / \"train.csv\")\n",
        "val_df   = pd.read_csv(SPLITS_DIR / \"val.csv\")\n",
        "test_df  = pd.read_csv(SPLITS_DIR / \"test.csv\")\n",
        "\n",
        "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    if \"text\" not in df.columns or \"label\" not in df.columns:\n",
        "        raise ValueError(f\"{name}.csv must contain columns: text, label\")\n",
        "\n",
        "print(\"Loaded splits:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Train label dist:\", np.bincount(train_df[\"label\"].astype(int).values))\n"
      ],
      "metadata": {
        "id": "KWua0Vomemme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
        "val_ds   = Dataset.from_pandas(val_df[[\"text\", \"label\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
        "\n",
        "print(train_ds)\n"
      ],
      "metadata": {
        "id": "daHZ5YJQeyI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN = 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=False,   # we'll pad dynamically in the collator\n",
        "    )\n",
        "\n",
        "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "val_tok   = val_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "test_tok  = test_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "print(\" Tokenized.\")\n"
      ],
      "metadata": {
        "id": "-9_SaYXge2Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "UYhsJDzze7Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n"
      ],
      "metadata": {
        "id": "68lOku3fe72J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1  = f1_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "mxHY_DLHfD6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training setup\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "OUTPUT_DIR = \"./artifacts/transformer_finetune/distilbert_run_v1\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    # save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    # load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    fp16=True,  # works on most Colab GPUs; if error, set fp16=False\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "JxN6S94FfHkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "iLE9I6UcfQHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_metrics = trainer.evaluate(val_tok)\n",
        "print(\"Val metrics:\", val_metrics)\n",
        "\n",
        "test_metrics = trainer.evaluate(test_tok)\n",
        "print(\"Test metrics:\", test_metrics)\n"
      ],
      "metadata": {
        "id": "fF4BDvI-fRIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- **DistilBERT fine-tuning results (fixed splits):**\n",
        "- Validation F1 ≈ 0.996\n",
        "- Test F1 ≈ 0.849\n",
        "\n",
        "Large generalization gap indicates strong domain shift between training and test data. -->\n"
      ],
      "metadata": {
        "id": "G_g1qMqul39O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcMW2dbBl9O6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}