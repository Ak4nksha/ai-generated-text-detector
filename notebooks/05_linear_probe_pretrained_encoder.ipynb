{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/05_linear_probe_pretrained_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Probe on a Pretrained Encoder\n",
        "\n",
        "Goal: Evaluate how well a **frozen pretrained text encoder** separates\n",
        "human-written vs LLM-generated text using a **linear classifier** on top.\n",
        "\n",
        "- Encoder is frozen (no fine-tuning).\n",
        "- Only a lightweight classifier is trained.\n",
        "- Uses the fixed `splits_v1` created in notebook 03.\n"
      ],
      "metadata": {
        "id": "yt5TFC4gIoiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "P-230c-GIvyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM5ssqKQGT6h"
      },
      "outputs": [],
      "source": [
        "!pip -q install sentence-transformers scikit-learn pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "8ei4NgcIg-iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOAD FIXED SPLITS (exported from baseline notebook) ===\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ART_DIR = Path(\"/content/drive/MyDrive/artifacts/data_splits_v1\")  # same folder used in baseline\n",
        "\n",
        "# --- load metadata ---\n",
        "with open(ART_DIR / \"meta.json\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "fmt = meta[\"format\"]\n",
        "style_cols = meta[\"style_cols\"]\n",
        "\n",
        "# --- load datasets ---\n",
        "if fmt == \"parquet\":\n",
        "    train_df = pd.read_parquet(ART_DIR / \"train_all.parquet\")\n",
        "    val_df   = pd.read_parquet(ART_DIR / \"val_all.parquet\")\n",
        "    test_df  = pd.read_parquet(ART_DIR / \"test_all.parquet\")\n",
        "else:\n",
        "    train_df = pd.read_csv(ART_DIR / \"train_all.csv\")\n",
        "    val_df   = pd.read_csv(ART_DIR / \"val_all.csv\")\n",
        "    test_df  = pd.read_csv(ART_DIR / \"test_all.csv\")\n",
        "\n",
        "# --- sanity checks (text + label + style columns) ---\n",
        "required_cols = [\"text\", \"label\", \"source\"] + style_cols\n",
        "\n",
        "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "    missing = [c for c in required_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name} split missing columns: {missing[:15]}{' ...' if len(missing) > 15 else ''}\")\n",
        "\n",
        "# --- labels as numpy arrays ---\n",
        "y_train = train_df[\"label\"].astype(int).values\n",
        "y_val   = val_df[\"label\"].astype(int).values\n",
        "y_test  = test_df[\"label\"].astype(int).values\n",
        "\n",
        "print(\"Loaded splits from:\", ART_DIR)\n",
        "print(\"Format:\", fmt)\n",
        "print(\"Sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Label dist train:\", np.bincount(y_train))\n",
        "print(\"Label dist val:  \", np.bincount(y_val))\n",
        "print(\"Label dist test: \", np.bincount(y_test))\n",
        "print(\"Num stylometry features:\", len(style_cols))\n"
      ],
      "metadata": {
        "id": "UMMCEQ3PIynw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "ENCODER_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "encoder = SentenceTransformer(ENCODER_NAME)\n",
        "encoder.max_seq_length = 256\n",
        "print(\"Loaded encoder:\", ENCODER_NAME)\n"
      ],
      "metadata": {
        "id": "YRI0ghhcI7QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache embeddings so we don't re-encode every time\n",
        "CACHE_DIR = Path(\"/content/drive/MyDrive/artifacts/linear_probe/cache\")\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def embed_texts(texts, cache_path: Path, batch_size: int = 64):\n",
        "    if cache_path.exists():\n",
        "        return np.load(cache_path)\n",
        "    emb = encoder.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    np.save(cache_path, emb)\n",
        "    return emb\n",
        "\n",
        "X_train = embed_texts(train_df[\"text\"].tolist(), CACHE_DIR / \"X_train.npy\")\n",
        "X_val   = embed_texts(val_df[\"text\"].tolist(),   CACHE_DIR / \"X_val.npy\")\n",
        "X_test  = embed_texts(test_df[\"text\"].tolist(),  CACHE_DIR / \"X_test.npy\")\n",
        "\n",
        "print(\" Embeddings shapes:\", X_train.shape, X_val.shape, X_test.shape)\n"
      ],
      "metadata": {
        "id": "YuoBDQU6JAUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "clf = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "val_pred = clf.predict(X_val)\n",
        "val_prob = clf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "test_pred = clf.predict(X_test)\n",
        "test_prob = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "val_acc = accuracy_score(y_val, val_pred)\n",
        "val_f1  = f1_score(y_val, val_pred)\n",
        "\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "test_f1  = f1_score(y_test, test_pred)\n",
        "\n",
        "print(\"VAL  acc/f1:\", val_acc, val_f1)\n",
        "print(\"TEST acc/f1:\", test_acc, test_f1)\n",
        "\n",
        "report = classification_report(y_test, test_pred, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df.round(4)\n"
      ],
      "metadata": {
        "id": "93tV9r6AJDEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_results(val_acc, val_f1, test_acc, test_f1):\n",
        "    df = pd.DataFrame({\n",
        "        \"Split\": [\"Validation\", \"Test\"],\n",
        "        \"Accuracy\": [val_acc * 100, test_acc *100],\n",
        "        \"F1\": [val_f1*100, test_f1*100],\n",
        "    })\n",
        "    return df.round(2)\n",
        "\n",
        "summarize_results(val_acc, val_f1, test_acc, test_f1)\n"
      ],
      "metadata": {
        "id": "Nm-aFO5LiWSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below confusion matrix shows that the linear probe correctly identifies most AI-generated texts, but frequently misclassifies human-written text as AI. This explains the high F1 for class 1 and low recall for class 0."
      ],
      "metadata": {
        "id": "s-9eAJ8-I5to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[\"Human (0)\", \"AI (1)\"],\n",
        "    columns=[\"Pred Human\", \"Pred AI\"]\n",
        ")\n",
        "\n",
        "cm_df"
      ],
      "metadata": {
        "id": "eNtCKmDnItQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Qualitative Analysis"
      ],
      "metadata": {
        "id": "BAwFhFRzJEUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = test_df.copy()\n",
        "test_results[\"pred\"] = test_pred\n",
        "test_results[\"prob_ai\"] = test_prob\n",
        "\n",
        "false_positives = test_results[\n",
        "    (test_results[\"label\"] == 0) & (test_results[\"pred\"] == 1)\n",
        "]\n",
        "\n",
        "false_negatives = test_results[\n",
        "    (test_results[\"label\"] == 1) & (test_results[\"pred\"] == 0)\n",
        "]\n"
      ],
      "metadata": {
        "id": "hhqDah0HJIWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "false_positives[[\"text\", \"prob_ai\"]].head(3)"
      ],
      "metadata": {
        "id": "qrbnRabVJLxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "false_negatives[[\"text\", \"prob_ai\"]].head(3)"
      ],
      "metadata": {
        "id": "eIuHO1n1JN19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "Many false positives (human text predicted as AI) are highly structured, neutral in tone, and lack personal context. These stylistic traits resemble LLM-generated text, causing the encoder to overgeneralize.\n",
        "\n",
        "False negatives (AI predicted as human) often contain informal phrasing or personal language, which reduces stereotypical AI patterns."
      ],
      "metadata": {
        "id": "1nfaM4MqJTrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "false_positives[\"prob_ai\"].describe()"
      ],
      "metadata": {
        "id": "qNE6XrASJe9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "false_negatives[\"prob_ai\"].describe()"
      ],
      "metadata": {
        "id": "m0xFpmVEJjUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UywXPq18JmOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVViERgcKIfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-B-Ie2aKIUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHIF80J6KISs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
        "# install can take a minute\n",
        "\n",
        "import os\n",
        "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
        "NOTEBOOKS_DIR = \"/content/drive/MyDrive/\" # @param {type:\"string\"}\n",
        "NOTEBOOK_NAME = \"05_linear_probe_pretrained_encoder.ipynb\" # @param {type:\"string\"}\n",
        "#------------------------------------------------------------------------------#\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
        "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
        "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
        "!apt install pandoc > /dev/null 2>&1\n",
        "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
        "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
        "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
        "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
      ],
      "metadata": {
        "id": "BA8UXJGZKI0h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}