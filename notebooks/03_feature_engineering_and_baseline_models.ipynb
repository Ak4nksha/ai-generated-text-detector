{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/ai-generated-text-detector/blob/main/notebooks/03_feature_engineering_and_baseline_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering & Baseline Models\n",
        "\n",
        "**Objective:**\n",
        "Extract advanced stylometric features and train a hybrid text classification model. **Crucially, this notebook implements a strict data splitting strategy to comply with project requirements regarding independent evaluation.**\n",
        "\n",
        "**Key Workflow:**\n",
        "1.  **Load Data:** Import `final_merged_dataset.csv` (from Notebook 2).\n",
        "2.  **Strict Data Splitting (Project Compliance):**\n",
        "    * **Test Set (Final Eval):** Composed *exclusively* of self-collected data (Wikipedia, ArXiv, News, ChatGPT)\n",
        "    * **Training Set:** A mix of downsampled Kaggle data (for efficiency and diversity) and the remaining self-collected data.\n",
        "    * **Validation Set:** Held-out Kaggle data for hyperparameter tuning.\n",
        "3.  **Feature Engineering (Stylometry):**\n",
        "    * **Lexical Diversity:** Type-Token Ratio (vocabulary richness).\n",
        "    * **Readability:** Flesch-Kincaid, Gunning Fog scores.\n",
        "    * **Structure:** Sentence length variance, punctuation patterns.\n",
        "    * **Linguistic Analysis:** Part-of-Speech (POS) tagging via spaCy.\n",
        "4.  **Hybrid Modeling:**\n",
        "    * Combine **TF-IDF vectors** (content) with **Stylometric features** (style).\n",
        "    * Train Baseline Models: Logistic Regression, Linear SVC, Random Forest.\n",
        "5.  **Evaluation:** Report Precision/Recall on the held-out Test set and analyze model coefficients to interpret \"AI-like\" writing markers."
      ],
      "metadata": {
        "id": "aZooBDM9YCnl"
      },
      "id": "aZooBDM9YCnl"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "qPdwZyC2X_70"
      },
      "id": "qPdwZyC2X_70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01fc3048-09c4-48de-ba26-778a6f5f62e7",
      "metadata": {
        "id": "01fc3048-09c4-48de-ba26-778a6f5f62e7"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Install all required libraries for this project\n",
        "# =========================\n",
        "!pip -q install -U \\\n",
        "  numpy pandas scipy \\\n",
        "  scikit-learn \\\n",
        "  matplotlib \\\n",
        "  tqdm \\\n",
        "  textstat \\\n",
        "  torch \\\n",
        "  transformers \\\n",
        "  datasets \\\n",
        "  accelerate\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61239cbd-96be-4410-b3a5-286dfa7389fe",
      "metadata": {
        "id": "61239cbd-96be-4410-b3a5-286dfa7389fe"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import re, math, hashlib, zlib\n",
        "from collections import Counter\n",
        "\n",
        "import textstat\n",
        "import spacy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tqdm.pandas()  # enable progress bar on apply\n",
        "\n",
        "# load lightweight spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9507689a-3aec-4110-9118-6bd67024b6ff",
      "metadata": {
        "id": "9507689a-3aec-4110-9118-6bd67024b6ff"
      },
      "outputs": [],
      "source": [
        "CSV_PATH = \"/content/drive/MyDrive/final_merged_dataset.csv\"\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"label\"\n",
        "\n",
        "# Robust CSV load (handles quotes, bad lines)\n",
        "df = pd.read_csv(\n",
        "    CSV_PATH,\n",
        "    engine=\"python\",\n",
        "    escapechar=\"\\\\\",\n",
        "    on_bad_lines=\"skip\"\n",
        ")\n",
        "\n",
        "# Drop rows with missing text/label\n",
        "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "\n",
        "# Clean label column: ensure numeric 0/1\n",
        "df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors=\"coerce\")\n",
        "df[LABEL_COL] = df[LABEL_COL].fillna(0).astype(int)\n",
        "\n",
        "#additional fixes for NAN values\n",
        "df[\"source\"] = df[\"source\"].fillna(\"kaggle\")\n",
        "nan_rows = df['doc_id'].isna()\n",
        "df.loc[nan_rows, 'doc_id'] = \"kaggle_\" + df.loc[nan_rows].index.astype(str)\n",
        "\n",
        "\n",
        "print(\"Columns:\", df.columns)\n",
        "print(\"First few rows:\")\n",
        "display(df.head())\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "\n",
        "print(\"\\nLabel value counts:\")\n",
        "# Count class distribution\n",
        "counts = df[LABEL_COL].value_counts().sort_index()\n",
        "print(counts)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "counts.plot(kind='bar', color=['steelblue', 'darkorange'])\n",
        "plt.title(\"Distribution of Human (0) vs AI (1) Texts\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "\n",
        "# Annotate bars with counts\n",
        "for i, v in enumerate(counts):\n",
        "    plt.text(i, v + 500, str(v), ha='center', fontsize=12)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a22624-6fef-4582-b12a-c429d7035be5",
      "metadata": {
        "id": "f4a22624-6fef-4582-b12a-c429d7035be5"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# FINAL FEATURE WRAPPER CELL\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "from typing import Dict\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def get_lines(text: str, max_lines: int):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return []\n",
        "    return text.splitlines()[:max_lines]\n",
        "\n",
        "def basic_counts(text: str):\n",
        "    text = text or \"\"\n",
        "    num_chars = len(text)\n",
        "\n",
        "    # sentence split\n",
        "    sentences = re.split(r\"[.!?]+\", text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    num_sent = len(sentences) if sentences else 1\n",
        "\n",
        "    # word tokens\n",
        "    words = re.findall(r\"\\w+\", text)\n",
        "    num_words = len(words) if words else 1\n",
        "\n",
        "    avg_sent_len = num_words / num_sent\n",
        "    return {\n",
        "        \"num_chars\": num_chars,\n",
        "        \"num_words\": num_words,\n",
        "        \"num_sentences\": num_sent,\n",
        "        \"avg_sentence_length\": avg_sent_len,\n",
        "    }\n",
        "\n",
        "\n",
        "def lexical_diversity(text: str):\n",
        "    words = re.findall(r\"\\w+\", str(text).lower())\n",
        "    if not words:\n",
        "        return {\n",
        "            \"type_token_ratio\": 0.0,\n",
        "            \"unique_words\": 0,\n",
        "        }\n",
        "    unique = set(words)\n",
        "    ttr = len(unique) / len(words)\n",
        "    return {\n",
        "        \"type_token_ratio\": ttr,\n",
        "        \"unique_words\": len(unique),\n",
        "    }\n",
        "\n",
        "\n",
        "def punctuation_stats(text: str):\n",
        "    text = text or \"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            \"pct_punct\": 0.0,\n",
        "            \"pct_upper\": 0.0,\n",
        "            \"pct_digit\": 0.0,\n",
        "        }\n",
        "    total = len(text)\n",
        "    punct = sum(ch in string.punctuation for ch in text)\n",
        "    upper = sum(ch.isupper() for ch in text)\n",
        "    digit = sum(ch.isdigit() for ch in text)\n",
        "    return {\n",
        "        \"pct_punct\": punct / total,\n",
        "        \"pct_upper\": upper / total,\n",
        "        \"pct_digit\": digit / total,\n",
        "    }\n",
        "\n",
        "\n",
        "def readability_features(text: str):\n",
        "    clean = text if isinstance(text, str) else \"\"\n",
        "    if len(clean.split()) < 3:\n",
        "        return {\n",
        "            \"flesch_reading_ease\": 0.0,\n",
        "            \"flesch_kincaid_grade\": 0.0,\n",
        "            \"gunning_fog\": 0.0,\n",
        "        }\n",
        "    try:\n",
        "        fre = textstat.flesch_reading_ease(clean)\n",
        "        fkg = textstat.flesch_kincaid_grade(clean)\n",
        "        gf  = textstat.gunning_fog(clean)\n",
        "    except Exception:\n",
        "        fre, fkg, gf = 0.0, 0.0, 0.0\n",
        "    return {\n",
        "        \"flesch_reading_ease\": fre,\n",
        "        \"flesch_kincaid_grade\": fkg,\n",
        "        \"gunning_fog\": gf,\n",
        "    }\n",
        "\n",
        "\n",
        "def repetition_features(text: str):\n",
        "    tokens = re.findall(r\"\\w+\", str(text).lower())\n",
        "    if len(tokens) < 4:\n",
        "        return {\"bigram_repetition_ratio\": 0.0}\n",
        "    bigrams = list(zip(tokens, tokens[1:]))\n",
        "    total_bigrams = len(bigrams)\n",
        "    counts = Counter(bigrams)\n",
        "    repeated = sum(c for c in counts.values() if c > 1)\n",
        "    return {\n",
        "        \"bigram_repetition_ratio\": repeated / total_bigrams\n",
        "    }\n",
        "\n",
        "\n",
        "def pos_features_spacy(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [t for t in doc if not t.is_space]\n",
        "    total_tokens = len(tokens)\n",
        "    if total_tokens == 0:\n",
        "        return {\n",
        "            \"pos_ratio_NOUN\": 0.0,\n",
        "            \"pos_ratio_VERB\": 0.0,\n",
        "            \"pos_ratio_ADJ\": 0.0,\n",
        "            \"pos_ratio_ADV\": 0.0,\n",
        "            \"pos_ratio_PRON\": 0.0,\n",
        "            \"pos_ratio_ADP\": 0.0,\n",
        "            \"pos_ratio_DET\": 0.0,\n",
        "        }\n",
        "    counts = Counter(tok.pos_ for tok in tokens)\n",
        "\n",
        "    def ratio(tag):\n",
        "        return counts.get(tag, 0) / total_tokens\n",
        "\n",
        "    return {\n",
        "        \"pos_ratio_NOUN\": ratio(\"NOUN\"),\n",
        "        \"pos_ratio_VERB\": ratio(\"VERB\"),\n",
        "        \"pos_ratio_ADJ\":  ratio(\"ADJ\"),\n",
        "        \"pos_ratio_ADV\":  ratio(\"ADV\"),\n",
        "        \"pos_ratio_PRON\": ratio(\"PRON\"),\n",
        "        \"pos_ratio_ADP\":  ratio(\"ADP\"),\n",
        "        \"pos_ratio_DET\":  ratio(\"DET\"),\n",
        "    }\n",
        "\n",
        "\n",
        "def sentence_length_stats(text: str):\n",
        "    sentences = re.split(r\"[.!?]+\", str(text))\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    if len(sentences) < 2:\n",
        "        return {\n",
        "            \"sentence_length_std\": 0.0,\n",
        "            \"sentence_length_mean\": len(str(text).split()),\n",
        "        }\n",
        "    lens = [len(s.split()) for s in sentences]\n",
        "    return {\n",
        "        \"sentence_length_std\": float(np.std(lens)),\n",
        "        \"sentence_length_mean\": float(np.mean(lens)),\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_all_features(text: str):\n",
        "    feats = {}\n",
        "    feats.update(basic_counts(text))\n",
        "    feats.update(lexical_diversity(text))\n",
        "    feats.update(punctuation_stats(text))\n",
        "    feats.update(readability_features(text))\n",
        "    feats.update(repetition_features(text))\n",
        "    feats.update(pos_features_spacy(text))\n",
        "    feats.update(sentence_length_stats(text))\n",
        "    return feats\n",
        "\n",
        "# ============================================================\n",
        "# NEW FEATURES\n",
        "# ============================================================\n",
        "\n",
        "# --- A) Em-dash / emphasis punctuation ---\n",
        "def emphasis_punctuation_features(text: str, max_lines: int = 5) -> Dict:\n",
        "    lines = get_lines(text, max_lines)\n",
        "    joined = \"\\n\".join(lines)\n",
        "    n = len(joined) if joined else 1\n",
        "\n",
        "    return {\n",
        "        \"emdash_ratio\": joined.count(\"—\") / n,\n",
        "        \"double_dash_ratio\": joined.count(\"--\") / n,\n",
        "        \"colon_ratio_emphasis\": joined.count(\":\") / n,\n",
        "        \"semicolon_ratio_emphasis\": joined.count(\";\") / n,\n",
        "        \"emphasis_punctuation_ratio\": (\n",
        "            joined.count(\"—\")\n",
        "            + joined.count(\"--\")\n",
        "            + joined.count(\":\")\n",
        "            + joined.count(\";\")\n",
        "        ) / n,\n",
        "    }\n",
        "\n",
        "\n",
        "# --- B) Hedging / cautious language ---\n",
        "HEDGE_PHRASES = [\n",
        "    \"some might argue\",\n",
        "    \"it could be said\",\n",
        "    \"it may be\",\n",
        "    \"it might be\",\n",
        "    \"it is possible that\",\n",
        "    \"it appears that\",\n",
        "    \"it seems that\",\n",
        "    \"may suggest\",\n",
        "    \"could suggest\",\n",
        "    \"is often considered\",\n",
        "    \"is generally regarded\",\n",
        "]\n",
        "\n",
        "def hedging_language_features(text: str, max_lines: int = 5) -> Dict:\n",
        "    lines = get_lines(text.lower(), max_lines)\n",
        "    if not lines:\n",
        "        return {\n",
        "            \"hedge_phrase_count\": 0,\n",
        "            \"hedge_phrase_ratio\": 0.0,\n",
        "            \"hedge_line_ratio\": 0.0,\n",
        "        }\n",
        "\n",
        "    hedge_count = 0\n",
        "    hedge_lines = 0\n",
        "\n",
        "    for ln in lines:\n",
        "        c = sum(ln.count(p) for p in HEDGE_PHRASES)\n",
        "        if c > 0:\n",
        "            hedge_count += c\n",
        "            hedge_lines += 1\n",
        "\n",
        "    total = len(lines)\n",
        "\n",
        "    return {\n",
        "        \"hedge_phrase_count\": hedge_count,\n",
        "        \"hedge_phrase_ratio\": hedge_count / total,\n",
        "        \"hedge_line_ratio\": hedge_lines / total,\n",
        "    }\n",
        "\n",
        "\n",
        "# --- C) Vague / hallucination-proxy language ---\n",
        "GENERIC_CLAIMS = [\n",
        "    \"studies suggest\",\n",
        "    \"research shows\",\n",
        "    \"experts believe\",\n",
        "    \"it is believed\",\n",
        "    \"it is widely accepted\",\n",
        "]\n",
        "\n",
        "VAGUE_QUANTIFIERS = [\n",
        "    \"various\",\n",
        "    \"numerous\",\n",
        "    \"several\",\n",
        "    \"many\",\n",
        "    \"a number of\",\n",
        "    \"some\",\n",
        "]\n",
        "\n",
        "FAKE_CITATION_PATTERNS = [\n",
        "    r\"\\[\\d+\\]\",        # [1], [23]\n",
        "    r\"\\(\\d{4}\\)\",      # (2021)\n",
        "    r\"et al\\.\",        # et al.\n",
        "]\n",
        "\n",
        "def vague_and_hallucination_features(text: str, max_lines: int = 5) -> Dict:\n",
        "    lines = get_lines(text.lower(), max_lines)\n",
        "    if not lines:\n",
        "        return {\n",
        "            \"generic_claim_ratio\": 0.0,\n",
        "            \"vague_quantifier_ratio\": 0.0,\n",
        "            \"fake_citation_ratio\": 0.0,\n",
        "        }\n",
        "\n",
        "    generic = 0\n",
        "    vague = 0\n",
        "    fake = 0\n",
        "\n",
        "    for ln in lines:\n",
        "        if any(p in ln for p in GENERIC_CLAIMS):\n",
        "            generic += 1\n",
        "        if any(q in ln for q in VAGUE_QUANTIFIERS):\n",
        "            vague += 1\n",
        "        if any(re.search(rx, ln) for rx in FAKE_CITATION_PATTERNS):\n",
        "            fake += 1\n",
        "\n",
        "    total = len(lines)\n",
        "\n",
        "    return {\n",
        "        \"generic_claim_ratio\": generic / total,\n",
        "        \"vague_quantifier_ratio\": vague / total,\n",
        "        \"fake_citation_ratio\": fake / total,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FINAL WRAPPER\n",
        "# ============================================================\n",
        "\n",
        "def compute_all_features_final(text: str, max_lines: int = 5) -> Dict:\n",
        "    feats = {}\n",
        "    feats.update(basic_counts(text))\n",
        "    feats.update(lexical_diversity(text))\n",
        "    feats.update(punctuation_stats(text))\n",
        "    feats.update(readability_features(text))\n",
        "    feats.update(repetition_features(text))\n",
        "    feats.update(pos_features_spacy(text))\n",
        "    feats.update(sentence_length_stats(text))\n",
        "    feats.update(emphasis_punctuation_features(text, max_lines))\n",
        "    feats.update(hedging_language_features(text, max_lines))\n",
        "    feats.update(vague_and_hallucination_features(text, max_lines))\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# quick sanity check\n",
        "# -------------------------\n",
        "print(compute_all_features_final(\n",
        "    \"Some might argue — it could be said that research shows various factors.\\n[1] et al. (2021)\",\n",
        "    max_lines=5\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b6a8ba-0856-46dc-93b3-4d99eca4c6de",
      "metadata": {
        "id": "29b6a8ba-0856-46dc-93b3-4d99eca4c6de"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "MAX_LINES_FOR_NEW_FEATURES = 5\n",
        "SAMPLE_N = 40000               #start here; increase later if needed\n",
        "\n",
        "# ---- 1) Create a stratified sample (label-balanced + length-bin coverage) ----\n",
        "# Length binning based on character length tertiles (computed on the full df, but only uses text length)\n",
        "char_len = df[\"text\"].astype(str).str.len()\n",
        "\n",
        "q1, q2 = char_len.quantile([1/3, 2/3]).values\n",
        "length_bin = pd.cut(char_len, bins=[-np.inf, q1, q2, np.inf], labels=[\"short\", \"medium\", \"long\"])\n",
        "\n",
        "# Add a temp bin column (small metadata, OK)\n",
        "df_tmp = df.copy()\n",
        "df_tmp[\"length_bin_tmp\"] = length_bin.astype(str)\n",
        "\n",
        "# Stratify by (label, length_bin)\n",
        "group_cols = [\"label\", \"length_bin_tmp\"]\n",
        "groups = df_tmp.groupby(group_cols, group_keys=False)\n",
        "\n",
        "n_groups = groups.ngroups\n",
        "per_group = SAMPLE_N // n_groups\n",
        "\n",
        "# Sample per group (balanced across label + bins)\n",
        "parts = []\n",
        "for key, g in groups:\n",
        "    take = min(per_group, len(g))\n",
        "    parts.append(g.sample(n=take, random_state=RANDOM_SEED))\n",
        "\n",
        "df_sample = pd.concat(parts, axis=0).sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "# If rounding left us short, top up with label-stratified sampling\n",
        "remaining = SAMPLE_N - len(df_sample)\n",
        "if remaining > 0:\n",
        "    df_rest = df_tmp.drop(df_sample.index, errors=\"ignore\")\n",
        "    topup = (\n",
        "        df_rest.groupby(\"label\", group_keys=False)\n",
        "        .apply(lambda x: x.sample(n=min(remaining // 2, len(x)), random_state=RANDOM_SEED))\n",
        "    )\n",
        "    df_sample = pd.concat([df_sample, topup], axis=0).sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "# Keep only needed columns (avoid carrying temp column forward)\n",
        "df_sample = df_sample[[\"text\", \"label\", \"doc_id\", \"source\"]].copy()\n",
        "\n",
        "print(\"Sample shape:\", df_sample.shape)\n",
        "print(\"Label distribution:\\n\", df_sample[\"label\"].value_counts())\n",
        "print(\"Source distribution (top):\\n\", df_sample[\"source\"].value_counts().head())\n",
        "\n",
        "#2) Compute features on the sample only\n",
        "#Returns one dict per row → DataFrame\n",
        "features_series = df_sample[\"text\"].progress_apply(\n",
        "    lambda t: compute_all_features_final(t, max_lines=MAX_LINES_FOR_NEW_FEATURES)\n",
        ")\n",
        "\n",
        "X_style = pd.DataFrame(list(features_series))\n",
        "\n",
        "#cast to float32 to reduce memory\n",
        "for c in X_style.columns:\n",
        "    if X_style[c].dtype == \"float64\":\n",
        "        X_style[c] = X_style[c].astype(\"float32\")\n",
        "\n",
        "print(\"Stylometric feature matrix:\", X_style.shape)\n",
        "display(X_style.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "835becab-519f-432f-bef5-31b11e4ded1e",
      "metadata": {
        "id": "835becab-519f-432f-bef5-31b11e4ded1e"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# FINAL: pipeline for TF-IDF(text) + Stylometry(features)\n",
        "# Trains LR, LinearSVC (full sparse), and KNN/RF (SVD-compressed but still TF-IDF+style).\n",
        "#\n",
        "# Requires:\n",
        "#   df_sample: has columns [\"text\",\"label\", ...]\n",
        "#   X_style:   DataFrame of your computed stylometric features, row-aligned with df_sample\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE  = 0.10  # of total df_sample\n",
        "\n",
        "# -------------------------\n",
        "# 0) Merge df_sample + X_style into one DataFrame (so ColumnTransformer can use both)\n",
        "# -------------------------\n",
        "assert len(df_sample) == len(X_style), \"df_sample and X_style must be row-aligned and same length.\"\n",
        "df_all = pd.concat([df_sample.reset_index(drop=True), X_style.reset_index(drop=True)], axis=1)\n",
        "\n",
        "text_col = \"text\"\n",
        "y = df_all[\"label\"].astype(int).values\n",
        "style_cols = list(X_style.columns)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Random, stratified train/val/test split\n",
        "# -------------------------\n",
        "idx = np.arange(len(df_all))\n",
        "\n",
        "train_idx, temp_idx = train_test_split(\n",
        "    idx,\n",
        "    test_size=(TEST_SIZE + VAL_SIZE),\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "val_frac_of_temp = VAL_SIZE / (VAL_SIZE + TEST_SIZE)\n",
        "\n",
        "val_idx, test_idx = train_test_split(\n",
        "    temp_idx,\n",
        "    test_size=(1 - val_frac_of_temp),\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y[temp_idx]\n",
        ")\n",
        "\n",
        "train_df = df_all.iloc[train_idx].reset_index(drop=True)\n",
        "val_df   = df_all.iloc[val_idx].reset_index(drop=True)\n",
        "test_df  = df_all.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "y_train = train_df[\"label\"].astype(int).values\n",
        "y_val   = val_df[\"label\"].astype(int).values\n",
        "y_test  = test_df[\"label\"].astype(int).values\n",
        "\n",
        "print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Label dist train:\", np.bincount(y_train))\n",
        "print(\"Label dist val:  \", np.bincount(y_val))\n",
        "print(\"Label dist test: \", np.bincount(y_test))\n",
        "\n",
        "# -------------------------\n",
        "# 2) Shared feature preprocessor: TF-IDF(text) + scaled style features\n",
        "#    - TF-IDF output is sparse\n",
        "#    - StandardScaler(with_mean=False) is required for sparse compatibility\n",
        "# -------------------------\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer=\"word\",\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=3,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"tfidf\", tfidf, text_col),\n",
        "        (\"style\", StandardScaler(with_mean=False), style_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.3  # keep output sparse when possible\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 3) Models\n",
        "#    LR + LinearSVC can handle the full sparse matrix\n",
        "#    KNN + RF will be trained on a compressed representation via TruncatedSVD\n",
        "# -------------------------\n",
        "def fit_eval(name, pipe):\n",
        "    pipe.fit(train_df, y_train)\n",
        "    print(f\"\\n==================== {name} ====================\")\n",
        "    print(\"VAL:\")\n",
        "    print(classification_report(y_val, pipe.predict(val_df), digits=4))\n",
        "    print(\"TEST:\")\n",
        "    print(classification_report(y_test, pipe.predict(test_df), digits=4))\n",
        "    return pipe\n",
        "\n",
        "# 3A) Logistic Regression (FULL sparse TF-IDF + style)\n",
        "lr_pipe = Pipeline([\n",
        "    (\"features\", preprocessor),\n",
        "    (\"clf\", LogisticRegression(max_iter=300, class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "# 3B) Linear SVM (FULL sparse TF-IDF + style)\n",
        "svm_pipe = Pipeline([\n",
        "    (\"features\", preprocessor),\n",
        "    (\"clf\", LinearSVC(class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "# For KNN/RF: SVD makes it dense and manageable while still using TF-IDF info\n",
        "SVD_DIM = 300  # 200-500 is a reasonable range;\n",
        "\n",
        "# knn_pipe = Pipeline([\n",
        "#     (\"features\", preprocessor),\n",
        "#     (\"svd\", TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_SEED)),\n",
        "#     (\"scale_dense\", StandardScaler()),  # now dense, safe to mean-center\n",
        "#     (\"clf\", KNeighborsClassifier(n_neighbors=15, weights=\"distance\"))\n",
        "# ])\n",
        "\n",
        "rf_pipe = Pipeline([\n",
        "    (\"features\", preprocessor),\n",
        "    (\"svd\", TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_SEED)),\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=20,\n",
        "        n_jobs=-1,\n",
        "        class_weight=\"balanced_subsample\",\n",
        "        random_state=RANDOM_SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "lr_pipe  = fit_eval(\"Logistic Regression (TF-IDF + Style)\", lr_pipe)\n",
        "svm_pipe = fit_eval(\"Linear SVM (TF-IDF + Style)\", svm_pipe)\n",
        "# knn_pipe = fit_eval(\"KNN (TF-IDF + Style via SVD)\", knn_pipe)\n",
        "rf_pipe  = fit_eval(\"Random Forest (TF-IDF + Style via SVD)\", rf_pipe)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Getting TF-IDF vector feature names (ngram vocabulary)\n",
        "#    After fitting, you can inspect what TF-IDF \"features\" correspond to.\n",
        "# -------------------------\n",
        "tfidf_fitted = lr_pipe.named_steps[\"features\"].named_transformers_[\"tfidf\"]\n",
        "tfidf_feature_names = tfidf_fitted.get_feature_names_out()\n",
        "\n",
        "print(\"\\nTF-IDF feature count:\", len(tfidf_feature_names))\n",
        "print(\"Example TF-IDF features:\", tfidf_feature_names[:25])\n",
        "\n",
        "#full hybrid feature names (TF-IDF + style)\n",
        "hybrid_feature_names = np.concatenate([tfidf_feature_names, np.array(style_cols, dtype=object)])\n",
        "print(\"Total hybrid feature count:\", len(hybrid_feature_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662d2cf6-872f-445b-bd39-15d22a294bf9",
      "metadata": {
        "id": "662d2cf6-872f-445b-bd39-15d22a294bf9"
      },
      "outputs": [],
      "source": [
        "#Simulating real world scenario by pre training on ML data and testing on real-life data to see if model training fails\n",
        "\n",
        "# STRICT SPLIT: Train/Val on Kaggle ONLY, Test on Scraped ONLY (from df_sample)\n",
        "# - Uses df_sample as the universe\n",
        "# - Keeps X_style aligned\n",
        "# - Removes any scraped rows whose normalized-text hash appears in Kaggle (within df_sample)\n",
        "#\n",
        "# Outputs:\n",
        "#   train_df, val_df, test_df\n",
        "#   X_style_train, X_style_val, X_style_test\n",
        "#   y_train, y_val, y_test\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, hashlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "VAL_SIZE = 0.15   # fraction of Kaggle portion used for validation (e.g., 15%)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "#sanity checks\n",
        "assert \"source\" in df_sample.columns, \"df_sample must have a 'source' column.\"\n",
        "assert \"text\" in df_sample.columns and \"label\" in df_sample.columns, \"df_sample must have 'text' and 'label'.\"\n",
        "assert len(df_sample) == len(X_style), \"df_sample and X_style must be row-aligned and same length.\"\n",
        "\n",
        "#helper: text hash for overlap removal\n",
        "_ws = re.compile(r\"\\s+\")\n",
        "def norm_for_hash(s: str) -> str:\n",
        "    s = \"\" if s is None else str(s)\n",
        "    return _ws.sub(\" \", s.strip().lower())\n",
        "\n",
        "def text_hash(s: str) -> str:\n",
        "    return hashlib.sha1(norm_for_hash(s).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "#1) Get Kaggle indices and Scraped indices (within df_sample)\n",
        "all_idx = np.arange(len(df_sample))\n",
        "k_idx = all_idx[df_sample[\"source\"].astype(str).values == \"kaggle\"]\n",
        "s_idx = all_idx[df_sample[\"source\"].astype(str).values != \"kaggle\"]\n",
        "\n",
        "print(\"Within df_sample:\")\n",
        "print(\"  Kaggle rows :\", len(k_idx))\n",
        "print(\"  Scraped rows:\", len(s_idx))\n",
        "\n",
        "if len(k_idx) == 0:\n",
        "    raise ValueError(\"No Kaggle rows found inside df_sample. Cannot do Kaggle-only train/val.\")\n",
        "if len(s_idx) == 0:\n",
        "    raise ValueError(\"No scraped rows found inside df_sample. Cannot do scraped-only test.\")\n",
        "\n",
        "# 2) Remove Kaggle↔Scraped overlaps (within df_sample)\n",
        "# Hash Kaggle texts\n",
        "k_text = df_sample.loc[k_idx, \"text\"].astype(str)\n",
        "k_hash_set = set(k_text.map(text_hash).values)\n",
        "\n",
        "#Hash scraped texts and keep only non-overlapping\n",
        "s_text = df_sample.loc[s_idx, \"text\"].astype(str)\n",
        "s_hash = s_text.map(text_hash)\n",
        "keep_scraped = ~s_hash.isin(k_hash_set)\n",
        "\n",
        "test_idx = s_idx[keep_scraped.values]\n",
        "dropped = int((~keep_scraped).sum())\n",
        "\n",
        "print(f\"  Dropped scraped overlaps vs Kaggle: {dropped}\")\n",
        "print(f\"  Final TEST rows (scraped, deduped): {len(test_idx)}\")\n",
        "\n",
        "if len(test_idx) == 0:\n",
        "    raise ValueError(\"After overlap removal, no scraped rows remain for TEST. Increase df_sample size or change sampling.\")\n",
        "\n",
        "#3) Train/Val split ONLY on Kaggle (stratified by label)\n",
        "k_labels = df_sample.loc[k_idx, \"label\"].astype(int).values\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    k_idx,\n",
        "    test_size=VAL_SIZE,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=k_labels\n",
        ")\n",
        "\n",
        "#4) Slice df_sample + X_style together (keeps alignment)\n",
        "train_df = df_sample.iloc[train_idx].reset_index(drop=True)\n",
        "val_df   = df_sample.iloc[val_idx].reset_index(drop=True)\n",
        "test_df  = df_sample.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "X_style_train = X_style.iloc[train_idx].reset_index(drop=True)\n",
        "X_style_val   = X_style.iloc[val_idx].reset_index(drop=True)\n",
        "X_style_test  = X_style.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "y_train = train_df[\"label\"].astype(int).values\n",
        "y_val   = val_df[\"label\"].astype(int).values\n",
        "y_test  = test_df[\"label\"].astype(int).values\n",
        "\n",
        "# Hash overlap checks\n",
        "train_hashes = set(train_df[\"text\"].astype(str).map(text_hash).values)\n",
        "val_hashes   = set(val_df[\"text\"].astype(str).map(text_hash).values)\n",
        "test_hashes  = set(test_df[\"text\"].astype(str).map(text_hash).values)\n",
        "\n",
        "print(\"\\nText-hash overlaps (should be 0):\")\n",
        "print(\"  TRAIN ∩ VAL :\", len(train_hashes & val_hashes))\n",
        "print(\"  TRAIN ∩ TEST:\", len(train_hashes & test_hashes))\n",
        "print(\"  VAL   ∩ TEST:\", len(val_hashes & test_hashes))\n",
        "\n",
        "print(\"\\nSplit sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Label dist TRAIN:\", np.bincount(y_train))\n",
        "print(\"Label dist VAL:  \", np.bincount(y_val))\n",
        "print(\"Label dist TEST: \", np.bincount(y_test))\n",
        "\n",
        "#Kaggle-only TRAIN/VAL, Scraped-only TEST, with deduping and aligned X_style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3b1bf0-0788-4859-8da9-4230b4fd72df",
      "metadata": {
        "id": "ce3b1bf0-0788-4859-8da9-4230b4fd72df"
      },
      "outputs": [],
      "source": [
        "# Train + Evaluate models on STRICT split:\n",
        "#   TRAIN/VAL = Kaggle only\n",
        "#   TEST      = Scraped only\n",
        "# Uses ALL features together:\n",
        "#   TF-IDF(text) + precomputed stylometric X_style_*\n",
        "#\n",
        "# we have  the following from the split cell:\n",
        "#   train_df, val_df, test_df\n",
        "#   X_style_train, X_style_val, X_style_test\n",
        "#   y_train, y_val, y_test\n",
        "#\n",
        "# Trains: Logistic Regression, Linear SVM, Random Forest (via SVD) all using TF-IDF+Style \"together\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# -------------------------\n",
        "# 0) Merge text+labels with precomputed style features (keeps alignment)\n",
        "# -------------------------\n",
        "assert len(train_df) == len(X_style_train)\n",
        "assert len(val_df)   == len(X_style_val)\n",
        "assert len(test_df)  == len(X_style_test)\n",
        "\n",
        "train_all = pd.concat([train_df.reset_index(drop=True), X_style_train.reset_index(drop=True)], axis=1) #Akanksha use this\n",
        "val_all   = pd.concat([val_df.reset_index(drop=True),   X_style_val.reset_index(drop=True)], axis=1)\n",
        "test_all  = pd.concat([test_df.reset_index(drop=True),  X_style_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "text_col = \"text\"\n",
        "style_cols = list(X_style_train.columns)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Shared preprocessor: TF-IDF + scaled style (together)\n",
        "# -------------------------\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer=\"word\",\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=3,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"tfidf\", tfidf, text_col),\n",
        "        (\"style\", StandardScaler(with_mean=False), style_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.3\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 2) Pipelines\n",
        "# -------------------------\n",
        "lr_pipe = Pipeline([\n",
        "    (\"features\", preprocessor),\n",
        "    (\"clf\", LogisticRegression(max_iter=300, class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "svm_pipe = Pipeline([\n",
        "    (\"features\", preprocessor),\n",
        "    (\"clf\", LinearSVC(class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "# Random Forest needs dense input -> use SVD compression (still TF-IDF + style together)\n",
        "SVD_DIM = 300  # 200-500 reasonable\n",
        "rf_pipe = Pipeline([\n",
        "    (\"features\", preprocessor),\n",
        "    (\"svd\", TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_SEED)),\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=20,\n",
        "        n_jobs=-1,\n",
        "        class_weight=\"balanced_subsample\",\n",
        "        random_state=RANDOM_SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "models = {\n",
        "    \"LR (TF-IDF + Style)\": lr_pipe,\n",
        "    \"Linear SVM (TF-IDF + Style)\": svm_pipe,\n",
        "    f\"RF (TF-IDF + Style via SVD={SVD_DIM})\": rf_pipe,\n",
        "}\n",
        "\n",
        "# -------------------------\n",
        "# 3) Compact results table (VAL + TEST)\n",
        "# -------------------------\n",
        "def eval_metrics(y_true, y_pred):\n",
        "    p, r, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"binary\", zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
        "\n",
        "rows = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(train_all, y_train)\n",
        "\n",
        "    val_pred = model.predict(val_all)\n",
        "    test_pred = model.predict(test_all)\n",
        "\n",
        "    rows.append({\"model\": name, \"split\": \"VAL\", **eval_metrics(y_val, val_pred)})\n",
        "    rows.append({\"model\": name, \"split\": \"TEST\", **eval_metrics(y_test, test_pred)})\n",
        "\n",
        "    print(f\"\\n==================== {name} ====================\")\n",
        "    print(\"VAL:\")\n",
        "    print(classification_report(y_val, val_pred, digits=4))\n",
        "    print(\"TEST:\")\n",
        "    print(classification_report(y_test, test_pred, digits=4))\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values([\"split\", \"f1\"], ascending=[True, False])\n",
        "print(\"\\nCompact Results Table\")\n",
        "display(results_df.round(4))\n",
        "\n",
        "# 4) LR coefficient interpretation (Top AI vs Human features)\n",
        "# Only for LR (interpretable). SVM coefficients are also available but less nicely probabilistic.\n",
        "tfidf_fitted = lr_pipe.named_steps[\"features\"].named_transformers_[\"tfidf\"]\n",
        "tfidf_feature_names = tfidf_fitted.get_feature_names_out()\n",
        "\n",
        "feature_names = np.concatenate([tfidf_feature_names, np.array(style_cols, dtype=object)])\n",
        "coef = lr_pipe.named_steps[\"clf\"].coef_[0]\n",
        "\n",
        "coef_df = pd.DataFrame({\"feature\": feature_names, \"coefficient\": coef})\n",
        "\n",
        "TOPK = 25\n",
        "top_ai = coef_df.sort_values(\"coefficient\", ascending=False).head(TOPK)\n",
        "top_human = coef_df.sort_values(\"coefficient\", ascending=True).head(TOPK)\n",
        "\n",
        "print(f\"\\n Top {TOPK} AI-indicative features (positive coef)\")\n",
        "display(top_ai)\n",
        "\n",
        "print(f\"\\n Top {TOPK} Human-indicative features (negative coef)\")\n",
        "display(top_human)\n",
        "\n",
        "# Stylometry-only interpretation (often the most report-friendly)\n",
        "n_tfidf = len(tfidf_feature_names)\n",
        "style_coef_df = coef_df.iloc[n_tfidf:].copy()\n",
        "\n",
        "print(\"\\n Stylometric features pushing AI prediction\")\n",
        "display(style_coef_df.sort_values(\"coefficient\", ascending=False).head(15))\n",
        "\n",
        "print(\"\\n Stylometric features pushing Human prediction\")\n",
        "display(style_coef_df.sort_values(\"coefficient\", ascending=True).head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53328ad-e8f8-466b-960c-f2b1a8f1881e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "c53328ad-e8f8-466b-960c-f2b1a8f1881e"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import json\n",
        "\n",
        "ART_DIR = \"artifacts_ml\"\n",
        "import os\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "joblib.dump(lr_pipe, f\"{ART_DIR}/lr_pipe.joblib\")\n",
        "\n",
        "with open(f\"{ART_DIR}/style_cols.json\", \"w\") as f:\n",
        "    json.dump(style_cols, f)\n",
        "\n",
        "print(\" Saved ML artifacts to:\", ART_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DASHBOARD: LR (TF-IDF + Stylometry) interpretability\n",
        "#\n",
        "# Requires:\n",
        "#   lr_pipe, val_all, test_all\n",
        "#   val_df, test_df (must include 'text' and 'source')\n",
        "#   y_val, y_test (numpy arrays)\n",
        "#   style_cols (list)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "\n",
        "def _metrics(y_true, y_pred, y_prob=None):\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    out = {\"acc\": accuracy_score(y_true, y_pred), \"prec\": p, \"rec\": r, \"f1\": f1}\n",
        "    if y_prob is not None:\n",
        "        try: out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
        "        except Exception: out[\"roc_auc\"] = np.nan\n",
        "        try: out[\"pr_auc\"]  = average_precision_score(y_true, y_prob)\n",
        "        except Exception: out[\"pr_auc\"]  = np.nan\n",
        "    return out\n",
        "\n",
        "def prob_ai(model, X):\n",
        "    if not hasattr(model, \"predict_proba\"):\n",
        "        raise ValueError(\"Dashboard expects a model with predict_proba (Logistic Regression).\")\n",
        "    return model.predict_proba(X)[:, 1]\n",
        "\n",
        "def length_bins(texts: pd.Series, q=(0.33, 0.66)):\n",
        "    lens = texts.astype(str).str.len()\n",
        "    q1, q2 = lens.quantile(list(q)).values\n",
        "    return pd.cut(lens, [-np.inf, q1, q2, np.inf], labels=[\"short\",\"medium\",\"long\"]).astype(str)\n",
        "\n",
        "def plot_confidence_distributions(y_true, p_ai, title_suffix=\"\"):\n",
        "    p_h = p_ai[y_true == 0]\n",
        "    p_a = p_ai[y_true == 1]\n",
        "\n",
        "    bins = np.linspace(0, 1, 51)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(p_h, bins=bins, alpha=0.6, density=True, label=\"True Human (0)\")\n",
        "    ax.hist(p_a, bins=bins, alpha=0.6, density=True, label=\"True AI (1)\")\n",
        "    ax.set_title(f\"Prob(AI) distributions (SCRAPED TEST){title_suffix}\")\n",
        "    ax.set_xlabel(\"Predicted Prob(AI)\")\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    def cdf(vals):\n",
        "        v = np.sort(vals)\n",
        "        y = np.linspace(0, 1, len(v), endpoint=True)\n",
        "        return v, y\n",
        "\n",
        "    hx, hy = cdf(p_h)\n",
        "    axx, ayy = cdf(p_a)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(hx, hy, label=\"True Human (0)\")\n",
        "    ax.plot(axx, ayy, label=\"True AI (1)\")\n",
        "    ax.set_title(f\"CDF of Prob(AI) (SCRAPED TEST){title_suffix}\")\n",
        "    ax.set_xlabel(\"Predicted Prob(AI)\")\n",
        "    ax.set_ylabel(\"Cumulative fraction\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_length_bin_f1(df_text, y_true, p_ai, title):\n",
        "    bins = length_bins(df_text[\"text\"])\n",
        "    y_pred = (p_ai >= 0.5).astype(int)\n",
        "\n",
        "    labels = [\"short\",\"medium\",\"long\"]\n",
        "    f1s, counts = [], []\n",
        "    for b in labels:\n",
        "        m = (bins == b).values\n",
        "        counts.append(int(m.sum()))\n",
        "        if m.sum() == 0:\n",
        "            f1s.append(np.nan)\n",
        "        else:\n",
        "            _, _, f1, _ = precision_recall_fscore_support(y_true[m], y_pred[m], average=\"binary\", zero_division=0)\n",
        "            f1s.append(float(f1))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(labels, f1s)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_ylabel(\"F1\")\n",
        "    ax.set_title(title)\n",
        "    for i, c in enumerate(counts):\n",
        "        ax.text(i, 0.02, f\"n={c}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.show()\n",
        "\n",
        "def per_source_table(test_df, y_true, p_ai, min_n=20):\n",
        "    y_pred = (p_ai >= 0.5).astype(int)\n",
        "    rows = []\n",
        "    for src, idx in test_df.groupby(\"source\").indices.items():\n",
        "        idx = np.array(list(idx))\n",
        "        if len(idx) < min_n:\n",
        "            continue\n",
        "        m = _metrics(y_true[idx], y_pred[idx], p_ai[idx])\n",
        "        rows.append({\"source\": src, \"n\": len(idx), **m})\n",
        "    out = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
        "    print(f\"\\n=== Per-source performance on SCRAPED TEST (min_n={min_n}) ===\")\n",
        "    display(out.round(4))\n",
        "    return out\n",
        "\n",
        "# ---- Compute probabilities ----\n",
        "val_p = prob_ai(lr_pipe, val_all)\n",
        "test_p = prob_ai(lr_pipe, test_all)\n",
        "\n",
        "val_pred = (val_p >= 0.5).astype(int)\n",
        "test_pred = (test_p >= 0.5).astype(int)\n",
        "\n",
        "#Compact metrics\n",
        "rows = [\n",
        "    {\"model\":\"LR (TF-IDF+Style)\", \"split\":\"VAL\",  **_metrics(np.array(y_val),  val_pred,  val_p)},\n",
        "    {\"model\":\"LR (TF-IDF+Style)\", \"split\":\"TEST\", **_metrics(np.array(y_test), test_pred, test_p)},\n",
        "]\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "print(\"\\n=== Compact metrics ===\")\n",
        "display(metrics_df.round(4))\n",
        "\n",
        "#Confusion matrix (scraped test)\n",
        "cm = confusion_matrix(np.array(y_test), test_pred, labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Human(0)\",\"AI(1)\"])\n",
        "fig, ax = plt.subplots()\n",
        "disp.plot(ax=ax, values_format=\"d\")\n",
        "ax.set_title(\"Confusion Matrix (SCRAPED TEST) - LR\")\n",
        "plt.show()\n",
        "\n",
        "# Per-source table (scraped test)\n",
        "per_source_table(test_df.reset_index(drop=True), np.array(y_test), test_p, min_n=20)\n",
        "\n",
        "# Length-bin performance (scraped test)\n",
        "plot_length_bin_f1(test_df.reset_index(drop=True), np.array(y_test), test_p,\n",
        "                   \"F1 by Length Bin (SCRAPED TEST) - LR\")\n",
        "\n",
        "# Confidence distributions (scraped test)\n",
        "plot_confidence_distributions(np.array(y_test), test_p)\n",
        "\n",
        "# Coefficient interpretability\n",
        "# show_lr_top_features(lr_pipe, style_cols, topk=25)\n",
        "\n",
        "# Expose test_p so Cell B can reuse it without recomputing\n",
        "TEST_PROB_AI = test_p"
      ],
      "metadata": {
        "id": "3bp3XSra55Ix"
      },
      "id": "3bp3XSra55Ix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# INTERACTIVE CUSTOM INPUT TESTER (LogReg TF-IDF + Stylometry)\n",
        "# Single-cell, notebook-friendly\n",
        "#\n",
        "# Requires (already defined in notebook):\n",
        "#   lr_pipe\n",
        "#   style_cols\n",
        "#   compute_all_features_final()\n",
        "#   TEST_PROB_AI        (from dashboard cell)\n",
        "#   y_test              (scraped test labels)\n",
        "#\n",
        "# If widgets do not render:\n",
        "#   pip install ipywidgets\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "# helper: overlay confidence marker\n",
        "def _plot_conf_dist_with_marker(y_true, p_ai, marker_prob):\n",
        "    p_h = p_ai[y_true == 0]\n",
        "    p_a = p_ai[y_true == 1]\n",
        "    bins = np.linspace(0, 1, 51)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(p_h, bins=bins, alpha=0.6, density=True, label=\"True Human (0)\")\n",
        "    ax.hist(p_a, bins=bins, alpha=0.6, density=True, label=\"True AI (1)\")\n",
        "    ax.axvline(marker_prob, linestyle=\"--\", linewidth=2, color=\"black\")\n",
        "    ax.text(marker_prob, ax.get_ylim()[1]*0.95, \"Custom input\",\n",
        "            rotation=90, va=\"top\", ha=\"right\")\n",
        "    ax.set_title(\"Prob(AI) distribution on SCRAPED TEST\")\n",
        "    ax.set_xlabel(\"Predicted Prob(AI)\")\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# core prediction\n",
        "def _run_custom_prediction(text, max_lines, top_k, overlay):\n",
        "    feats = compute_all_features_final(text, max_lines=max_lines)\n",
        "\n",
        "    row = {c: float(feats.get(c, 0.0)) for c in style_cols}\n",
        "    row[\"text\"] = text\n",
        "    X_one = pd.DataFrame([row])\n",
        "\n",
        "    prob_ai = float(lr_pipe.predict_proba(X_one)[:, 1][0])\n",
        "    pred = int(prob_ai >= 0.5)\n",
        "    conf = prob_ai if pred == 1 else (1 - prob_ai)\n",
        "\n",
        "    print(\"Prediction:\", \"AI (1)\" if pred == 1 else \"Human (0)\")\n",
        "    print(f\"Prob(AI):   {prob_ai:.4f}\")\n",
        "    print(f\"Confidence: {conf:.4f}\")\n",
        "\n",
        "    # show key stylometry (interpretable)\n",
        "    print(\"\\n--- Stylometric summary ---\")\n",
        "    for k in [\n",
        "        \"num_words\",\"num_sentences\",\"avg_sentence_length\",\n",
        "        \"type_token_ratio\",\"pct_punct\",\n",
        "        \"sentence_length_std\",\"bigram_repetition_ratio\"\n",
        "    ]:\n",
        "        if k in feats:\n",
        "            print(f\"{k}: {feats[k]}\")\n",
        "\n",
        "    # per-sample LR contributions\n",
        "    try:\n",
        "        features = lr_pipe.named_steps[\"features\"]\n",
        "        tfidf_vec = features.named_transformers_[\"tfidf\"]\n",
        "        tfidf_names = tfidf_vec.get_feature_names_out()\n",
        "\n",
        "        X_trans = features.transform(X_one).tocoo()\n",
        "        coef = lr_pipe.named_steps[\"clf\"].coef_[0]\n",
        "        contrib = X_trans.data * coef[X_trans.col]\n",
        "\n",
        "        feature_names = np.concatenate([tfidf_names, np.array(style_cols, dtype=object)])\n",
        "        order = np.argsort(np.abs(contrib))[::-1][:top_k]\n",
        "\n",
        "        print(f\"\\n--- Top {top_k} feature contributions ---\")\n",
        "        for i in order:\n",
        "            fname = feature_names[X_trans.col[i]]\n",
        "            print(f\"{fname}: {contrib[i]:+.6f}\")\n",
        "    except Exception as e:\n",
        "        print(\"\\n(Contribution breakdown skipped:\", e, \")\")\n",
        "\n",
        "    # overlay marker on TEST distribution\n",
        "    if overlay:\n",
        "        _plot_conf_dist_with_marker(np.array(y_test), TEST_PROB_AI, prob_ai)\n",
        "\n",
        "\n",
        "# widgets UI\n",
        "text_box = widgets.Textarea(\n",
        "    placeholder=\"Paste text here...\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"180px\")\n",
        ")\n",
        "\n",
        "max_lines_slider = widgets.IntSlider(\n",
        "    value=5, min=1, max=50, step=1,\n",
        "    description=\"max_lines\"\n",
        ")\n",
        "\n",
        "topk_slider = widgets.IntSlider(\n",
        "    value=12, min=5, max=30, step=1,\n",
        "    description=\"top_k\"\n",
        ")\n",
        "\n",
        "overlay_checkbox = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Overlay on TEST confidence plot\"\n",
        ")\n",
        "\n",
        "run_button = widgets.Button(\n",
        "    description=\"Predict\",\n",
        "    button_style=\"primary\"\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_run(_):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        txt = text_box.value.strip()\n",
        "        if not txt:\n",
        "            print(\"Please paste some text.\")\n",
        "            return\n",
        "        _run_custom_prediction(\n",
        "            txt,\n",
        "            max_lines_slider.value,\n",
        "            topk_slider.value,\n",
        "            overlay_checkbox.value\n",
        "        )\n",
        "\n",
        "run_button.on_click(on_run)\n",
        "\n",
        "display(\n",
        "    text_box,\n",
        "    widgets.HBox([max_lines_slider, topk_slider, overlay_checkbox, run_button]),\n",
        "    output\n",
        ")"
      ],
      "metadata": {
        "id": "ucGN4-DH55Aw"
      },
      "id": "ucGN4-DH55Aw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SAVE FIXED SPLITS TO GOOGLE DRIVE (for reuse in LSTM) ===\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "ART_DIR = Path(\"/content/drive/MyDrive/artifacts/data_splits_v1\")\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#Make sure these exists\n",
        "# train_all, val_all, test_all  (DataFrames with 'text' + style feature columns)\n",
        "# y_train, y_val, y_test\n",
        "# style_cols (list of stylometry column names)\n",
        "\n",
        "# --- sanity checks ---\n",
        "for name, df in [(\"train_all\", train_all), (\"val_all\", val_all), (\"test_all\", test_all)]:\n",
        "    missing = [c for c in [\"text\"] + style_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name} missing columns: {missing[:10]}{'...' if len(missing)>10 else ''}\")\n",
        "\n",
        "# --- keep only what LSTM needs ---\n",
        "train_export = train_all[[\"text\"] + style_cols].copy()\n",
        "val_export   = val_all[[\"text\"] + style_cols].copy()\n",
        "test_export  = test_all[[\"text\"] + style_cols].copy()\n",
        "\n",
        "train_export[\"label\"] = y_train\n",
        "val_export[\"label\"]   = y_val\n",
        "test_export[\"label\"]  = y_test\n",
        "\n",
        "# --- save datasets ---\n",
        "try:\n",
        "    train_export.to_parquet(ART_DIR / \"train_all.parquet\", index=False)\n",
        "    val_export.to_parquet(ART_DIR / \"val_all.parquet\", index=False)\n",
        "    test_export.to_parquet(ART_DIR / \"test_all.parquet\", index=False)\n",
        "    fmt = \"parquet\"\n",
        "except Exception:\n",
        "    train_export.to_csv(ART_DIR / \"train_all.csv\", index=False)\n",
        "    val_export.to_csv(ART_DIR / \"val_all.csv\", index=False)\n",
        "    test_export.to_csv(ART_DIR / \"test_all.csv\", index=False)\n",
        "    fmt = \"csv\"\n",
        "\n",
        "meta = {\n",
        "    \"format\": fmt,\n",
        "    \"style_cols\": style_cols\n",
        "}\n",
        "with open(ART_DIR / \"meta.json\", \"w\") as f:\n",
        "    json.dump(meta, f)\n",
        "\n",
        "print(\" Exported text+stylometry datasets to:\", ART_DIR)\n",
        "print(\"Format:\", fmt)\n",
        "print(\"Train shape:\", train_export.shape, \"Val shape:\", val_export.shape, \"Test shape:\", test_export.shape)"
      ],
      "metadata": {
        "id": "ADItw3Bo5430"
      },
      "id": "ADItw3Bo5430",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Commit"
      ],
      "metadata": {
        "id": "qbOPW_Jt540K"
      },
      "id": "qbOPW_Jt540K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
        "# install can take a minute\n",
        "\n",
        "import os\n",
        "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
        "NOTEBOOKS_DIR = \"/content/drive/MyDrive/\" # @param {type:\"string\"}\n",
        "NOTEBOOK_NAME = \"03_feature_engineering_and_baseline_models.ipynb\" # @param {type:\"string\"}\n",
        "#------------------------------------------------------------------------------#\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
        "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
        "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
        "!apt install pandoc > /dev/null 2>&1\n",
        "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
        "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
        "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
        "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
      ],
      "metadata": {
        "id": "qGqB_eMu54wz"
      },
      "id": "qGqB_eMu54wz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}